{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T19:42:21.154639Z",
     "start_time": "2024-07-06T19:42:21.142423Z"
    }
   },
   "source": [
    "# imports \n",
    "import os \n",
    "import tensorflow as tf \n",
    "import cv2 \n",
    "import numpy\n",
    "from matplotlib import pyplot as plt\n",
    "from jiwer import wer \n",
    "# making GPU be used, and setting memory limits\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "# gpus = tf.config.list_logical_devices('GPU')\n",
    "print(gpus)\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    print(\"gpu set\")\n",
    "except:\n",
    "    pass\n",
    "    print(\"failed\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "gpu set\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "7c48bcf107f4c31b",
   "metadata": {},
   "source": [
    "## basic functions"
   ]
  },
  {
   "cell_type": "code",
   "id": "787272fefa6ec811",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T19:42:23.374748Z",
     "start_time": "2024-07-06T19:42:22.110446Z"
    }
   },
   "source": [
    "# setting up the functions to convert from chars to num and vice versa\n",
    "vocab = [x for x in \"ABCDEFGHIJKLMNOPQRSTUVWXYZ \"]\n",
    "charToNum = tf.keras.layers.StringLookup(vocabulary=vocab, oov_token=\"\")\n",
    "numToChar = tf.keras.layers.StringLookup(vocabulary=charToNum.get_vocabulary(), oov_token=\"\", invert=True)\n",
    "\n",
    "# facial detection vars \n",
    "faceCascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "lastKnownCrop = (0, 0, 160, 150)\n",
    "\n",
    "# data dir\n",
    "rootDir = 'A:\\Lip Reading\\Potential Datasets\\BBC LRS2\\\\allFiles'\n",
    "rootDir2 = 'A:\\Lip Reading\\Potential Datasets\\\\BBC LRS2'\n",
    "# r = \"A:\\Lip Reading\\Potential Datasets\\BBC LRS2\\\\allFiles\"\n",
    "\n",
    "errorNums = 0 \n",
    "errorPaths = []\n",
    "errorInfo = []\n",
    "frameSize = None\n",
    "frameSizeOld = None\n",
    "newFrameSize = None\n",
    "grayFrame = None\n",
    "newImageSize = (40, 120)\n",
    "\n",
    "maxCharCt = 145 # found from the dataStats.ipynb\n",
    "maxFrameCt = 2*maxCharCt"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "c2e49edea7bd4712",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T19:42:24.280443Z",
     "start_time": "2024-07-06T19:42:24.258422Z"
    }
   },
   "source": [
    "# util funcs \n",
    "def faceDetection(img):\n",
    "    # TROUBLESHOOTING\n",
    "    # print(\"max size:\",img.shape, img.shape[0] - 3 * padding, img.shape[1] - 3 * padding)\n",
    "    return faceCascade.detectMultiScale(\n",
    "        img,\n",
    "        scaleFactor=1.3,\n",
    "        minNeighbors=5,\n",
    "        minSize=(30, 30),\n",
    "    )\n",
    "\n",
    "def cropForMouth(img) -> numpy.ndarray:\n",
    "    global lastKnownCrop\n",
    "    rects = faceDetection(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY))\n",
    "    \n",
    "    # finding the largest face in a given image \n",
    "    largestFace = (0,0,0,0)\n",
    "    for (x, y, w, l) in rects:\n",
    "        if (w * l) > largestFace[2] * largestFace[3]:\n",
    "            largestFace = (x, y,w,l)\n",
    "        \n",
    "    if largestFace == (0,0,0,0):\n",
    "        largestFace =lastKnownCrop\n",
    "    # cropping for face \n",
    "    lastKnownCrop = largestFace\n",
    "    y1 = lastKnownCrop[1] \n",
    "    x1 = lastKnownCrop[0]\n",
    "    y2 = y1 + lastKnownCrop[3] \n",
    "    x2 = x1 + lastKnownCrop[2]\n",
    "    return img[y1 + int(0.65 * lastKnownCrop[3]): y2, x1 + int(0.05 * lastKnownCrop[2]): int(0.95 * x2)]\n",
    "\n",
    "def numberToWords(num):  \n",
    "    if num == 0:  \n",
    "        return \"zero\"  \n",
    "    ones = [\"\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"]  \n",
    "    tens = [\"\", \"\", \"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\"]  \n",
    "    teens = [\"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\", \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\"]  \n",
    "    words = \"\"  \n",
    "    if num>= 1000:  \n",
    "        words += ones[num // 1000] + \" thousand \"  \n",
    "        num %= 1000  \n",
    "    if num>= 100:  \n",
    "        words += ones[num // 100] + \" hundred \"  \n",
    "        num %= 100  \n",
    "    if num>= 10 and num<= 19:  \n",
    "        words += teens[num - 10] + \" \"  \n",
    "        num = 0  \n",
    "    elif num>= 20:  \n",
    "        words += tens[num // 10] + \" \"  \n",
    "        num %= 10  \n",
    "    if num>= 1 and num<= 9:  \n",
    "        words += ones[num] + \" \"  \n",
    "    return words.strip().upper()"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "c7fbad6e80991f3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T19:42:24.767494Z",
     "start_time": "2024-07-06T19:42:24.755976Z"
    }
   },
   "source": [
    "def loadData(path): \n",
    "    # tf has the paths as bytes so decode that\n",
    "    path = bytes.decode(path.numpy())\n",
    "    \n",
    "    # extract just the file names\n",
    "    global rootDir\n",
    "    fileName = path.split('\\\\')[-1].split('.')[0]\n",
    "    # generate the respective paths of the data\n",
    "    videoPath = os.path.join(rootDir,f'{fileName}.mp4')\n",
    "    alignmentPath = os.path.join(rootDir,f'{fileName}.txt')\n",
    "    \n",
    "    # return the frames and alignments\n",
    "    frames = loadVideo(videoPath) \n",
    "    alignments = loadText(alignmentPath)\n",
    "    return frames, alignments\n",
    "\n",
    "def loadVideo(path): \n",
    "    cap = cv2.VideoCapture(path)\n",
    "    global lastKnownCrop, frameSizeOld, frameSize, newFrameSize, grayFrame\n",
    "    global errorNums\n",
    "    processedFrames = []\n",
    "    isFirstFrame = True\n",
    "    frameShape = None\n",
    "    # for each frame \n",
    "    for n in range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))): \n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # in case a frame is missing, just continue\n",
    "        if frame is None or frame.shape[0] == 0: \n",
    "            continue\n",
    "        \n",
    "        if isFirstFrame: \n",
    "            frameShape  = frame.shape\n",
    "            isFirstFrame = False\n",
    "        \n",
    "        if frame.shape != frameShape: \n",
    "            continue\n",
    "        # crop only the mouth like we'll do on the RPI \n",
    "        frameSizeOld = frame.shape\n",
    "        frame = cropForMouth(frame)\n",
    "        frameSize = frame.shape\n",
    "        frame = cv2.resize(frame, (newImageSize[1], newImageSize[0]))\n",
    "        newFrameSize = frame.shape\n",
    "        grayFrame = tf.image.rgb_to_grayscale(frame)\n",
    "        processedFrames.append(grayFrame)\n",
    "        # processedFrames = [*processedFrames, grayFrame]\n",
    "\n",
    "    \n",
    "    cap.release()    \n",
    "\n",
    "    # generate the normalized frames (deviation from the average)\n",
    "    mean = tf.math.reduce_mean(processedFrames, keepdims=True)\n",
    "    try: \n",
    "        std = tf.math.reduce_std(tf.cast(processedFrames, tf.float32),  keepdims=True)\n",
    "    except: \n",
    "        \n",
    "        errorPaths.append(path)\n",
    "        errorInfo.append(\"SECOND STATEMENT\")\n",
    "        errorInfo.append(len(processedFrames))\n",
    "        errorInfo.append(frameSizeOld)\n",
    "        errorInfo.append(frameSize)\n",
    "        errorInfo.append(newFrameSize)\n",
    "        errorInfo.append(grayFrame)\n",
    "    std = tf.math.reduce_std(tf.cast(processedFrames, tf.float32), keepdims=True)\n",
    "    frames = tf.cast(processedFrames, tf.float32)\n",
    "    normalizedFrames = (tf.cast(frames, tf.float32) - tf.cast(mean, tf.float32)) / tf.cast(std, tf.float32)\n",
    "    return normalizedFrames\n",
    "\n",
    "def loadText(path): \n",
    "    # open and parse the file \n",
    "    with open(path, 'r') as file: lines = file.readlines()\n",
    "    file.close()\n",
    "    \n",
    "    # return the number equivalent of each of the characters of the word \n",
    "    tokens = []\n",
    "    words = lines[0].split()\n",
    "    del words[0]\n",
    "\n",
    "    for word in words: \n",
    "        if word.isnumeric():\n",
    "            newWord = numberToWords(int(word))\n",
    "            words[words.index(word)] = newWord\n",
    "    words = \" \".join(words).split()\n",
    "    # print(words)\n",
    "    for word in words: \n",
    "        tokens = [*tokens,' ', word]\n",
    "    try:\n",
    "        return charToNum(tf.reshape(tf.strings.unicode_split(tokens, input_encoding='UTF-8'), (-1)))[1:]   \n",
    "    except: \n",
    "        print(tokens)\n",
    "\n",
    "def processData(path): \n",
    "    return tf.py_function(loadData, [path],  (tf.float32, tf.int64))"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "a401cd6fa0067aed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T19:42:25.239514Z",
     "start_time": "2024-07-06T19:42:25.230003Z"
    }
   },
   "source": [
    "def getFrameCount(path) -> int: \n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frameCount = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    cap.release()\n",
    "    return frameCount\n",
    "\n",
    "def getCharCount(path) -> int: \n",
    "    return len(loadText(path))"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "9203a9532b428b04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T19:42:25.758615Z",
     "start_time": "2024-07-06T19:42:25.755610Z"
    }
   },
   "source": [
    "# numberPath = \"A:\\\\Lip Reading\\\\Potential Datasets\\\\BBC LRS2\\\\Numbers.txt\"\n",
    "# tensorPath = tf.convert_to_tensor(numberPath, dtype=tf.string)\n",
    "# path = bytes.decode(tensorPath.numpy())\n",
    "# fileName = path.split('\\\\')[-1].split('.')[0]\n",
    "# \n",
    "# # testing if the loadData, loadVideo, and loadText function all work\n",
    "# alignmentPath = os.path.join(rootDir2,f'{fileName}.txt')\n",
    "# loadText(alignmentPath)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "1dc8265fa1dc4688",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T19:42:28.723001Z",
     "start_time": "2024-07-06T19:42:26.308512Z"
    }
   },
   "source": [
    "rawPath = \"A:\\\\Lip Reading\\\\Potential Datasets\\\\BBC LRS2\\\\allFiles\\\\5535415699068794046_00006.mp4\"\n",
    "\n",
    "tensorPath = tf.convert_to_tensor(rawPath, dtype=tf.string)\n",
    "path = bytes.decode(tensorPath.numpy())\n",
    "fileName = path.split('\\\\')[-1].split('.')[0]\n",
    "\n",
    "# testing if the loadData, loadVideo, and loadText function all work\n",
    "videoPath = os.path.join(rootDir,f'{fileName}.mp4')\n",
    "alignmentPath = os.path.join(rootDir,f'{fileName}.txt')\n",
    "\n",
    "loadVideo(videoPath)\n",
    "loadText(alignmentPath)\n",
    "\n",
    "frames, text = loadData(tensorPath)\n",
    "print(type(frames))\n",
    "print(frames)\n",
    "print(len(frames[0][0]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor(\n",
      "[[[[ 0.9325923 ]\n",
      "   [ 0.97313976]\n",
      "   [ 1.0947822 ]\n",
      "   ...\n",
      "   [-1.8651845 ]\n",
      "   [-2.0273745 ]\n",
      "   [-2.1490169 ]]\n",
      "\n",
      "  [[ 0.8920448 ]\n",
      "   [ 0.97313976]\n",
      "   [ 1.0542347 ]\n",
      "   ...\n",
      "   [-1.8651845 ]\n",
      "   [-2.0273745 ]\n",
      "   [-2.1490169 ]]\n",
      "\n",
      "  [[ 0.8920448 ]\n",
      "   [ 0.9325923 ]\n",
      "   [ 0.97313976]\n",
      "   ...\n",
      "   [-1.905732  ]\n",
      "   [-2.0679219 ]\n",
      "   [-2.1490169 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.9325923 ]\n",
      "   [ 0.8109498 ]\n",
      "   [ 0.68930733]\n",
      "   ...\n",
      "   [-1.7029946 ]\n",
      "   [-1.7029946 ]\n",
      "   [-1.6624471 ]]\n",
      "\n",
      "  [[ 0.8514973 ]\n",
      "   [ 0.7704023 ]\n",
      "   [ 0.68930733]\n",
      "   ...\n",
      "   [-1.7029946 ]\n",
      "   [-1.6624471 ]\n",
      "   [-1.7029946 ]]\n",
      "\n",
      "  [[ 0.7298548 ]\n",
      "   [ 0.68930733]\n",
      "   [ 0.64875984]\n",
      "   ...\n",
      "   [-1.6218996 ]\n",
      "   [-1.6624471 ]\n",
      "   [-1.7029946 ]]]\n",
      "\n",
      "\n",
      " [[[ 0.8514973 ]\n",
      "   [ 0.8920448 ]\n",
      "   [ 0.97313976]\n",
      "   ...\n",
      "   [-1.4191622 ]\n",
      "   [-1.6218996 ]\n",
      "   [-1.7435421 ]]\n",
      "\n",
      "  [[ 0.8514973 ]\n",
      "   [ 0.9325923 ]\n",
      "   [ 0.97313976]\n",
      "   ...\n",
      "   [-1.5002571 ]\n",
      "   [-1.6624471 ]\n",
      "   [-1.7840896 ]]\n",
      "\n",
      "  [[ 0.8920448 ]\n",
      "   [ 0.9325923 ]\n",
      "   [ 0.97313976]\n",
      "   ...\n",
      "   [-1.5408046 ]\n",
      "   [-1.7029946 ]\n",
      "   [-1.7840896 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.68930733]\n",
      "   [ 0.68930733]\n",
      "   [ 0.64875984]\n",
      "   ...\n",
      "   [-1.6624471 ]\n",
      "   [-1.6624471 ]\n",
      "   [-1.6624471 ]]\n",
      "\n",
      "  [[ 0.68930733]\n",
      "   [ 0.68930733]\n",
      "   [ 0.68930733]\n",
      "   ...\n",
      "   [-1.7029946 ]\n",
      "   [-1.7029946 ]\n",
      "   [-1.7029946 ]]\n",
      "\n",
      "  [[ 0.68930733]\n",
      "   [ 0.68930733]\n",
      "   [ 0.68930733]\n",
      "   ...\n",
      "   [-1.7029946 ]\n",
      "   [-1.7029946 ]\n",
      "   [-1.7435421 ]]]\n",
      "\n",
      "\n",
      " [[[ 0.56766486]\n",
      "   [ 0.60821235]\n",
      "   [ 0.68930733]\n",
      "   ...\n",
      "   [-0.9325923 ]\n",
      "   [-1.1353297 ]\n",
      "   [-1.2569722 ]]\n",
      "\n",
      "  [[ 0.56766486]\n",
      "   [ 0.64875984]\n",
      "   [ 0.7298548 ]\n",
      "   ...\n",
      "   [-0.97313976]\n",
      "   [-1.2164247 ]\n",
      "   [-1.3380672 ]]\n",
      "\n",
      "  [[ 0.56766486]\n",
      "   [ 0.60821235]\n",
      "   [ 0.68930733]\n",
      "   ...\n",
      "   [-1.0947822 ]\n",
      "   [-1.2569722 ]\n",
      "   [-1.3786147 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.64875984]\n",
      "   [ 0.60821235]\n",
      "   [ 0.60821235]\n",
      "   ...\n",
      "   [-1.6624471 ]\n",
      "   [-1.6218996 ]\n",
      "   [-1.5813521 ]]\n",
      "\n",
      "  [[ 0.64875984]\n",
      "   [ 0.60821235]\n",
      "   [ 0.60821235]\n",
      "   ...\n",
      "   [-1.6218996 ]\n",
      "   [-1.6218996 ]\n",
      "   [-1.5813521 ]]\n",
      "\n",
      "  [[ 0.64875984]\n",
      "   [ 0.64875984]\n",
      "   [ 0.64875984]\n",
      "   ...\n",
      "   [-1.5408046 ]\n",
      "   [-1.5813521 ]\n",
      "   [-1.5813521 ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[-0.8920448 ]\n",
      "   [-0.8920448 ]\n",
      "   [-0.8514973 ]\n",
      "   ...\n",
      "   [-0.08109498]\n",
      "   [-0.12164247]\n",
      "   [-0.16218996]]\n",
      "\n",
      "  [[-0.8920448 ]\n",
      "   [-0.8514973 ]\n",
      "   [-0.8514973 ]\n",
      "   ...\n",
      "   [-0.08109498]\n",
      "   [-0.12164247]\n",
      "   [-0.16218996]]\n",
      "\n",
      "  [[-0.8109498 ]\n",
      "   [-0.8109498 ]\n",
      "   [-0.8109498 ]\n",
      "   ...\n",
      "   [-0.08109498]\n",
      "   [-0.12164247]\n",
      "   [-0.16218996]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.64875984]\n",
      "   [-0.64875984]\n",
      "   [-0.64875984]\n",
      "   ...\n",
      "   [-0.32437992]\n",
      "   [-0.32437992]\n",
      "   [-0.28383243]]\n",
      "\n",
      "  [[-0.68930733]\n",
      "   [-0.68930733]\n",
      "   [-0.68930733]\n",
      "   ...\n",
      "   [-0.32437992]\n",
      "   [-0.32437992]\n",
      "   [-0.32437992]]\n",
      "\n",
      "  [[-0.68930733]\n",
      "   [-0.68930733]\n",
      "   [-0.64875984]\n",
      "   ...\n",
      "   [-0.24328494]\n",
      "   [-0.28383243]\n",
      "   [-0.32437992]]]\n",
      "\n",
      "\n",
      " [[[-0.8514973 ]\n",
      "   [-0.8514973 ]\n",
      "   [-0.8109498 ]\n",
      "   ...\n",
      "   [ 0.04054749]\n",
      "   [-0.04054749]\n",
      "   [-0.08109498]]\n",
      "\n",
      "  [[-0.8920448 ]\n",
      "   [-0.8514973 ]\n",
      "   [-0.8109498 ]\n",
      "   ...\n",
      "   [ 0.04054749]\n",
      "   [-0.04054749]\n",
      "   [-0.08109498]]\n",
      "\n",
      "  [[-0.8514973 ]\n",
      "   [-0.8109498 ]\n",
      "   [-0.8109498 ]\n",
      "   ...\n",
      "   [ 0.04054749]\n",
      "   [-0.04054749]\n",
      "   [-0.04054749]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.68930733]\n",
      "   [-0.64875984]\n",
      "   [-0.60821235]\n",
      "   ...\n",
      "   [-0.3649274 ]\n",
      "   [-0.3649274 ]\n",
      "   [-0.3649274 ]]\n",
      "\n",
      "  [[-0.64875984]\n",
      "   [-0.64875984]\n",
      "   [-0.60821235]\n",
      "   ...\n",
      "   [-0.3649274 ]\n",
      "   [-0.3649274 ]\n",
      "   [-0.4054749 ]]\n",
      "\n",
      "  [[-0.60821235]\n",
      "   [-0.60821235]\n",
      "   [-0.56766486]\n",
      "   ...\n",
      "   [-0.3649274 ]\n",
      "   [-0.3649274 ]\n",
      "   [-0.3649274 ]]]\n",
      "\n",
      "\n",
      " [[[-0.97313976]\n",
      "   [-0.97313976]\n",
      "   [-0.9325923 ]\n",
      "   ...\n",
      "   [-0.04054749]\n",
      "   [-0.12164247]\n",
      "   [-0.16218996]]\n",
      "\n",
      "  [[-1.0136873 ]\n",
      "   [-0.97313976]\n",
      "   [-0.97313976]\n",
      "   ...\n",
      "   [-0.12164247]\n",
      "   [-0.16218996]\n",
      "   [-0.20273745]]\n",
      "\n",
      "  [[-0.9325923 ]\n",
      "   [-0.9325923 ]\n",
      "   [-0.9325923 ]\n",
      "   ...\n",
      "   [-0.20273745]\n",
      "   [-0.24328494]\n",
      "   [-0.24328494]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.7298548 ]\n",
      "   [-0.68930733]\n",
      "   [-0.64875984]\n",
      "   ...\n",
      "   [-0.3649274 ]\n",
      "   [-0.3649274 ]\n",
      "   [-0.4054749 ]]\n",
      "\n",
      "  [[-0.8109498 ]\n",
      "   [-0.7704023 ]\n",
      "   [-0.64875984]\n",
      "   ...\n",
      "   [-0.4054749 ]\n",
      "   [-0.4054749 ]\n",
      "   [-0.4054749 ]]\n",
      "\n",
      "  [[-0.8920448 ]\n",
      "   [-0.8109498 ]\n",
      "   [-0.64875984]\n",
      "   ...\n",
      "   [-0.4054749 ]\n",
      "   [-0.4054749 ]\n",
      "   [-0.4054749 ]]]], shape=(68, 40, 120, 1), dtype=float32)\n",
      "120\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "b323b3673a461c65",
   "metadata": {},
   "source": [
    "## reading data"
   ]
  },
  {
   "cell_type": "code",
   "id": "15d040463f02dc61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T19:42:31.059439Z",
     "start_time": "2024-07-06T19:42:30.888137Z"
    }
   },
   "source": [
    "# reading all files within the root directory\n",
    "# data = tf.data.Dataset.list_files('A:\\Lip Reading\\Potential Datasets\\BBC LRS2\\mvlrs_v1\\main\\*\\*.mp4')\n",
    "data = tf.data.Dataset.list_files('A:/Lip Reading/Potential Datasets/BBC LRS2/allFiles2/*.mp4')\n",
    "# data = tf.data.Dataset.list_files('A:/Lip Reading/Potential Datasets/BBC LRS2/trainFiles6/*.mp4')\n",
    "\n",
    "global maxCharCt\n",
    "\n",
    "data = data.shuffle(500, reshuffle_each_iteration=False) # shuffling data\n",
    "data = data.map(processData) # \"processing\" the data to obtain frames and the respective text \n",
    "\n",
    "dim1 = newImageSize[0]\n",
    "dim2 = newImageSize[1]\n",
    "print(\"dataset size before padding:\", len(data))\n",
    "print(\"data shape of example frame:\", newImageSize)\n",
    "print(\"data shape of example video:\", frames.shape)\n",
    "print(\"dims: \",dim1, \"x\",dim2)\n",
    "\n",
    "# combining 8 videos as one \"input\"\n",
    "# ensuring all videos are padded to match the longest video, \n",
    "# ensuring the length of all the alignments is the size of the longest text characters, as some are lower. \n",
    "batchSize = 4\n",
    "data = data.padded_batch(batchSize, padded_shapes=([maxFrameCt,None, None,None], [maxCharCt])) \n",
    "print(\"autotune: \",tf.data.AUTOTUNE)\n",
    "data = data.prefetch(tf.data.AUTOTUNE)\n",
    "# data=data.prefetch(3)\n",
    "print(\"data length after padding:\", len(data))\n",
    "print(\"batch size:\", batchSize)\n",
    "\n",
    "train = data.take(int(len(data) * 0.7))\n",
    "test = data.skip(int(len(data) * 0.7))\n",
    "print(\"train data size:\", len(train))\n",
    "print(\"test data size:\",  len(test))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size before padding: 3783\n",
      "data shape of example frame: (40, 120)\n",
      "data shape of example video: (68, 40, 120, 1)\n",
      "dims:  40 x 120\n",
      "autotune:  -1\n",
      "data length after padding: 946\n",
      "batch size: 4\n",
      "train data size: 662\n",
      "test data size: 284\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "b98206929338ffa0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T19:42:35.014414Z",
     "start_time": "2024-07-06T19:42:35.005411Z"
    }
   },
   "source": [
    "data"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 290, None, None, None), dtype=tf.float32, name=None), TensorSpec(shape=(None, 145), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "771515f5cef4617f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T19:42:37.094757Z",
     "start_time": "2024-07-06T19:42:35.605322Z"
    }
   },
   "source": [
    "val = data.as_numpy_iterator().next()\n",
    "plt.imshow(val[0][0][2], cmap='gray_r')\n",
    "print(len(val[0][0]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "e1b71a67fbe998bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T19:42:38.281242Z",
     "start_time": "2024-07-06T19:42:37.942410Z"
    }
   },
   "source": [
    "print(tf.strings.reduce_join([numToChar(word) for word in val[1][0]]))\n",
    "print(\"num of chars:\", len(([numToChar(word) for word in val[1][0]])))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'THEY THINK I SWAN ROUND MAYFAIR SPENDING RICH PEOPLES MONEY', shape=(), dtype=string)\n",
      "num of chars: 145\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "7b9fe2e89759a25",
   "metadata": {},
   "source": [
    "## designing the model"
   ]
  },
  {
   "cell_type": "code",
   "id": "44de38608ebec792",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T19:42:40.314285Z",
     "start_time": "2024-07-06T19:42:40.300764Z"
    }
   },
   "source": [
    "# imports for the model architecture \n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv3D, LSTM, Dense, Dropout, Bidirectional, MaxPooling3D, TimeDistributed, Flatten, GRU\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, Nadam, SGD\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, LearningRateScheduler"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "a4e35802d732d3c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T19:42:42.720610Z",
     "start_time": "2024-07-06T19:42:40.803229Z"
    }
   },
   "source": [
    "inputShape = data.as_numpy_iterator().next()[0][0].shape\n",
    "print(inputShape)\n",
    "print(charToNum.get_vocabulary())\n",
    "print(len(charToNum.get_vocabulary()))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(290, 40, 120, 1)\n",
      "['', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', ' ']\n",
      "28\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "84706072692f52d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T02:21:30.215344Z",
     "start_time": "2024-07-08T02:21:30.192216Z"
    }
   },
   "source": [
    "# model to be actually trained\n",
    "def createModel(x): \n",
    "    model = Sequential([\n",
    "    Conv3D(32, 3, input_shape=inputShape, padding='same', activation='relu', name=\"conv1\"),\n",
    "    MaxPooling3D((1,2,2), name=\"maxPool1\"),\n",
    "    \n",
    "    Conv3D(64, 3, padding='same', activation='relu', name=\"conv2\"),\n",
    "    MaxPooling3D((1,2,2), name=\"maxPool2\"),\n",
    "        \n",
    "    Conv3D(96, 3,  padding='same', activation='relu', name=\"conv3\"),\n",
    "    MaxPooling3D((1,2,2), name=\"maxPool3\"),\n",
    "    \n",
    "    TimeDistributed(Flatten()),\n",
    "    \n",
    "    # Bidirectional(LSTM(256, kernel_initializer='orthogonal', return_sequences=True)),\n",
    "    Bidirectional(GRU(x, kernel_initializer='orthogonal', return_sequences=True)),\n",
    "    Dropout(.5),\n",
    "        \n",
    "    # Bidirectional(LSTM(256, kernel_initializer='orthogonal' , return_sequences=True)),\n",
    "    Bidirectional(GRU(x, kernel_initializer='orthogonal' , return_sequences=True)),\n",
    "    Dropout(.5),\n",
    "    \n",
    "    Dense(charToNum.vocabulary_size()+1, kernel_initializer='he_normal', activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "# model to be actually trained\n",
    "def createModel2(x): \n",
    "    model = Sequential([\n",
    "    Conv3D(32, 3, strides = (1,2,2),input_shape=inputShape, padding='same', activation='relu', name=\"conv1\"),\n",
    "    MaxPooling3D((1,2,2),strides = (1,2,2), name=\"maxPool1\"),\n",
    "    \n",
    "    Conv3D(64, 3, strides = (1,2,2), padding='same', activation='relu', name=\"conv2\"),\n",
    "    MaxPooling3D((1,2,2),strides = (1,2,2), name=\"maxPool2\"),\n",
    "        \n",
    "    Conv3D(96, 3,  padding='same',activation='relu', name=\"conv3\"),\n",
    "    MaxPooling3D((1,2,2), name=\"maxPool3\"),\n",
    "    \n",
    "    TimeDistributed(Flatten()),\n",
    "    \n",
    "    # Bidirectional(LSTM(256, kernel_initializer='orthogonal', return_sequences=True)),\n",
    "    Bidirectional(GRU(x, kernel_initializer='orthogonal', return_sequences=True)),\n",
    "    Dropout(.5),\n",
    "        \n",
    "    # Bidirectional(LSTM(256, kernel_initializer='orthogonal' , return_sequences=True)),\n",
    "    Bidirectional(GRU(x, kernel_initializer='orthogonal' , return_sequences=True)),\n",
    "    Dropout(.5),\n",
    "    \n",
    "    Dense(charToNum.vocabulary_size()+1, kernel_initializer='he_normal', activation='softmax')\n",
    "    ])\n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "d728bc2771721d6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T02:21:33.732113Z",
     "start_time": "2024-07-08T02:21:30.948175Z"
    }
   },
   "source": [
    "model2 = createModel(256)\n",
    "print(\"final model input shape:\",  inputShape)\n",
    "model2.summary()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final model input shape: (290, 40, 120, 1)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1 (Conv3D)              (None, 290, 40, 120, 32)  896       \n",
      "                                                                 \n",
      " maxPool1 (MaxPooling3D)     (None, 290, 20, 60, 32)   0         \n",
      "                                                                 \n",
      " conv2 (Conv3D)              (None, 290, 20, 60, 64)   55360     \n",
      "                                                                 \n",
      " maxPool2 (MaxPooling3D)     (None, 290, 10, 30, 64)   0         \n",
      "                                                                 \n",
      " conv3 (Conv3D)              (None, 290, 10, 30, 96)   165984    \n",
      "                                                                 \n",
      " maxPool3 (MaxPooling3D)     (None, 290, 5, 15, 96)    0         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 290, 7200)        0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 290, 512)         11455488  \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 290, 512)          0         \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 290, 512)         1182720   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 290, 512)          0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 290, 29)           14877     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,875,325\n",
      "Trainable params: 12,875,325\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "9cd1b05996f23f8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T04:24:21.444226Z",
     "start_time": "2024-07-10T04:24:21.421203Z"
    }
   },
   "source": [
    "# custom functions \n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 5:\n",
    "        return lr        \n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "    \n",
    "def scheduler2(epoch, lr):\n",
    "    return lr * tf.math.exp(-0.1)\n",
    "# custom loss function \n",
    "def CTCLoss(yTrue, yPred):\n",
    "    # y true is the text alignment (None, 99) \n",
    "    # y pred is the end result of the model (154, 41) \n",
    "    batchLen = tf.cast(tf.shape(yTrue)[0], dtype=\"int64\")\n",
    "\n",
    "    inputLen = tf.cast(tf.shape(yPred)[1], dtype=\"int64\")\n",
    "    labelLen = tf.cast(tf.shape(yTrue)[1], dtype=\"int64\")\n",
    "    inputLen = inputLen * tf.ones(shape=(batchLen, 1), dtype=\"int64\")\n",
    "    labelLen = labelLen * tf.ones(shape=(batchLen, 1), dtype=\"int64\")\n",
    "\n",
    "    loss = tf.keras.backend.ctc_batch_cost(yTrue, yPred, inputLen, labelLen)   \n",
    "    return loss \n",
    "\n",
    "class ProduceExample(tf.keras.callbacks.Callback): \n",
    "    def __init__(self, dataset) -> None: \n",
    "        self.dataset = dataset.as_numpy_iterator()\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None) -> None:\n",
    "        try: \n",
    "            data = self.dataset.next()\n",
    "            yhat = self.model.predict(data[0])\n",
    "       \n",
    "            decoded = tf.keras.backend.ctc_decode(yhat, [maxFrameCt, maxFrameCt, maxFrameCt, maxFrameCt], greedy=False)[0][0].numpy()\n",
    "            for x in range(len(yhat)):           \n",
    "                print('Original:', tf.strings.reduce_join(numToChar(data[1][x])).numpy().decode('utf-8'))\n",
    "                print('Prediction:', tf.strings.reduce_join(numToChar(decoded[x])).numpy().decode('utf-8'))\n",
    "                print(\"Word Error Rate: \", str(wer(tf.strings.reduce_join(numToChar(data[1][x])).numpy().decode('utf-8'), tf.strings.reduce_join(numToChar(decoded[x])).numpy().decode('utf-8') ) * 100) + \"%\")\n",
    "                print('~'*100)\n",
    "        except: \n",
    "            pass     \n",
    "      "
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "id": "d5afa6ac776aca30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T02:21:36.549960Z",
     "start_time": "2024-07-08T02:21:36.541345Z"
    }
   },
   "source": [
    "def makePrediction(model, valuePassed=None): \n",
    "    if valuePassed is None: \n",
    "        value = val\n",
    "    else: \n",
    "        value = valuePassed\n",
    "    yHat = model.predict(value[0])\n",
    "    \n",
    "    decoded = tf.keras.backend.ctc_decode(yHat, [maxFrameCt, maxFrameCt, maxFrameCt, maxFrameCt], greedy=False)[0][0].numpy()\n",
    "    originalArr = []\n",
    "    predArr = []\n",
    "    for x in range(len(yHat)):          \n",
    "        original = tf.strings.reduce_join(numToChar(val[1][x])).numpy().decode('utf-8')\n",
    "        prediction = tf.strings.reduce_join(numToChar(decoded[x])).numpy().decode('utf-8')\n",
    "        originalArr.append(original)\n",
    "        predArr.append(prediction )\n",
    "        \n",
    "        print('Original:', original)\n",
    "        print('Prediction:', prediction)\n",
    "        print(\"Word Error Rate on Prediction:\", str(wer(original,prediction) * 100) + \"%\")\n",
    "        print('~' * 40)\n",
    "    print(\"Avg Word Error Rate:\", str(wer(originalArr, predArr) * 100) + \"%\")\n",
    "    \n",
    "    return value, yHat, decoded"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T02:21:39.723836Z",
     "start_time": "2024-07-08T02:21:37.167123Z"
    }
   },
   "cell_type": "code",
   "source": "makePrediction(model2)",
   "id": "18278119fccacab6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "Original: THEY THINK I SWAN ROUND MAYFAIR SPENDING RICH PEOPLES MONEY\n",
      "Prediction: UJUJUJUJUWUJUJUJWJWJUJUJUJUJUJO O O H H FEDBIUMBIUMBIUMBIUMBIUMBIUMBIUMBIUMBIUMBIUMBIUMBIUMBIUMBIUMBIUMBIH\n",
      "Word Error Rate on Prediction: 100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: HE WAS VERY HAPPY\n",
      "Prediction: JUJUJWJWJWJWJWJO O X S BHOSNSNSNSNSNSNSNSNSNSNSNSNSNSNSNSNSNSNSNSNSNSNSNSNSNSNSNSNSNSNSNSNSNSNSNSNSNSN\n",
      "Word Error Rate on Prediction: 125.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: DO I HAVE A MOBILE\n",
      "Prediction: UJIWUWUWUWIWC X X H TKTMJNJNJNJNJNJNJNJNJNJNJNJNJNJNJNJNJNJNJNJNJNJNJNJNJNJNJNJNJNJNJNJNJNJNJNJNJNJNY\n",
      "Word Error Rate on Prediction: 100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: CHRIS IS STAYING IN AUSTRALIA\n",
      "Prediction: UCUCUQUIUIUIUIUIUIUW X X H UXZCFBFBFBFBFBFBFBFBFBFBFBFBFBFBFBFBFBFBFBFBFBFBFBFBFBFBFBFBFBFBFBFBFBFBFBFC\n",
      "Word Error Rate on Prediction: 100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Avg Word Error Rate: 104.16666666666667%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((array([[[[[-0.06988747],\n",
       "            [ 0.10483121],\n",
       "            [ 0.31449363],\n",
       "            ...,\n",
       "            [ 1.2579745 ],\n",
       "            [ 1.0832559 ],\n",
       "            [ 0.90853715]],\n",
       "  \n",
       "           [[-0.03494374],\n",
       "            [ 0.2795499 ],\n",
       "            [ 0.4892123 ],\n",
       "            ...,\n",
       "            [ 1.2230308 ],\n",
       "            [ 1.0483121 ],\n",
       "            [ 0.87359345]],\n",
       "  \n",
       "           [[ 0.13977495],\n",
       "            [ 0.10483121],\n",
       "            [ 0.34943736],\n",
       "            ...,\n",
       "            [ 1.0133684 ],\n",
       "            [ 0.90853715],\n",
       "            [ 0.76876223]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 1.2929182 ],\n",
       "            [ 1.2230308 ],\n",
       "            [ 1.1531433 ],\n",
       "            ...,\n",
       "            [ 0.59404355],\n",
       "            [ 0.5590998 ],\n",
       "            [ 0.5590998 ]],\n",
       "  \n",
       "           [[ 1.2230308 ],\n",
       "            [ 1.1531433 ],\n",
       "            [ 1.0832559 ],\n",
       "            ...,\n",
       "            [ 0.59404355],\n",
       "            [ 0.5590998 ],\n",
       "            [ 0.5590998 ]],\n",
       "  \n",
       "           [[ 1.2230308 ],\n",
       "            [ 1.1531433 ],\n",
       "            [ 1.1181996 ],\n",
       "            ...,\n",
       "            [ 0.4892123 ],\n",
       "            [ 0.4892123 ],\n",
       "            [ 0.52415603]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.10483121],\n",
       "            [ 0.20966242],\n",
       "            [ 0.17471868],\n",
       "            ...,\n",
       "            [ 0.87359345],\n",
       "            [ 0.62898725],\n",
       "            [ 0.5590998 ]],\n",
       "  \n",
       "           [[ 0.38438112],\n",
       "            [ 0.45426857],\n",
       "            [ 0.4892123 ],\n",
       "            ...,\n",
       "            [ 0.8386497 ],\n",
       "            [ 0.62898725],\n",
       "            [ 0.5590998 ]],\n",
       "  \n",
       "           [[ 0.4892123 ],\n",
       "            [ 0.59404355],\n",
       "            [ 0.663931  ],\n",
       "            ...,\n",
       "            [ 0.80370593],\n",
       "            [ 0.59404355],\n",
       "            [ 0.5590998 ]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 1.1531433 ],\n",
       "            [ 1.0832559 ],\n",
       "            [ 1.0483121 ],\n",
       "            ...,\n",
       "            [ 0.45426857],\n",
       "            [ 0.38438112],\n",
       "            [ 0.34943736]],\n",
       "  \n",
       "           [[ 1.1181996 ],\n",
       "            [ 1.0832559 ],\n",
       "            [ 1.0832559 ],\n",
       "            ...,\n",
       "            [ 0.5590998 ],\n",
       "            [ 0.52415603],\n",
       "            [ 0.4892123 ]],\n",
       "  \n",
       "           [[ 1.0832559 ],\n",
       "            [ 1.0832559 ],\n",
       "            [ 1.1181996 ],\n",
       "            ...,\n",
       "            [ 0.59404355],\n",
       "            [ 0.59404355],\n",
       "            [ 0.5590998 ]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.17471868],\n",
       "            [ 0.31449363],\n",
       "            [ 0.31449363],\n",
       "            ...,\n",
       "            [ 0.9784246 ],\n",
       "            [ 0.76876223],\n",
       "            [ 0.5590998 ]],\n",
       "  \n",
       "           [[ 0.45426857],\n",
       "            [ 0.59404355],\n",
       "            [ 0.62898725],\n",
       "            ...,\n",
       "            [ 0.90853715],\n",
       "            [ 0.7338185 ],\n",
       "            [ 0.59404355]],\n",
       "  \n",
       "           [[ 0.38438112],\n",
       "            [ 0.5590998 ],\n",
       "            [ 0.663931  ],\n",
       "            ...,\n",
       "            [ 0.87359345],\n",
       "            [ 0.663931  ],\n",
       "            [ 0.5590998 ]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 1.1531433 ],\n",
       "            [ 1.0832559 ],\n",
       "            [ 1.0483121 ],\n",
       "            ...,\n",
       "            [ 0.4892123 ],\n",
       "            [ 0.38438112],\n",
       "            [ 0.2795499 ]],\n",
       "  \n",
       "           [[ 1.1181996 ],\n",
       "            [ 1.0832559 ],\n",
       "            [ 1.0832559 ],\n",
       "            ...,\n",
       "            [ 0.5590998 ],\n",
       "            [ 0.4892123 ],\n",
       "            [ 0.38438112]],\n",
       "  \n",
       "           [[ 1.0832559 ],\n",
       "            [ 1.0832559 ],\n",
       "            [ 1.1181996 ],\n",
       "            ...,\n",
       "            [ 0.5590998 ],\n",
       "            [ 0.52415603],\n",
       "            [ 0.4892123 ]]],\n",
       "  \n",
       "  \n",
       "          ...,\n",
       "  \n",
       "  \n",
       "          [[[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "         [[[[ 0.07049421],\n",
       "            [ 0.03524711],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [-0.17623554],\n",
       "            [-0.21148264],\n",
       "            [-0.21148264]],\n",
       "  \n",
       "           [[ 0.10574132],\n",
       "            [ 0.07049421],\n",
       "            [ 0.03524711],\n",
       "            ...,\n",
       "            [-0.21148264],\n",
       "            [-0.21148264],\n",
       "            [-0.24672975]],\n",
       "  \n",
       "           [[ 0.10574132],\n",
       "            [ 0.10574132],\n",
       "            [ 0.10574132],\n",
       "            ...,\n",
       "            [-0.24672975],\n",
       "            [-0.24672975],\n",
       "            [-0.28197685]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 0.07049421],\n",
       "            [ 0.07049421],\n",
       "            [ 0.07049421],\n",
       "            ...,\n",
       "            [-0.9164248 ],\n",
       "            [-1.0221661 ],\n",
       "            [-1.0574132 ]],\n",
       "  \n",
       "           [[ 0.03524711],\n",
       "            [ 0.03524711],\n",
       "            [ 0.03524711],\n",
       "            ...,\n",
       "            [-0.986919  ],\n",
       "            [-1.0574132 ],\n",
       "            [-1.1279074 ]],\n",
       "  \n",
       "           [[-0.03524711],\n",
       "            [-0.03524711],\n",
       "            [-0.03524711],\n",
       "            ...,\n",
       "            [-1.0926603 ],\n",
       "            [-1.0926603 ],\n",
       "            [-1.0926603 ]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.14098842],\n",
       "            [ 0.10574132],\n",
       "            [ 0.10574132],\n",
       "            ...,\n",
       "            [-0.17623554],\n",
       "            [-0.24672975],\n",
       "            [-0.31722397]],\n",
       "  \n",
       "           [[ 0.17623554],\n",
       "            [ 0.10574132],\n",
       "            [ 0.10574132],\n",
       "            ...,\n",
       "            [-0.21148264],\n",
       "            [-0.24672975],\n",
       "            [-0.28197685]],\n",
       "  \n",
       "           [[ 0.17623554],\n",
       "            [ 0.14098842],\n",
       "            [ 0.10574132],\n",
       "            ...,\n",
       "            [-0.21148264],\n",
       "            [-0.24672975],\n",
       "            [-0.28197685]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 0.07049421],\n",
       "            [ 0.07049421],\n",
       "            [ 0.07049421],\n",
       "            ...,\n",
       "            [-0.986919  ],\n",
       "            [-1.0221661 ],\n",
       "            [-1.0221661 ]],\n",
       "  \n",
       "           [[ 0.03524711],\n",
       "            [ 0.03524711],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [-1.0221661 ],\n",
       "            [-1.0926603 ],\n",
       "            [-1.1631546 ]],\n",
       "  \n",
       "           [[ 0.03524711],\n",
       "            [ 0.        ],\n",
       "            [-0.03524711],\n",
       "            ...,\n",
       "            [-1.0926603 ],\n",
       "            [-1.1631546 ],\n",
       "            [-1.2336488 ]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.24672975],\n",
       "            [ 0.4934595 ],\n",
       "            [ 0.4934595 ],\n",
       "            ...,\n",
       "            [-0.17623554],\n",
       "            [-0.24672975],\n",
       "            [-0.24672975]],\n",
       "  \n",
       "           [[ 0.17623554],\n",
       "            [ 0.35247108],\n",
       "            [ 0.31722397],\n",
       "            ...,\n",
       "            [-0.21148264],\n",
       "            [-0.17623554],\n",
       "            [-0.21148264]],\n",
       "  \n",
       "           [[ 0.10574132],\n",
       "            [ 0.24672975],\n",
       "            [ 0.24672975],\n",
       "            ...,\n",
       "            [-0.21148264],\n",
       "            [-0.21148264],\n",
       "            [-0.21148264]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 0.03524711],\n",
       "            [ 0.03524711],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [-0.8459306 ],\n",
       "            [-0.9164248 ],\n",
       "            [-0.9164248 ]],\n",
       "  \n",
       "           [[ 0.03524711],\n",
       "            [ 0.03524711],\n",
       "            [-0.03524711],\n",
       "            ...,\n",
       "            [-0.88117766],\n",
       "            [-0.986919  ],\n",
       "            [-1.0221661 ]],\n",
       "  \n",
       "           [[ 0.03524711],\n",
       "            [ 0.03524711],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [-0.9164248 ],\n",
       "            [-1.0221661 ],\n",
       "            [-1.1279074 ]]],\n",
       "  \n",
       "  \n",
       "          ...,\n",
       "  \n",
       "  \n",
       "          [[[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "         [[[[-2.6318436 ],\n",
       "            [-2.6318436 ],\n",
       "            [-2.707039  ],\n",
       "            ...,\n",
       "            [ 0.37597767],\n",
       "            [ 0.37597767],\n",
       "            [ 0.45117322]],\n",
       "  \n",
       "           [[-2.6318436 ],\n",
       "            [-2.6318436 ],\n",
       "            [-2.707039  ],\n",
       "            ...,\n",
       "            [ 0.37597767],\n",
       "            [ 0.37597767],\n",
       "            [ 0.45117322]],\n",
       "  \n",
       "           [[-2.6318436 ],\n",
       "            [-2.6318436 ],\n",
       "            [-2.6318436 ],\n",
       "            ...,\n",
       "            [ 0.37597767],\n",
       "            [ 0.37597767],\n",
       "            [ 0.45117322]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-3.0830169 ],\n",
       "            [-3.0830169 ],\n",
       "            [-3.0830169 ],\n",
       "            ...,\n",
       "            [ 0.22558661],\n",
       "            [ 0.07519554],\n",
       "            [ 0.07519554]],\n",
       "  \n",
       "           [[-3.3086035 ],\n",
       "            [-3.3086035 ],\n",
       "            [-3.233408  ],\n",
       "            ...,\n",
       "            [ 0.30078214],\n",
       "            [ 0.07519554],\n",
       "            [-0.07519554]],\n",
       "  \n",
       "           [[-3.383799  ],\n",
       "            [-3.383799  ],\n",
       "            [-3.3086035 ],\n",
       "            ...,\n",
       "            [ 0.30078214],\n",
       "            [ 0.07519554],\n",
       "            [-0.15039107]]],\n",
       "  \n",
       "  \n",
       "          [[[-2.6318436 ],\n",
       "            [-2.6318436 ],\n",
       "            [-2.707039  ],\n",
       "            ...,\n",
       "            [ 0.45117322],\n",
       "            [ 0.45117322],\n",
       "            [ 0.45117322]],\n",
       "  \n",
       "           [[-2.6318436 ],\n",
       "            [-2.6318436 ],\n",
       "            [-2.707039  ],\n",
       "            ...,\n",
       "            [ 0.37597767],\n",
       "            [ 0.37597767],\n",
       "            [ 0.45117322]],\n",
       "  \n",
       "           [[-2.6318436 ],\n",
       "            [-2.6318436 ],\n",
       "            [-2.6318436 ],\n",
       "            ...,\n",
       "            [ 0.37597767],\n",
       "            [ 0.37597767],\n",
       "            [ 0.45117322]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-2.9326258 ],\n",
       "            [-3.0078213 ],\n",
       "            [-3.0830169 ],\n",
       "            ...,\n",
       "            [ 0.30078214],\n",
       "            [ 0.15039107],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[-3.3086035 ],\n",
       "            [-3.3086035 ],\n",
       "            [-3.233408  ],\n",
       "            ...,\n",
       "            [ 0.37597767],\n",
       "            [ 0.15039107],\n",
       "            [-0.07519554]],\n",
       "  \n",
       "           [[-3.383799  ],\n",
       "            [-3.383799  ],\n",
       "            [-3.3086035 ],\n",
       "            ...,\n",
       "            [ 0.45117322],\n",
       "            [ 0.07519554],\n",
       "            [-0.15039107]]],\n",
       "  \n",
       "  \n",
       "          [[[-2.5566483 ],\n",
       "            [-2.6318436 ],\n",
       "            [-2.6318436 ],\n",
       "            ...,\n",
       "            [ 0.45117322],\n",
       "            [ 0.45117322],\n",
       "            [ 0.45117322]],\n",
       "  \n",
       "           [[-2.5566483 ],\n",
       "            [-2.6318436 ],\n",
       "            [-2.6318436 ],\n",
       "            ...,\n",
       "            [ 0.37597767],\n",
       "            [ 0.37597767],\n",
       "            [ 0.45117322]],\n",
       "  \n",
       "           [[-2.5566483 ],\n",
       "            [-2.6318436 ],\n",
       "            [-2.5566483 ],\n",
       "            ...,\n",
       "            [ 0.37597767],\n",
       "            [ 0.37597767],\n",
       "            [ 0.45117322]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-2.9326258 ],\n",
       "            [-3.0078213 ],\n",
       "            [-3.0078213 ],\n",
       "            ...,\n",
       "            [ 0.30078214],\n",
       "            [ 0.15039107],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[-3.3086035 ],\n",
       "            [-3.233408  ],\n",
       "            [-3.233408  ],\n",
       "            ...,\n",
       "            [ 0.37597767],\n",
       "            [ 0.15039107],\n",
       "            [-0.07519554]],\n",
       "  \n",
       "           [[-3.383799  ],\n",
       "            [-3.383799  ],\n",
       "            [-3.3086035 ],\n",
       "            ...,\n",
       "            [ 0.45117322],\n",
       "            [ 0.07519554],\n",
       "            [-0.15039107]]],\n",
       "  \n",
       "  \n",
       "          ...,\n",
       "  \n",
       "  \n",
       "          [[[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]]]],\n",
       "  \n",
       "  \n",
       "  \n",
       "         [[[[-0.24565512],\n",
       "            [-0.24565512],\n",
       "            [-0.24565512],\n",
       "            ...,\n",
       "            [ 1.2282755 ],\n",
       "            [ 1.2282755 ],\n",
       "            [ 1.2282755 ]],\n",
       "  \n",
       "           [[-0.24565512],\n",
       "            [-0.24565512],\n",
       "            [-0.29478613],\n",
       "            ...,\n",
       "            [ 1.1791445 ],\n",
       "            [ 1.1791445 ],\n",
       "            [ 1.1791445 ]],\n",
       "  \n",
       "           [[-0.24565512],\n",
       "            [-0.29478613],\n",
       "            [-0.34391716],\n",
       "            ...,\n",
       "            [ 1.1300135 ],\n",
       "            [ 1.1300135 ],\n",
       "            [ 1.1300135 ]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-0.39304817],\n",
       "            [-0.39304817],\n",
       "            [-0.4421792 ],\n",
       "            ...,\n",
       "            [ 0.5404413 ],\n",
       "            [ 0.4421792 ],\n",
       "            [ 0.39304817]],\n",
       "  \n",
       "           [[-0.39304817],\n",
       "            [-0.39304817],\n",
       "            [-0.39304817],\n",
       "            ...,\n",
       "            [ 0.49131024],\n",
       "            [ 0.34391716],\n",
       "            [ 0.29478613]],\n",
       "  \n",
       "           [[-0.34391716],\n",
       "            [-0.34391716],\n",
       "            [-0.34391716],\n",
       "            ...,\n",
       "            [ 0.4421792 ],\n",
       "            [ 0.34391716],\n",
       "            [ 0.29478613]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.24565512],\n",
       "            [-0.24565512],\n",
       "            [-0.24565512],\n",
       "            ...,\n",
       "            [ 1.2774066 ],\n",
       "            [ 1.2774066 ],\n",
       "            [ 1.2774066 ]],\n",
       "  \n",
       "           [[-0.24565512],\n",
       "            [-0.24565512],\n",
       "            [-0.29478613],\n",
       "            ...,\n",
       "            [ 1.2282755 ],\n",
       "            [ 1.2282755 ],\n",
       "            [ 1.2282755 ]],\n",
       "  \n",
       "           [[-0.19652408],\n",
       "            [-0.24565512],\n",
       "            [-0.29478613],\n",
       "            ...,\n",
       "            [ 1.2282755 ],\n",
       "            [ 1.2282755 ],\n",
       "            [ 1.2282755 ]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-0.39304817],\n",
       "            [-0.4421792 ],\n",
       "            [-0.4421792 ],\n",
       "            ...,\n",
       "            [ 0.58957225],\n",
       "            [ 0.58957225],\n",
       "            [ 0.5404413 ]],\n",
       "  \n",
       "           [[-0.39304817],\n",
       "            [-0.39304817],\n",
       "            [-0.4421792 ],\n",
       "            ...,\n",
       "            [ 0.58957225],\n",
       "            [ 0.5404413 ],\n",
       "            [ 0.49131024]],\n",
       "  \n",
       "           [[-0.39304817],\n",
       "            [-0.39304817],\n",
       "            [-0.39304817],\n",
       "            ...,\n",
       "            [ 0.58957225],\n",
       "            [ 0.49131024],\n",
       "            [ 0.4421792 ]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.24565512],\n",
       "            [-0.24565512],\n",
       "            [-0.24565512],\n",
       "            ...,\n",
       "            [ 1.3265376 ],\n",
       "            [ 1.3756686 ],\n",
       "            [ 1.4247997 ]],\n",
       "  \n",
       "           [[-0.24565512],\n",
       "            [-0.24565512],\n",
       "            [-0.29478613],\n",
       "            ...,\n",
       "            [ 1.2774066 ],\n",
       "            [ 1.3265376 ],\n",
       "            [ 1.3756686 ]],\n",
       "  \n",
       "           [[-0.24565512],\n",
       "            [-0.29478613],\n",
       "            [-0.34391716],\n",
       "            ...,\n",
       "            [ 1.2282755 ],\n",
       "            [ 1.2774066 ],\n",
       "            [ 1.2774066 ]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-0.34391716],\n",
       "            [-0.39304817],\n",
       "            [-0.39304817],\n",
       "            ...,\n",
       "            [ 0.5404413 ],\n",
       "            [ 0.5404413 ],\n",
       "            [ 0.58957225]],\n",
       "  \n",
       "           [[-0.34391716],\n",
       "            [-0.34391716],\n",
       "            [-0.39304817],\n",
       "            ...,\n",
       "            [ 0.58957225],\n",
       "            [ 0.58957225],\n",
       "            [ 0.5404413 ]],\n",
       "  \n",
       "           [[-0.34391716],\n",
       "            [-0.34391716],\n",
       "            [-0.34391716],\n",
       "            ...,\n",
       "            [ 0.6387033 ],\n",
       "            [ 0.58957225],\n",
       "            [ 0.5404413 ]]],\n",
       "  \n",
       "  \n",
       "          ...,\n",
       "  \n",
       "  \n",
       "          [[[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]],\n",
       "  \n",
       "           [[ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            ...,\n",
       "            [ 0.        ],\n",
       "            [ 0.        ],\n",
       "            [ 0.        ]]]]], dtype=float32),\n",
       "  array([[20,  8,  5, 25, 27, 20,  8,  9, 14, 11, 27,  9, 27, 19, 23,  1,\n",
       "          14, 27, 18, 15, 21, 14,  4, 27, 13,  1, 25,  6,  1,  9, 18, 27,\n",
       "          19, 16,  5, 14,  4,  9, 14,  7, 27, 18,  9,  3,  8, 27, 16,  5,\n",
       "          15, 16, 12,  5,  0, 19, 27, 13, 15, 14,  5, 25,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0],\n",
       "         [ 8,  5, 27, 23,  1, 19, 27, 22,  5, 18, 25, 27,  8,  1, 16, 16,\n",
       "          25,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0],\n",
       "         [ 4, 15, 27,  9, 27,  8,  1, 22,  5, 27,  1, 27, 13, 15,  2,  9,\n",
       "          12,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0],\n",
       "         [ 3,  8, 18,  9, 19, 27,  9, 19, 27, 19, 20,  1, 25,  9, 14,  7,\n",
       "          27,  9, 14, 27,  1, 21, 19, 20, 18,  1, 12,  9,  1,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0]], dtype=int64)),\n",
       " array([[[0.03254603, 0.03121216, 0.02903817, ..., 0.03117296,\n",
       "          0.03305664, 0.03247369],\n",
       "         [0.03357183, 0.03126372, 0.02736833, ..., 0.02952171,\n",
       "          0.0335541 , 0.03132967],\n",
       "         [0.03455492, 0.03149132, 0.02655275, ..., 0.02824589,\n",
       "          0.03383582, 0.02989348],\n",
       "         ...,\n",
       "         [0.03448276, 0.03448276, 0.03448276, ..., 0.03448276,\n",
       "          0.03448276, 0.03448276],\n",
       "         [0.03448276, 0.03448276, 0.03448276, ..., 0.03448276,\n",
       "          0.03448276, 0.03448276],\n",
       "         [0.03448276, 0.03448276, 0.03448276, ..., 0.03448276,\n",
       "          0.03448276, 0.03448276]],\n",
       " \n",
       "        [[0.03034518, 0.03078661, 0.02998208, ..., 0.03213513,\n",
       "          0.03513918, 0.03008507],\n",
       "         [0.03112479, 0.03044039, 0.02869995, ..., 0.03156534,\n",
       "          0.03519826, 0.0291063 ],\n",
       "         [0.031592  , 0.03038761, 0.02762226, ..., 0.03102079,\n",
       "          0.03471882, 0.02878223],\n",
       "         ...,\n",
       "         [0.03448276, 0.03448276, 0.03448276, ..., 0.03448276,\n",
       "          0.03448276, 0.03448276],\n",
       "         [0.03448276, 0.03448276, 0.03448276, ..., 0.03448276,\n",
       "          0.03448276, 0.03448276],\n",
       "         [0.03448276, 0.03448276, 0.03448276, ..., 0.03448276,\n",
       "          0.03448276, 0.03448276]],\n",
       " \n",
       "        [[0.03069823, 0.03316192, 0.03292695, ..., 0.03144495,\n",
       "          0.03403293, 0.03373147],\n",
       "         [0.03180513, 0.03363123, 0.03155211, ..., 0.03011872,\n",
       "          0.03513318, 0.03241281],\n",
       "         [0.03279063, 0.03429295, 0.03005248, ..., 0.02964422,\n",
       "          0.03618556, 0.03173878],\n",
       "         ...,\n",
       "         [0.03448276, 0.03448276, 0.03448276, ..., 0.03448276,\n",
       "          0.03448276, 0.03448276],\n",
       "         [0.03448276, 0.03448276, 0.03448276, ..., 0.03448276,\n",
       "          0.03448276, 0.03448276],\n",
       "         [0.03448276, 0.03448276, 0.03448276, ..., 0.03448276,\n",
       "          0.03448276, 0.03448276]],\n",
       " \n",
       "        [[0.03062315, 0.03326158, 0.03421452, ..., 0.02982099,\n",
       "          0.03366231, 0.03205765],\n",
       "         [0.03134162, 0.03355184, 0.03294257, ..., 0.02841792,\n",
       "          0.03444869, 0.03092039],\n",
       "         [0.03230795, 0.03410275, 0.03165958, ..., 0.02789864,\n",
       "          0.03478391, 0.03002171],\n",
       "         ...,\n",
       "         [0.03448276, 0.03448276, 0.03448276, ..., 0.03448276,\n",
       "          0.03448276, 0.03448276],\n",
       "         [0.03448276, 0.03448276, 0.03448276, ..., 0.03448276,\n",
       "          0.03448276, 0.03448276],\n",
       "         [0.03448276, 0.03448276, 0.03448276, ..., 0.03448276,\n",
       "          0.03448276, 0.03448276]]], dtype=float32),\n",
       " array([[21, 10, 21, ..., -1, -1, -1],\n",
       "        [10, 21, 10, ..., -1, -1, -1],\n",
       "        [21, 10,  9, ..., -1, -1, -1],\n",
       "        [21,  3, 21, ..., -1, -1, -1]], dtype=int64))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## compilign the model??",
   "id": "25b028a1d086a394"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T02:21:39.754086Z",
     "start_time": "2024-07-08T02:21:39.726829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model2.compile(optimizer=Adam(learning_rate=0.0002), loss=CTCLoss)\n",
    "# create all the callbacks \n",
    "checkpointCallback2 = ModelCheckpoint('updatedModel_v1.weights.h5', monitor='loss',save_weights_only=False, save_freq='epoch') # save checkpoints after each epoch\n",
    "exampleCallback2 = ProduceExample(test)"
   ],
   "id": "ec6634918fbbb5b3",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T02:21:41.660118Z",
     "start_time": "2024-07-08T02:21:39.757088Z"
    }
   },
   "cell_type": "code",
   "source": "model2.load_weights('updatedModel_v1.weights.h5')",
   "id": "a982c33bd3d52eb3",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot assign value to variable ' bidirectional_2/forward_gru_2/gru_cell_7/kernel:0': Shape mismatch.The variable shape (7200, 768), and the assigned value shape (288, 768) are incompatible.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[29], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmodel2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_weights\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mupdatedModel_v1.weights.h5\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\backend.py:4302\u001B[0m, in \u001B[0;36mbatch_set_value\u001B[1;34m(tuples)\u001B[0m\n\u001B[0;32m   4300\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mexecuting_eagerly() \u001B[38;5;129;01mor\u001B[39;00m tf\u001B[38;5;241m.\u001B[39minside_function():\n\u001B[0;32m   4301\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m x, value \u001B[38;5;129;01min\u001B[39;00m tuples:\n\u001B[1;32m-> 4302\u001B[0m         \u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43massign\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43masarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype_numpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4303\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   4304\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m get_graph()\u001B[38;5;241m.\u001B[39mas_default():\n",
      "\u001B[1;31mValueError\u001B[0m: Cannot assign value to variable ' bidirectional_2/forward_gru_2/gru_cell_7/kernel:0': Shape mismatch.The variable shape (7200, 768), and the assigned value shape (288, 768) are incompatible."
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T13:19:52.039584Z",
     "start_time": "2024-07-08T02:21:44.253946Z"
    }
   },
   "cell_type": "code",
   "source": "model2.fit(train, validation_data=test, epochs=300, callbacks=[checkpointCallback2, exampleCallback2], use_multiprocessing=True)",
   "id": "8c3fe5f2bba6b6ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Original: DOGS ARE FAMOUS FOR BEING MANS BEST FRIEND\n",
      "Prediction: IE E E E E E E \n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: EVERY MARRIAGE IS DIFFERENT\n",
      "Prediction: IE E E E \n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THERE WAS A STONE CIRCLE HERE\n",
      "Prediction: IE E E E E \n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: ID LIKE TO SEE HIM TRY\n",
      "Prediction: IE E E \n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2629s 4s/step - loss: 208.0283 - val_loss: 200.4195\n",
      "Epoch 2/300\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "Original: BUT WHAT ABOUT JOHN MCKINNON\n",
      "Prediction: TH E E E\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: CLIMATE CHANGE AND INVADING SPECIES ALL TAKING THEIR TOLL ON SOME OF OUR BEST LOVED CREATURES\n",
      "Prediction: TH E E E E E E E E E E E E E E E E E E E E E E\n",
      "Word Error Rate:  143.75%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: IF I SAY IT PROPERLY\n",
      "Prediction: TH E E E \n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: DONT LETS BOTHER DRESSING UP\n",
      "Prediction: TH E E E E E E E \n",
      "Word Error Rate:  160.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1492s 2s/step - loss: 170.4603 - val_loss: 112.8613\n",
      "Epoch 3/300\n",
      "1/1 [==============================] - 0s 178ms/step\n",
      "Original: BUY SOME NIGERIAN FOOD\n",
      "Prediction: TE E E E E\n",
      "Word Error Rate:  125.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: ITS IMPORTANT TO KEEP YOUR WEIGHT DOWN\n",
      "Prediction: IH E E E E E\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: ONCE WE GOT OVER THAT\n",
      "Prediction: WHE E E E E E\n",
      "Word Error Rate:  120.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: EVERYBODY PROBABLY WENT OUT ON AN OPERATION WITH THAT THOUGHT IN MIND\n",
      "Prediction: IH E E E E\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1546s 2s/step - loss: 111.9305 - val_loss: 112.8787\n",
      "Epoch 4/300\n",
      "1/1 [==============================] - 0s 301ms/step\n",
      "Original: CARRYING NEARLY TWO\n",
      "Prediction: TH TE \n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: AND LOOK AT THE DETAIL OF THE CARVING\n",
      "Prediction: THE T E E \n",
      "Word Error Rate:  87.5%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: IT WAS STOLEN FROM MY CASE\n",
      "Prediction: IH E \n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: MY FATHER WAS A HUGE INFLUENCE BUT HE REALLY LIKED THE HYBRID TEAS\n",
      "Prediction: WHE E E E \n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2012s 3s/step - loss: 111.7284 - val_loss: 110.0421\n",
      "Epoch 5/300\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "Original: GET THE BOY WHAT HE WANTS\n",
      "Prediction: THE E E E\n",
      "Word Error Rate:  83.33333333333334%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: LETS GO BARGAIN HUNTING\n",
      "Prediction: IT E E T\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: I GOT BIG BEARD\n",
      "Prediction: TE T E E E\n",
      "Word Error Rate:  125.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: WE HAVE TAKEN THEIR PLACE\n",
      "Prediction: WHE E E E E O\n",
      "Word Error Rate:  120.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1696s 3s/step - loss: 110.4990 - val_loss: 110.1375\n",
      "Epoch 6/300\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "Original: AND THAT WAS PROBABLY ONE OF THE REASONS WHY SHE LEFT FRANCE AND ACTUALLY MOVED TO THE UK\n",
      "Prediction: I E E E E E E E A A E\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: IF YOU WANT TO CREATE AN AREA OF WILD FLOWERS\n",
      "Prediction: I T O E E OENE E E\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THEY SHARE A LOT OF THE SAME DESIGN AND TECHNOLOGY AS HIGH PERFORMANCE RUNNING SHOES\n",
      "Prediction: IH A E E E E E\n",
      "Word Error Rate:  93.33333333333333%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: JUST FOR TEN SECONDS TOO LONG\n",
      "Prediction: THE TE E E E E E S E\n",
      "Word Error Rate:  150.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1326s 2s/step - loss: 108.9942 - val_loss: 106.8851\n",
      "Epoch 7/300\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "Original: BUT I CANT TALK\n",
      "Prediction: WE E IE TET\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: LETS HAVE A LOOK AT THE OTHER PROGRAMMES THAT HAVE BEEN ATTRACTING YOUR ATTENTION\n",
      "Prediction: IE E E OE OE O A O E A T O TE\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: TO A CERTAIN EXTENT\n",
      "Prediction: WH E OE OE\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: WHATS ALL THIS CREAM\n",
      "Prediction: WE E E O O I \n",
      "Word Error Rate:  150.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1325s 2s/step - loss: 107.4356 - val_loss: 103.2943\n",
      "Epoch 8/300\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "Original: THIS IS THE FIRST DAY OF LOOKING AFTER HER\n",
      "Prediction: WHE E E E E E IN\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: FORMER HOME OF THE DUKE AND DUCHESS OF CAMBRIDGE\n",
      "Prediction: IE E E E OE A E A E OE E E\n",
      "Word Error Rate:  133.33333333333331%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: DONT MAKE OUT IM THE ONLY ONE FELT SOMETHING\n",
      "Prediction: WE E A T E E E I O OE\n",
      "Word Error Rate:  111.11111111111111%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: IVE GROWN OLD LOOKING AT THEM\n",
      "Prediction: IE OE OE A AN IT\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1401s 2s/step - loss: 104.9811 - val_loss: 100.8520\n",
      "Epoch 9/300\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "Original: PROBABLY HAD LANGUAGE AND\n",
      "Prediction: I O E E I I\n",
      "Word Error Rate:  150.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: TONIGHT FOUR CONTENDERS WILL FACE THE ULTIMATE TV TEST OF NERVE AND KNOWLEDGE\n",
      "Prediction: THE E O O E E E E E E A TO T E OE I\n",
      "Word Error Rate:  123.07692307692308%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: RIGHT IN FRONT OF THE TOWN HALL\n",
      "Prediction: IE AE E E BE E ON\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: TODAY THAT OBJECT IS GOING TO BE ME\n",
      "Prediction: IE E E O O A E I\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2069s 3s/step - loss: 100.5389 - val_loss: 94.7840\n",
      "Epoch 10/300\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "Original: IM WITH MARK TWAIN LIES\n",
      "Prediction: I N H A I I\n",
      "Word Error Rate:  120.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: WERE UNWRAPPING OUR VERY OWN CHRISTMAS SPECIAL\n",
      "Prediction: I A A CA A IN I A E I IT\n",
      "Word Error Rate:  157.14285714285714%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: SERBIA IS PANDEMONIUM LET LOOSE\n",
      "Prediction: IT HET TE HISHE AN AN AN N N M\n",
      "Word Error Rate:  200.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: ITS FINALLY OPENED\n",
      "Prediction: THEIN T OENE\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1438s 2s/step - loss: 99.9732 - val_loss: 95.6731\n",
      "Epoch 11/300\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "Original: GREEN GRASS AND SHRUBBERY AROUND THAT THE ELEPHANTS LOVE AT THIS TIME OF YEAR\n",
      "Prediction: THE A A A HE OE O A A O HE O A T I ON E\n",
      "Word Error Rate:  121.42857142857142%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: IN TWO WEEKS TIME\n",
      "Prediction: I O A O I O ON\n",
      "Word Error Rate:  175.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: AS FAR AS THIS APPLE IS CONCERNED\n",
      "Prediction: A A AS HAS PLE IE IREN\n",
      "Word Error Rate:  85.71428571428571%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THE PILGRIM FATHERS\n",
      "Prediction: AN O O A T\n",
      "Word Error Rate:  166.66666666666669%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1331s 2s/step - loss: 96.6712 - val_loss: 88.3020\n",
      "Epoch 12/300\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "Original: THEN THEY CAN BE RE BURIED WITH ALL THE DIGNITY THEY DESERVE\n",
      "Prediction: IHE ON O A BAE E HE WAL LE IE E E E E\n",
      "Word Error Rate:  116.66666666666667%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: ITS OBVIOUSLY THE HIGH STREET ELECTRICAL RETAILERS\n",
      "Prediction: I O ON O HLELCALRI\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THIS MARVELLOUS CATHEDRAL AND ABBEY CHURCH BEHIND ME WAS DEDICATED TO HIM\n",
      "Prediction: AO H E O HE ADAD O H ECE HE E HE AD CTE O O\n",
      "Word Error Rate:  133.33333333333331%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: FOR THE FIRST TIME\n",
      "Prediction: TE S AR MOM\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1326s 2s/step - loss: 93.6748 - val_loss: 86.6075\n",
      "Epoch 13/300\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "Original: I DONT REALLY MIND\n",
      "Prediction: AN O O A OT\n",
      "Word Error Rate:  125.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: YOURE ALL THE SAME\n",
      "Prediction: THE E E AE\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: I HAVE BEEN GIVEN THE HONOUR AND ACCOLADE OF ORGANISING THE CHURCH BAZAAR\n",
      "Prediction: I O OE AE N ON I E O H H O\n",
      "Word Error Rate:  92.3076923076923%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: IT WAS KIND OF COLOURING IN\n",
      "Prediction: THEN ORIN IN I N N N E E\n",
      "Word Error Rate:  150.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1334s 2s/step - loss: 91.3568 - val_loss: 83.8748\n",
      "Epoch 14/300\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "Original: PROVIDING OVER THREE\n",
      "Prediction: WHE E O RATE\n",
      "Word Error Rate:  133.33333333333331%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: I CAME HERE TO HUNT\n",
      "Prediction: I H A IT\n",
      "Word Error Rate:  80.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: COMES FROM ONE OF THE MOST BEAUTIFUL PARTS OF THE WORLD\n",
      "Prediction: TEM OM OE OE OS PRIE TET ORES\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: LETS GET ERIK DOWN AND GIVE HIM THAT LITTLE CHAT\n",
      "Prediction: THE A TO O I I I A TER\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1321s 2s/step - loss: 87.1521 - val_loss: 79.9657\n",
      "Epoch 15/300\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "Original: ITS THE RESULT OF IT\n",
      "Prediction: I ON E TE TH I\n",
      "Word Error Rate:  120.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: ME AND MRS AXELBY DIDNT FANCY GOING ABROAD THIS YEAR\n",
      "Prediction: THE A O OU E ON E O T U\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: SOME ROCK AND ROLL\n",
      "Prediction: SHS O ON O TIT O\n",
      "Word Error Rate:  150.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THEYRE IN USE ALL OVER EUROPE\n",
      "Prediction: WE I I O OT\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1327s 2s/step - loss: 85.6846 - val_loss: 80.6439\n",
      "Epoch 16/300\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "Original: I WAS JUST THINKING ABOUT AMELIA\n",
      "Prediction: A E E E O E O\n",
      "Word Error Rate:  116.66666666666667%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: AND THAT ALMONDS\n",
      "Prediction: I A A OE ON O \n",
      "Word Error Rate:  200.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THANKS TO THE PERSISTENCE OF LOCAL PEOPLE AND PLANNERS\n",
      "Prediction: THAS TO THE ES SESOE A AE LILS\n",
      "Word Error Rate:  77.77777777777779%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: I THINK IF I TOOK\n",
      "Prediction: I HE IT IT\n",
      "Word Error Rate:  80.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1327s 2s/step - loss: 83.4126 - val_loss: 73.5688\n",
      "Epoch 17/300\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "Original: LIKE POST TRAUMATIC STRESS\n",
      "Prediction: IE POT TRUATIT SRES\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THE HARROGATE TOFFEE IS UNIQUE\n",
      "Prediction: TH R AT FOF RE I QU\n",
      "Word Error Rate:  140.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: ITS A DEMOCRATIC DISH\n",
      "Prediction: IS A DECRTHE IS\n",
      "Word Error Rate:  75.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THE ROMANS STATIONED FOUR OF THEIR BEST LEGIONS IN BRITAIN\n",
      "Prediction: AN WE IE TO ORS INE E OS PE\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1333s 2s/step - loss: 79.3038 - val_loss: 72.1502\n",
      "Epoch 18/300\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "Original: ITS AN AMAZING PLACE\n",
      "Prediction: IS A AMAING PLC\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: IVE SEEN IT IN NEWSPAPER CUTTINGS AND THIS KIND OF THING\n",
      "Prediction: IVE SE IN INS PAE ITIN AN IS ID OTH\n",
      "Word Error Rate:  81.81818181818183%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: BUT AT THE END OF THE DAY\n",
      "Prediction: BU AT THEND OF THEY\n",
      "Word Error Rate:  71.42857142857143%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THEN YOU REALLY NEED TO LOOK AFTER THEM\n",
      "Prediction: THET A E I O OW\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1331s 2s/step - loss: 87.4221 - val_loss: 76.1146\n",
      "Epoch 19/300\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "Original: BEHIND THESE GREAT DOORS\n",
      "Prediction: BE IND THE ERA OR\n",
      "Word Error Rate:  125.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: I CANT REMEMBER MUCH AND UNTIL CHALLENGED\n",
      "Prediction: I CANT EMEME COIT TOE N AIN E\n",
      "Word Error Rate:  85.71428571428571%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THE DRIVER USED TO TAKE ME HOME VIA A VERY FAMOUS CHICKEN OUTLET\n",
      "Prediction: THT AE B IN TIN TOT E TA ONLIY HE ME O E\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: HES BEEN BANGING ON ABOUT IT ALL WEEKEND AND IVE BEEN A SUPPORTIVE GIRLFRIEND\n",
      "Prediction: IS IN IN ES W HN AL ALE AN HES CON E TE E\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1325s 2s/step - loss: 80.5438 - val_loss: 70.0334\n",
      "Epoch 20/300\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "Original: HERRING AND BENEDICTINE LIQUEUR\n",
      "Prediction: WUT E O E HTU\n",
      "Word Error Rate:  125.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: WANT TO GO TO THERE AND LOOK FOR SOMEBODY\n",
      "Prediction: WAT ONG O THE AND LOK FOR MBOEY\n",
      "Word Error Rate:  77.77777777777779%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: AND THEY WERE ONLY TOO HAPPY TO HELP\n",
      "Prediction: IE WI COE A O A E\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: ITS A LANCASTER BOMBER\n",
      "Prediction: THT AER TAE\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1327s 2s/step - loss: 75.2585 - val_loss: 67.2485\n",
      "Epoch 21/300\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "Original: WHILE SHE KEEPS TRYING\n",
      "Prediction: WHI HE EPS RING\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THIS ONE HAS BEEN MADE INTO A TRAY\n",
      "Prediction: WO E O OE AD INTO RAY\n",
      "Word Error Rate:  87.5%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THE BAD NEWS IS IT INVOLVES ANIMAL DUNG\n",
      "Prediction: THE BAD BANG TA END IN OE BES AR SOMADN\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: AND THEN IT SPILT OUT INTO A COURTYARD\n",
      "Prediction: I A LE I I H TE E O O I E\n",
      "Word Error Rate:  150.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1327s 2s/step - loss: 71.7579 - val_loss: 64.1382\n",
      "Epoch 22/300\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "Original: WE HAD A BLAZING ROW\n",
      "Prediction: THE TES BALING WORT\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: RIGHT IN FRONT OF ME\n",
      "Prediction: WEIN FORT FOF ME\n",
      "Word Error Rate:  80.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: ITS ALWAYS NICE AND WARM\n",
      "Prediction: I E A I E\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: AND LETS BE HONEST\n",
      "Prediction: AND TETES\n",
      "Word Error Rate:  75.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1325s 2s/step - loss: 68.8927 - val_loss: 60.1207\n",
      "Epoch 23/300\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "Original: THERES NOT ANYTHING WE CAN DO ABOUT THAT NOW\n",
      "Prediction: THERES NO AN THING WE CAN O BOUT THAT NON\n",
      "Word Error Rate:  66.66666666666666%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: WERE VICTIMS OF OUR OWN SUCCESS\n",
      "Prediction: WE I I O O O\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: PRECIPITATION OVER THE LAST COUPLE OF DAYS\n",
      "Prediction: I A O O O O O T\n",
      "Word Error Rate:  114.28571428571428%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: SO SHOULD WE BE DOING MORE TO DEFEND OUR ISLANDS FROM THE SEA\n",
      "Prediction: S SOS AT THA O A BAE OE E OILE AE FUTHSE\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1322s 2s/step - loss: 67.1357 - val_loss: 58.7913\n",
      "Epoch 24/300\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "Original: WEVE GOT THE WONDERFUL\n",
      "Prediction: WE IE HE E OE\n",
      "Word Error Rate:  125.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: ONCE THOSE LIFT DOORS OPEN\n",
      "Prediction: ONCE THOSE LIFD DORS OPEN\n",
      "Word Error Rate:  40.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: MY BRADSHAWS GUIDE HAS HELPED ME TO UNDERSTAND THE ENGINEERING\n",
      "Prediction: T D IS SIST L END O THESI IN OUEAI TOERINS\n",
      "Word Error Rate:  110.00000000000001%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: HE RESCUED A YOUNG BOY AND HIS MOTHER FROM THE CLUTCHES OF THE FRENCH REVOLUTION\n",
      "Prediction: IVE SES FAENTAR OUSA TES O AN TH HES IOM SE REN\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1323s 2s/step - loss: 67.0527 - val_loss: 56.8612\n",
      "Epoch 25/300\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "Original: I ALWAYS PUT THE FISH BACK FROM THE DON\n",
      "Prediction: TE E BA A AE E T T A\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: WITH ME JOHN HUMPHRYS\n",
      "Prediction: WIT WM JIN HOPS\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: SO WE SET OFF AND ON OUR WAY\n",
      "Prediction: WE A ON OU HE IS AO OT\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: I UNDERSTAND BLOWING OFF STEAM\n",
      "Prediction: IT A THANSE IN AN WE AR\n",
      "Word Error Rate:  140.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1321s 2s/step - loss: 63.6107 - val_loss: 53.7370\n",
      "Epoch 26/300\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "Original: WERE GOING TO DO IT PROPERLY\n",
      "Prediction: T E E E E\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: YOU HAVE TO HOLD YOUR NERVE\n",
      "Prediction: YOU HAVE TO HOLD YOUR NERVE\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: FOUR MORE CONTENDERS ARE ALL SET TO TAKE TO THE STAGE TONIGHT\n",
      "Prediction: FOUR MORE THL FON O A O HE COLI O OTOT TE ELET O CI\n",
      "Word Error Rate:  108.33333333333333%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: SO ITS NOT ALL BAD\n",
      "Prediction: O EORE\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1323s 2s/step - loss: 62.7438 - val_loss: 53.7315\n",
      "Epoch 27/300\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "Original: ALL FIVE OF OUR CONTENDERS HAVE BUT ONE THOUGHT ON THEIR MINDS CAN THEY MAKE IT THROUGH TO THE\n",
      "Prediction: AL FIVE OF OUR CONTENDERS HAVE BUL ONE THOUGHT ON THIR MISIOE THENE BUS SHR HO THESO\n",
      "Word Error Rate:  57.89473684210527%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: IVE BEEN DABBLING IN THIS BUSINESS FOR FORTY YEARS\n",
      "Prediction: I SHE TEY ING THE HESEAS SOE FIR RURE\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: WELL LOOK BACK AT SOME OF THE MOST MEMORABLE MOMENTS OF SONGS OF PRAISE\n",
      "Prediction: ISI HO ICEO F THE MOU MEA ATEME COUT ON T ONS\n",
      "Word Error Rate:  92.85714285714286%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: SO WE ARE TO TAKE A STAND\n",
      "Prediction: WO IND HAR OURE\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1329s 2s/step - loss: 61.2008 - val_loss: 51.9096\n",
      "Epoch 28/300\n",
      "  1/662 [..............................] - ETA: 9:33 - loss: 88.8317"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\n2 root error(s) found.\n  (0) UNKNOWN:  InvalidArgumentError: {{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Input to reshape is a tensor with 3 values, but the requested shape has 4800 [Op:Reshape]\nTraceback (most recent call last):\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 269, in __call__\n    return func(device, token, args)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 147, in __call__\n    outputs = self._call(device, args)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 154, in _call\n    ret = self._func(*args)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\monis\\AppData\\Local\\Temp\\ipykernel_12452\\3629012439.py\", line 13, in loadData\n    frames = loadVideo(videoPath)\n\n  File \"C:\\Users\\monis\\AppData\\Local\\Temp\\ipykernel_12452\\3629012439.py\", line 44, in loadVideo\n    grayFrame = tf.image.rgb_to_grayscale(frame)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 153, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 54, in quick_execute\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\ntensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Input to reshape is a tensor with 3 values, but the requested shape has 4800 [Op:Reshape]\n\n\n\t [[{{node EagerPyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[CTCLoss/CTCLoss/_76]]\n  (1) UNKNOWN:  InvalidArgumentError: {{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Input to reshape is a tensor with 3 values, but the requested shape has 4800 [Op:Reshape]\nTraceback (most recent call last):\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 269, in __call__\n    return func(device, token, args)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 147, in __call__\n    outputs = self._call(device, args)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 154, in _call\n    ret = self._func(*args)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\monis\\AppData\\Local\\Temp\\ipykernel_12452\\3629012439.py\", line 13, in loadData\n    frames = loadVideo(videoPath)\n\n  File \"C:\\Users\\monis\\AppData\\Local\\Temp\\ipykernel_12452\\3629012439.py\", line 44, in loadVideo\n    grayFrame = tf.image.rgb_to_grayscale(frame)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 153, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 54, in quick_execute\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\ntensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Input to reshape is a tensor with 3 values, but the requested shape has 4800 [Op:Reshape]\n\n\n\t [[{{node EagerPyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_452087387]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnknownError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[30], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmodel2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m300\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mcheckpointCallback2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexampleCallback2\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_multiprocessing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 54\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m pywrap_tfe\u001B[38;5;241m.\u001B[39mTFE_Py_Execute(ctx\u001B[38;5;241m.\u001B[39m_handle, device_name, op_name,\n\u001B[0;32m     55\u001B[0m                                       inputs, attrs, num_outputs)\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     57\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mUnknownError\u001B[0m: Graph execution error:\n\n2 root error(s) found.\n  (0) UNKNOWN:  InvalidArgumentError: {{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Input to reshape is a tensor with 3 values, but the requested shape has 4800 [Op:Reshape]\nTraceback (most recent call last):\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 269, in __call__\n    return func(device, token, args)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 147, in __call__\n    outputs = self._call(device, args)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 154, in _call\n    ret = self._func(*args)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\monis\\AppData\\Local\\Temp\\ipykernel_12452\\3629012439.py\", line 13, in loadData\n    frames = loadVideo(videoPath)\n\n  File \"C:\\Users\\monis\\AppData\\Local\\Temp\\ipykernel_12452\\3629012439.py\", line 44, in loadVideo\n    grayFrame = tf.image.rgb_to_grayscale(frame)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 153, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 54, in quick_execute\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\ntensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Input to reshape is a tensor with 3 values, but the requested shape has 4800 [Op:Reshape]\n\n\n\t [[{{node EagerPyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[CTCLoss/CTCLoss/_76]]\n  (1) UNKNOWN:  InvalidArgumentError: {{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Input to reshape is a tensor with 3 values, but the requested shape has 4800 [Op:Reshape]\nTraceback (most recent call last):\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 269, in __call__\n    return func(device, token, args)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 147, in __call__\n    outputs = self._call(device, args)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 154, in _call\n    ret = self._func(*args)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\monis\\AppData\\Local\\Temp\\ipykernel_12452\\3629012439.py\", line 13, in loadData\n    frames = loadVideo(videoPath)\n\n  File \"C:\\Users\\monis\\AppData\\Local\\Temp\\ipykernel_12452\\3629012439.py\", line 44, in loadVideo\n    grayFrame = tf.image.rgb_to_grayscale(frame)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 153, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 54, in quick_execute\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\ntensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Input to reshape is a tensor with 3 values, but the requested shape has 4800 [Op:Reshape]\n\n\n\t [[{{node EagerPyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_452087387]"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T07:06:31.311981Z",
     "start_time": "2024-07-08T17:24:51.680436Z"
    }
   },
   "cell_type": "code",
   "source": "model2.fit(train, validation_data=test, epochs=300, callbacks=[checkpointCallback2, exampleCallback2], use_multiprocessing=True)",
   "id": "df68abce0cca2684",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1/1 [==============================] - 0s 196ms/step\n",
      "Original: TO PUT IT IN A PLAIN\n",
      "Prediction: I E E E\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: BUT HE WAS VERY\n",
      "Prediction: TOS A WEARY\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THE SHOW WHERE A TEAM OF FIVE QUIZ CHALLENGERS ATTEMPT TO BEAT POSSIBLY THE GREATEST QUIZ TEAM IN\n",
      "Prediction: THE SHOW WHERE ATEM OF FIVE QOUR IN ERE AEMS TO OT TERSBLY THANGERE FAR TEATHE \n",
      "Word Error Rate:  72.22222222222221%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: IT REMINDS US THERE IS SO MUCH NORTH OF INVERNESS\n",
      "Prediction: TH IN I MOM MOMI I\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1722s 3s/step - loss: 59.9899 - val_loss: 53.7414\n",
      "Epoch 2/300\n",
      "662/662 [==============================] - 1522s 2s/step - loss: 59.2090 - val_loss: 52.5871\n",
      "Epoch 3/300\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "Original: WEVE ACTUALLY HAD REPORTS\n",
      "Prediction: WHA A A A A\n",
      "Word Error Rate:  125.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: GET READY FOR A GOURMETS GREATEST HIT\n",
      "Prediction: GET EAY FOR A GRURETS GRETET TIT\n",
      "Word Error Rate:  57.14285714285714%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: YOULL PROBABLY FIND IT RIGHT HERE\n",
      "Prediction: YOUL POBLY FIND IT IGHT HERE\n",
      "Word Error Rate:  50.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: GOING UNDER THE HAMMER RIGHT NOW\n",
      "Prediction: TUT UND THR TA MER A\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1425s 2s/step - loss: 57.1565 - val_loss: 49.9184\n",
      "Epoch 4/300\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "Original: FROM EVERY WINDOW FLOWERS\n",
      "Prediction: WOE WE E E O O W\n",
      "Word Error Rate:  175.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: CIDER WITH ROSIE\n",
      "Prediction: YOUE EME E\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: I MADE A FEW CALLS\n",
      "Prediction: I E E A \n",
      "Word Error Rate:  80.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THATS ALL I COULD MANAGE IN HALF AN HOUR\n",
      "Prediction: T E E A E E\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2074s 3s/step - loss: 56.0483 - val_loss: 47.4810\n",
      "Epoch 5/300\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "Original: WHEN ALL THE TIME\n",
      "Prediction: WEN AL THE TIE\n",
      "Word Error Rate:  75.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: WEVE GOT HIGH VALUATIONS AND AUCTION TENSION\n",
      "Prediction: WEVE GOT HIGH ALUATIONS AND AUTIN TENSIN\n",
      "Word Error Rate:  42.857142857142854%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: IN RETURN FOR MY LIFE\n",
      "Prediction: IE E OE OE OM TETEN\n",
      "Word Error Rate:  120.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: FOR THAT REASON\n",
      "Prediction: TRELE E E EAE\n",
      "Word Error Rate:  133.33333333333331%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2059s 3s/step - loss: 54.8421 - val_loss: 47.6700\n",
      "Epoch 6/300\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "Original: BRITAINS FAVOURITE FLOWER\n",
      "Prediction: WE A I\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: ESPECIALLY THAT LOT\n",
      "Prediction: ESPECIALY THAT LOT\n",
      "Word Error Rate:  33.33333333333333%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: IM A STAUNCH TEETOTALLER AND STRICTLY AGAINST DRINKING LEGAL OR ILLEGAL\n",
      "Prediction: T A OE EAN A AND TOENY TO TRSOS TEIN L ARS TIT IN IN\n",
      "Word Error Rate:  118.18181818181819%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: WHY DONT WE FIND OUT\n",
      "Prediction: WO AN OW AS SOT\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2061s 3s/step - loss: 54.5664 - val_loss: 46.7467\n",
      "Epoch 7/300\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "Original: IM HELEN AND IM A CHEMICAL ENGINEER\n",
      "Prediction: I ID CI I I\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: I DIDNT SAY THAT\n",
      "Prediction: I DIDNT SOS THAT\n",
      "Word Error Rate:  25.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: LIFE IN OUR COUNTRY CHANGED FOR EVER\n",
      "Prediction: O E IN ON COUL ON AN ICES SE ERE\n",
      "Word Error Rate:  128.57142857142858%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THERE IS NO WAY\n",
      "Prediction: MUE O O WAY\n",
      "Word Error Rate:  75.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2078s 3s/step - loss: 53.8181 - val_loss: 48.1397\n",
      "Epoch 8/300\n",
      "1/1 [==============================] - 0s 193ms/step\n",
      "Original: THIS TIME OF THE MORNING\n",
      "Prediction: THIS TIE OF THE MRNING\n",
      "Word Error Rate:  40.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: ON VALENTINES DAY\n",
      "Prediction: WUTI OU TER\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: IM A RETIRED CRAFT CENTRE OWNER\n",
      "Prediction: I A E TETRE ONWE\n",
      "Word Error Rate:  83.33333333333334%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: YOU THE GREAT BRITISH PUBLIC PULL OUT ALL THE STOPS AND HELP RAISE AN INCREDIBLE AMOUNT OF MONEY\n",
      "Prediction: IN GHOR AT BIT IT A ONHI ONT TAL BA THO PO ARMIN THANTHA IN TAS\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2229s 3s/step - loss: 53.8583 - val_loss: 46.5595\n",
      "Epoch 9/300\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "Original: WHEN IT COMES TO DESIGNING FABULOUS COSTUMES\n",
      "Prediction: WHI IN O O S INS AN OT E\n",
      "Word Error Rate:  128.57142857142858%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: AS SOON AS WE MET\n",
      "Prediction: I O A E O NG\n",
      "Word Error Rate:  120.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: YOU SAW WHAT PETER WAS LIKE AFTERWARDS\n",
      "Prediction: WA TAT THOP B CE FOIRT LIE TUSER\n",
      "Word Error Rate:  114.28571428571428%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: AFTER JUST A COUPLE OF NIGHTS OF NOT SLEEPING PROPERLY\n",
      "Prediction: THT JUS A CHOL IL FIRE SES E PRORL\n",
      "Word Error Rate:  90.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2126s 3s/step - loss: 52.6141 - val_loss: 45.5734\n",
      "Epoch 10/300\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "Original: MUST HAVE LIGHT\n",
      "Prediction: BUST HAVE LIGHT\n",
      "Word Error Rate:  33.33333333333333%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: HOW WATERPROOF THEY ARE\n",
      "Prediction: A WAR OF THAY ARE\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THIS IS WHAT I REALLY AM\n",
      "Prediction: THIS IS WHAT I REALY A\n",
      "Word Error Rate:  33.33333333333333%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THEY GET ON BOARD AN AEROPLANE AND WE TELL THEM THAT THEY CANT SMOKE\n",
      "Prediction: IS A WINT TUT BI AO RA IND WETL TO THAT THEY CANT TOM\n",
      "Word Error Rate:  78.57142857142857%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2749s 4s/step - loss: 50.8911 - val_loss: 43.2280\n",
      "Epoch 11/300\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "Original: WERE BRINGING YOU A COCKTAIL OF DANCE DELIGHTS\n",
      "Prediction: T WO E O E O TE O\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THE BEST WAY TO GET AROUND WASNT BY ROAD\n",
      "Prediction: TE AS A BE E BUND A ES RON\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: YOU ARE SO STRONG\n",
      "Prediction: THE H IF H HAY\n",
      "Word Error Rate:  125.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: AND YOU FEEL REALLY STUPID\n",
      "Prediction: BE IE CE O ETE BE\n",
      "Word Error Rate:  120.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1458s 2s/step - loss: 50.6367 - val_loss: 43.7321\n",
      "Epoch 12/300\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "Original: VERY SOON AFTER TAKING OFF\n",
      "Prediction: TE ON OF TER TAKING OF\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: ON THE OTHER HAND\n",
      "Prediction: THE H THUR HAND\n",
      "Word Error Rate:  75.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: ITS RATHER LIKE REACHING A TOP OF A MOUNTAIN\n",
      "Prediction: IE E TOT TE ATE IS HRE TH TE\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: TODAY THOSE NATIONS FACE EACH OTHER ONCE AGAIN\n",
      "Prediction: I O A OU U LE NOE R FOUL\n",
      "Word Error Rate:  112.5%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1608s 2s/step - loss: 50.0769 - val_loss: 44.1151\n",
      "Epoch 13/300\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "Original: SO YOU AINT GOT WHITNEY WORKING\n",
      "Prediction: SO YOU ANT GOT WITEY WORKING\n",
      "Word Error Rate:  33.33333333333333%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: DONT YOU THINK\n",
      "Prediction: DO YOU THINK\n",
      "Word Error Rate:  33.33333333333333%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THANKS FOR WATCHING\n",
      "Prediction: THANS FOR WATCHING\n",
      "Word Error Rate:  33.33333333333333%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: YOU CAN BE A HEARTLESS BOY SOMETIMES\n",
      "Prediction: YO THOU POLT MOUN MUL SOAN ASE FRI MIS\n",
      "Word Error Rate:  128.57142857142858%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1455s 2s/step - loss: 49.7156 - val_loss: 42.9990\n",
      "Epoch 14/300\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "Original: ABOUT FORTY ONE YEARS AGO\n",
      "Prediction: I E O O O O\n",
      "Word Error Rate:  120.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: WE ARE NOT LOOKING FOR TROUBLE\n",
      "Prediction: WE HE O TOK KOK WOR RORE\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: BUT ITD PROBABLY BLOW YOUR MINDS SO THE BBC HAS ASKED ME NOT TO\n",
      "Prediction: BE E O BY O YOUR AN ALIE TO ALE HS OD AOTI\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: BUT I WANTED TO GO THAT ARCHITECTURAL SALVAGE YARD ON THE WAY OUT TO LICHFIELD\n",
      "Prediction: BUT A ING GON HAR E IOT HONE TIT TAT TON TH AIE OUT OUL CICHIL\n",
      "Word Error Rate:  93.33333333333333%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1456s 2s/step - loss: 48.2866 - val_loss: 42.1910\n",
      "Epoch 15/300\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "Original: I DO ACTUALLY QUITE LIKE MY JOB\n",
      "Prediction: I D ATULY QUITE LIKEY JOB\n",
      "Word Error Rate:  57.14285714285714%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: YOU CAN FIND YOURSELF A NEW HOME\n",
      "Prediction: YOU CAN FIND YOUSEL OBTE BORE\n",
      "Word Error Rate:  57.14285714285714%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THIS IS THE FINAL EVENING\n",
      "Prediction: TH S TEME REMINEND OPE\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: ESPECIALLY MAMMAL LIFE\n",
      "Prediction: WON I LEPEAPLOL LIN\n",
      "Word Error Rate:  133.33333333333331%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1307s 2s/step - loss: 47.4041 - val_loss: 40.7546\n",
      "Epoch 16/300\n",
      "1/1 [==============================] - 0s 158ms/step\n",
      "Original: ITS EASY TO THINK WE KNOW THIS STORY\n",
      "Prediction: ITS EASY TO THIKNK WE KNOW THS STORY\n",
      "Word Error Rate:  25.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: LOOK AT OTHER GARDENS\n",
      "Prediction: LHOK A OTHR FORDON\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THE FIRST MUSLIM WOMAN TO SERVE AS A CABINET MINISTER\n",
      "Prediction: WE E E HE E E E HE E E E E\n",
      "Word Error Rate:  120.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: BUT TODAYS COUNTY INSPIRED ONE OF BRITAINS MOST FAMOUS ROMANTIC PAINTERS\n",
      "Prediction: WHIT DIT A CAT INDEIN OUT OUDE H HANE BE E E PORDENT INE ANTET\n",
      "Word Error Rate:  136.36363636363635%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1315s 2s/step - loss: 47.3022 - val_loss: 43.9193\n",
      "Epoch 17/300\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "Original: LOUISE AND KEVIN\n",
      "Prediction: I ES E VN\n",
      "Word Error Rate:  133.33333333333331%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: YOU WOULD THINK HE IS A VERY WITTY\n",
      "Prediction: YOU WOLD THIN KHE IS A VERY WITY\n",
      "Word Error Rate:  50.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: ITS A BIT OF A MESS\n",
      "Prediction: IE A A A A\n",
      "Word Error Rate:  66.66666666666666%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: ANYTHING LIKE THAT\n",
      "Prediction: A IE OE THAT\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1393s 2s/step - loss: 47.3482 - val_loss: 42.3181\n",
      "Epoch 18/300\n",
      "1/1 [==============================] - 0s 343ms/step\n",
      "Original: AROUND THESE PARTS OF BUCKINGHAMSHIRE\n",
      "Prediction: IO OD O THESE PARTS OF BUCKINGHAM THIM\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: TRY ANOTHER SWING\n",
      "Prediction: TRY ANOTHER SWING\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: YOU ONLY HAVE ONE ITEM TO WORRY ABOUT\n",
      "Prediction: IE E E A WE OD YOUT\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: MANY OF US DO MUCH OF OUR WRITING ON A KEYBOARD AND THE COMPUTER OR THE SMARTPHONE IS OUR PAGE\n",
      "Prediction: BUYID SIS T HO ON IEY A ON SHINS BOUND LIND AS ORT TAT ANS AED PO TE ANT HERE\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 10378s 16s/step - loss: 46.6016 - val_loss: 42.2227\n",
      "Epoch 19/300\n",
      "1/1 [==============================] - 0s 245ms/step\n",
      "Original: I WOULD SAY IM OPTIMISTIC\n",
      "Prediction: IO WOLD SAY IM OTIMSTIC\n",
      "Word Error Rate:  60.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THE LARGEST WEIGHING FIVE\n",
      "Prediction: THE LARGEST WEIGHIN FIVE\n",
      "Word Error Rate:  25.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: NOT GOOD ONES REALLY\n",
      "Prediction: NO SOESORY EN AY LA\n",
      "Word Error Rate:  125.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: YOU ARE A MAGNIFICENT WOMAN\n",
      "Prediction: YOU ARE IN THIM DID ON\n",
      "Word Error Rate:  80.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1737s 3s/step - loss: 46.0605 - val_loss: 39.9692\n",
      "Epoch 20/300\n",
      "1/1 [==============================] - 0s 227ms/step\n",
      "Original: WE MET WHEN WE WERE AT CHURCH TOGETHER IN ADELAIDE IN AUSTRALIA\n",
      "Prediction: WE MET WHEN WE WERE AT CHURCH TOG THE IN ARELAIDE IN USTRALI\n",
      "Word Error Rate:  33.33333333333333%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THATS VERY PUNCHY\n",
      "Prediction: THATS VERY PUNCE SMANE\n",
      "Word Error Rate:  66.66666666666666%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THAT SOMEONE DIED BECAUSE I OVERSLEPT\n",
      "Prediction: TH O TEME EI\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: LIKE A FISH OUT OF WATER\n",
      "Prediction: I A A O E\n",
      "Word Error Rate:  83.33333333333334%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1949s 3s/step - loss: 46.6325 - val_loss: 40.5118\n",
      "Epoch 21/300\n",
      "1/1 [==============================] - 0s 275ms/step\n",
      "Original: THEY HADNT YET BEEN DISCOVERED\n",
      "Prediction: THEROGTO TO A TEME\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: IM GOING TO JOIN THEM\n",
      "Prediction: I TO A T T\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: BUT BEFORE THAT\n",
      "Prediction: BUT BEFORE THAT\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: AND I PLAY IN A LOCAL SKITTLES TEAM\n",
      "Prediction: AND I PLY BIT EO RO AUTW\n",
      "Word Error Rate:  75.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2202s 3s/step - loss: 45.5737 - val_loss: 41.0018\n",
      "Epoch 22/300\n",
      "1/1 [==============================] - 0s 221ms/step\n",
      "Original: ORGANISING PLANNING STAGES\n",
      "Prediction: I AN PANING STAGES\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: ITS NOT SO BAD\n",
      "Prediction: ITS NO SO BAD\n",
      "Word Error Rate:  25.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: BIG JACK PLANES\n",
      "Prediction: BE E I I \n",
      "Word Error Rate:  133.33333333333331%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: I WISH I COULD HAVE DONE A LITTLE MORE PREPARATION\n",
      "Prediction: I H E E I E E\n",
      "Word Error Rate:  90.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1702s 3s/step - loss: 45.2791 - val_loss: 38.8543\n",
      "Epoch 23/300\n",
      "662/662 [==============================] - ETA: 0s - loss: 45.8428"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[31], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmodel2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m300\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mcheckpointCallback2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexampleCallback2\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_multiprocessing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:1606\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1591\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_eval_data_handler\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1592\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_eval_data_handler \u001B[38;5;241m=\u001B[39m data_adapter\u001B[38;5;241m.\u001B[39mget_data_handler(\n\u001B[0;32m   1593\u001B[0m         x\u001B[38;5;241m=\u001B[39mval_x,\n\u001B[0;32m   1594\u001B[0m         y\u001B[38;5;241m=\u001B[39mval_y,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1604\u001B[0m         steps_per_execution\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_steps_per_execution,\n\u001B[0;32m   1605\u001B[0m     )\n\u001B[1;32m-> 1606\u001B[0m val_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1607\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_x\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1608\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_y\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1609\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_sample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1610\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_batch_size\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1611\u001B[0m \u001B[43m    \u001B[49m\u001B[43msteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1612\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1613\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_queue_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_queue_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1614\u001B[0m \u001B[43m    \u001B[49m\u001B[43mworkers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mworkers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1615\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_multiprocessing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_multiprocessing\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1616\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1617\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_use_cached_eval_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1618\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1619\u001B[0m val_logs \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m   1620\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mval_\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m name: val \u001B[38;5;28;01mfor\u001B[39;00m name, val \u001B[38;5;129;01min\u001B[39;00m val_logs\u001B[38;5;241m.\u001B[39mitems()\n\u001B[0;32m   1621\u001B[0m }\n\u001B[0;32m   1622\u001B[0m epoch_logs\u001B[38;5;241m.\u001B[39mupdate(val_logs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:1947\u001B[0m, in \u001B[0;36mModel.evaluate\u001B[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001B[0m\n\u001B[0;32m   1943\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[0;32m   1944\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m\"\u001B[39m, step_num\u001B[38;5;241m=\u001B[39mstep, _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1945\u001B[0m ):\n\u001B[0;32m   1946\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_test_batch_begin(step)\n\u001B[1;32m-> 1947\u001B[0m     tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtest_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1948\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[0;32m   1949\u001B[0m         context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    912\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    914\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[1;32m--> 915\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    917\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[0;32m    918\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:954\u001B[0m, in \u001B[0;36mFunction._call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    951\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[0;32m    952\u001B[0m \u001B[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001B[39;00m\n\u001B[0;32m    953\u001B[0m \u001B[38;5;66;03m# run the first trace but we should fail if variables are created.\u001B[39;00m\n\u001B[1;32m--> 954\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateful_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    955\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_created_variables \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001B[0;32m    956\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCreating variables on a non-first call to a function\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    957\u001B[0m                    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m decorated with tf.function.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2493\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m   2494\u001B[0m   (graph_function,\n\u001B[0;32m   2495\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[1;32m-> 2496\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2497\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiltered_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1858\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[0;32m   1859\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[0;32m   1860\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[0;32m   1861\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[1;32m-> 1862\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_call_outputs(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1863\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcancellation_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcancellation_manager\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m   1864\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[0;32m   1865\u001B[0m     args,\n\u001B[0;32m   1866\u001B[0m     possible_gradient_type,\n\u001B[0;32m   1867\u001B[0m     executing_eagerly)\n\u001B[0;32m   1868\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001B[0m, in \u001B[0;36m_EagerDefinedFunction.call\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    497\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _InterpolateFunctionError(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    498\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m cancellation_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 499\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    501\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_num_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    503\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    504\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    505\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    506\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[0;32m    507\u001B[0m         \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msignature\u001B[38;5;241m.\u001B[39mname),\n\u001B[0;32m    508\u001B[0m         num_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_outputs,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    511\u001B[0m         ctx\u001B[38;5;241m=\u001B[39mctx,\n\u001B[0;32m    512\u001B[0m         cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_manager)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 54\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     57\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T21:46:24.390636Z",
     "start_time": "2024-07-09T07:06:57.401626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model2.load_weights('updatedModel_v1.weights.h5')\n",
    "\n",
    "model2.compile(optimizer=Adam(learning_rate=0.00006), loss=CTCLoss)\n",
    "# create all the callbacks \n",
    "checkpointCallback2 = ModelCheckpoint('updatedModel_v2.weights.h5', monitor='loss',save_weights_only=False, save_freq='epoch') # save checkpoints after each epoch\n",
    "exampleCallback2 = ProduceExample(test)\n",
    "\n",
    "model2.fit(train, validation_data=test, epochs=300, callbacks=[checkpointCallback2, exampleCallback2], use_multiprocessing=True)\n",
    "\n",
    "# finished with loss ~ 35"
   ],
   "id": "3b9c695e7fa6123e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "Original: ALL THE BIG JOBS\n",
      "Prediction: A TO I O\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: WHEN ALL THE TIME\n",
      "Prediction: WHEN AL THE TIME\n",
      "Word Error Rate:  25.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: AND THIS IS WHATS HAPPENING UNFORTUNATELY\n",
      "Prediction: I WO E O E E ER\n",
      "Word Error Rate:  116.66666666666667%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: HOW DO YOU MEAN\n",
      "Prediction: HE O M\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2487s 4s/step - loss: 43.6409 - val_loss: 36.5546\n",
      "Epoch 2/300\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "Original:  ACRES THAT ARE A SORT OF LIVING RECORD OF THE CHANGES WEVE MADE TO OUR LANDSCAPE\n",
      "Prediction:  ACRES THAT ARE A SORT OF LIVING RECORD OF THE CHANGES WEVE MADE TO OUR LANDSCAPE\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THIS IS REALLY SERIOUS\n",
      "Prediction: THIS IS REALY SERIOUS\n",
      "Word Error Rate:  25.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: MAYBE NEVER EVEN SEEN BEFORE\n",
      "Prediction: MAYBE NVER VEN SEN BEFORE\n",
      "Word Error Rate:  60.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THANK YOU FOR SUPPORTING MY MALVOLIO\n",
      "Prediction: THEAN YOUF FOR SUPORTING MY MALVOLO\n",
      "Word Error Rate:  66.66666666666666%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2123s 3s/step - loss: 41.3092 - val_loss: 35.9490\n",
      "Epoch 3/300\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "Original: KILLERTON HOUSE WAS BUILT IN ONE THOUSAND SEVEN HUNDRED SEVENTY NINE FOR ONE OF DEVONS OLDEST FAMILIES\n",
      "Prediction: KILERTOND OE HRPUITI ETHISND SEN HUNDREDENT PERE E AN GALE\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THE TURBULENCE OF THE AIR WILL CAUSE THEIR HABITS TO\n",
      "Prediction: THET YUBUNENCE TON TEN A IL WO H HEUS IF THE EN IA IT TO\n",
      "Word Error Rate:  140.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THERES SUPPOSED TO BE A CEASEFIRE IN UKRAINE\n",
      "Prediction: BU DIT TLOSED TABE A LILE TOUR SO HI\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: IM FROM BEVERLEY IN EAST YORKSHIRE\n",
      "Prediction: H ON BIERENE I THE LATH\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2129s 3s/step - loss: 41.7371 - val_loss: 37.2988\n",
      "Epoch 4/300\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "Original: WE HAVE ANOTHER REHEARSAL ON FRIDAY\n",
      "Prediction: WA E O E E\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: BUT IT WOULD HAVE BEEN LOVELY\n",
      "Prediction: I H E HE HARE I\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THIS STUFF WILL DO THE TRICK\n",
      "Prediction: IT O O\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: YOU REALLY LIKED THAT\n",
      "Prediction: YOU RAL LIKED TAT\n",
      "Word Error Rate:  50.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2129s 3s/step - loss: 39.9115 - val_loss: 34.7655\n",
      "Epoch 5/300\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "Original: IT REMINDS US THERE IS SO MUCH NORTH OF INVERNESS\n",
      "Prediction: I A O TE I A E I A TE\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: EVEN WHEN THEY SURFACE\n",
      "Prediction: WHE N HE SURFACE\n",
      "Word Error Rate:  75.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: SO THE DIGOXIN WORKED\n",
      "Prediction: TE E TA AE HELE\n",
      "Word Error Rate:  125.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THOUSANDS OF YEARS\n",
      "Prediction: HE HILY YEARS\n",
      "Word Error Rate:  66.66666666666666%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2128s 3s/step - loss: 40.0573 - val_loss: 36.2206\n",
      "Epoch 6/300\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "Original: IM JUST NOT A BIG FAN OF\n",
      "Prediction: ID IS NOT A BIG FAN OF\n",
      "Word Error Rate:  28.57142857142857%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THE PRESIDENT ELECT HAS MET WITH PRESIDENT OBAMA\n",
      "Prediction: THE PRESIDENT ELECT HAS MET WITH PRESIDENT OBAMA\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THEY MAY NEED A MIRACLE\n",
      "Prediction: WHE E AN N\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: IF YOU TAKE THAT LONELINESS AWAY FROM IT\n",
      "Prediction: TOR EAR TO L HO TONE I LCON NONM\n",
      "Word Error Rate:  112.5%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2126s 3s/step - loss: 39.7626 - val_loss: 33.5869\n",
      "Epoch 7/300\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "Original: IT DOESNT MATTER WHAT YOUR DENOMINATION IS\n",
      "Prediction: IH A E A TO O I I AN ON\n",
      "Word Error Rate:  142.85714285714286%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: AND ALONG THE WAY\n",
      "Prediction: AND ALONG THE WAY\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: I ALWAYS DO BUSMANS HOLIDAYS MYSELF\n",
      "Prediction: I ALWAYS DO BUSMANS HOL DADANESE\n",
      "Word Error Rate:  33.33333333333333%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: A GIANT EMERGED FROM AN ENORMOUS EGG\n",
      "Prediction: A E AE DENE BICE LENOR MOUS EG\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2123s 3s/step - loss: 39.6511 - val_loss: 35.3948\n",
      "Epoch 8/300\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "Original: THE ANSWER IS SIMPLE\n",
      "Prediction: THE ANSWER IS SIMPLE\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: ALL OF THEM HAVE BEEN DANCING FOR A WHILE SO\n",
      "Prediction: AL OF THEM HAVE BEN DANCING FOR A WHITO SOU PACGE\n",
      "Word Error Rate:  50.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: JUST EMAIL THAT TO ME\n",
      "Prediction: SE SER GANET AU OM\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: WELCOME TO EGGHEADS\n",
      "Prediction: BE CAS O FERTHANDS\n",
      "Word Error Rate:  133.33333333333331%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2126s 3s/step - loss: 38.4596 - val_loss: 33.6014\n",
      "Epoch 9/300\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "Original: COS SHE LIKES YOU\n",
      "Prediction: COS SHE LIKES YOU\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: PRECIPITATION OVER THE LAST COUPLE OF DAYS\n",
      "Prediction: TE E A E I E\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: SELLING FOR JUST A PENNY EACH\n",
      "Prediction: SELING FOR JUST A PE NEN T S\n",
      "Word Error Rate:  83.33333333333334%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: WHAT ABOUT EIGHT ON THAT\n",
      "Prediction: TH A T A A AT\n",
      "Word Error Rate:  120.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2121s 3s/step - loss: 38.3649 - val_loss: 35.0513\n",
      "Epoch 10/300\n",
      "1/1 [==============================] - 0s 178ms/step\n",
      "Original: I CAME IN TO BE CROSS WITH YOU\n",
      "Prediction: AE IE HA HEN BE CROS WITH YOU\n",
      "Word Error Rate:  62.5%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THE MEAT FAVOURED BY HER MAJESTY THE QUEEN\n",
      "Prediction: T E E E E E O T O TE\n",
      "Word Error Rate:  125.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: I THINK HES MADE ME REALISE THAT WE CANT DO A HALF HEARTED JOB IF YOURE GOING TO HAVE A BEACHY\n",
      "Prediction: I THEL CAE EO HINT N OCET TO AUT ED O A CHO LER CFINSA ANRSA ANCE MAMETEN THE IL MEL ONT\n",
      "Word Error Rate:  95.23809523809523%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: BUT FOOD AND BEDDING WAS IN SHORT SUPPLY\n",
      "Prediction: S I OD AND BEDIN CHA OTOURT USLY\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2125s 3s/step - loss: 39.5417 - val_loss: 32.9105\n",
      "Epoch 11/300\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "Original: IM KIND OF GLAD\n",
      "Prediction: IM KIND OF GLAD\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: NO ONES DOING NOTHING ABOUT IT\n",
      "Prediction: NO NES DOING NOTHING ABOUT IT\n",
      "Word Error Rate:  16.666666666666664%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: SIX FIGURES OR SO\n",
      "Prediction: IX FIGURES ON WONE\n",
      "Word Error Rate:  75.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: IT HAS TWO SYMPHONY ORCHESTRAS\n",
      "Prediction: TT TOIM TME OUT CUE CINT TINGREN AN\n",
      "Word Error Rate:  160.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2129s 3s/step - loss: 38.2264 - val_loss: 34.6138\n",
      "Epoch 12/300\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "Original: AFTER A NIGHT SHIFT IN HERE\n",
      "Prediction: IE E I I HE\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: IM STILL GOING TO STAY WORKING IN MARKETING BUT IM COMPLETELY CHANGING SECTOR SO IM GOING TO GO\n",
      "Prediction: I O TOT TOT TAY WORKING IN MARKETING BUT IM COMPLETEY THE ENIC CISN ITING THO GO\n",
      "Word Error Rate:  66.66666666666666%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: QUITE A BIT HAS CHANGED\n",
      "Prediction: IY BOL AC BINETLTD DID\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: ITS GONE FROM TIRED AND OLD\n",
      "Prediction: TO SESRCHEYOR EN\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2123s 3s/step - loss: 37.7565 - val_loss: 33.0076\n",
      "Epoch 13/300\n",
      "1/1 [==============================] - 0s 184ms/step\n",
      "Original: WHO IS AN EXPERT IN YORK\n",
      "Prediction: I E A I A\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: YOU THE GREAT BRITISH PUBLIC PULL OUT ALL THE STOPS AND HELP RAISE AN INCREDIBLE AMOUNT OF MONEY\n",
      "Prediction: YOU THE GREAT BRITISH PUBLIC PUL OUT AL THE STOPS AND HELP RISE OF FISTN EM MUALEN\n",
      "Word Error Rate:  44.44444444444444%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THATS ALWAYS FASCINATED ME\n",
      "Prediction: TACE AS AS FOIS GANE TEAD ME\n",
      "Word Error Rate:  150.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: BUT WHEN HE SAW ME\n",
      "Prediction: WU AN O O\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2128s 3s/step - loss: 38.4716 - val_loss: 31.8222\n",
      "Epoch 14/300\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "Original: CANT SAY THE SAME ABOUT THE REST OF THEM\n",
      "Prediction: I E E TI I O E E\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: AS A MATTER OF FACT\n",
      "Prediction: I E O E\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: SO WE COULD BE LOOKING AT AN EXTENSIVE RANGE THAT HAS A LARGE POPULATION OF ELEPHANTS ALONG\n",
      "Prediction: SO WE COULD BE L POINK TES TIN OUTIVE RANGE THAT HIS A LOLRE POTOT TOUT TO FOR GE SOF CALS\n",
      "Word Error Rate:  82.35294117647058%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: MADE FROM WHOLEMEAL FLOUR\n",
      "Prediction: BI ING GHRE ONETETE TOUR\n",
      "Word Error Rate:  125.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2124s 3s/step - loss: 38.1131 - val_loss: 33.1877\n",
      "Epoch 15/300\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "Original: WHEN YOURE YOUNGER\n",
      "Prediction: WHE O ER\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: MY MUM USED TO HAVE ONE\n",
      "Prediction: MUM MUMUSED TO HAVE ONE\n",
      "Word Error Rate:  33.33333333333333%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THERE IS NO ONE TO HOLD MY ORSE\n",
      "Prediction: I E A A A TE\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: IM ALWAYS SURPRISED THAT PEOPLE ARENT SCARED ENOUGH OF ME\n",
      "Prediction: TE E I O O TO I I O E\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2119s 3s/step - loss: 37.0841 - val_loss: 36.5317\n",
      "Epoch 16/300\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "Original: ARE WE GOING TO BE INTERFERING\n",
      "Prediction: I WE E O O ON\n",
      "Word Error Rate:  83.33333333333334%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: HE STILL HAD A CHANCE OF NOT SURVIVING THIS INJURY\n",
      "Prediction: HE STIL HAD A CHANCE OF NOT SURVIVING THIS INSOR\n",
      "Word Error Rate:  20.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: ITS ALL ABOUT MYTH MAKING\n",
      "Prediction: THIY O I IS S E KHING\n",
      "Word Error Rate:  140.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: YOU DONT HAVE TO JUST WATCH THESE IN THE EVENING WITH US\n",
      "Prediction: IO IY HITE JUST WATCH THE IN THE BE HE WITHU\n",
      "Word Error Rate:  66.66666666666666%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2253s 3s/step - loss: 36.1965 - val_loss: 33.6281\n",
      "Epoch 17/300\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "Original: WHEN THEY BECOME ADULT\n",
      "Prediction: WHEN THEY BECOME ADULT\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: I JUST HOLD ON TO EM FOR A BIT\n",
      "Prediction: I JUST HOLD ON TO EM FOR A BIT\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: IT MUST BE THAT TIME OF DAY AGAIN\n",
      "Prediction: I I T RE A\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: AND THEYRE BEING VERY DECISIVE\n",
      "Prediction: I ERE ISH\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2242s 3s/step - loss: 37.3008 - val_loss: 35.0661\n",
      "Epoch 18/300\n",
      "1/1 [==============================] - 0s 210ms/step\n",
      "Original: NOT THAT MANY PEOPLE LIVE HERE\n",
      "Prediction: NOT THAT MANY PEOPLE LIVE HERE\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: AND THEY MIGHT NOT EVEN MAKE IT THROUGH THE FIRST BATTLE\n",
      "Prediction: I HE MISTHT NOT VEN MAKE IT THROUGH THE FIRST BATE\n",
      "Word Error Rate:  45.45454545454545%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: AND I RECKON WITH THE PROPER TREATMENT\n",
      "Prediction: BO FO DI DRADAD TOR RATHESESIS\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: ACCORDING TO THE KENNEL CLUBS LATEST DATA\n",
      "Prediction: AFOR ITA TH ENEL CLUSDS ATA\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2221s 3s/step - loss: 36.5686 - val_loss: 34.5110\n",
      "Epoch 19/300\n",
      "1/1 [==============================] - 0s 198ms/step\n",
      "Original: THATS WHAT YOU THINK\n",
      "Prediction: W I IFERING\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THAT IS A FIRST\n",
      "Prediction: THE I BL O\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: WERE UNWRAPPING OUR VERY OWN CHRISTMAS SPECIAL\n",
      "Prediction: TE E E AR IN O THINO GHINGIS STHER\n",
      "Word Error Rate:  128.57142857142858%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: AND I WOULD LIKE THAT FOR MYSELF\n",
      "Prediction: I A E S TE A IN\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2274s 3s/step - loss: 37.3451 - val_loss: 33.4742\n",
      "Epoch 20/300\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "Original: WELCOME TO DRAGONS DEN\n",
      "Prediction: WELCOME TO DRAGONS DEN\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: BECAUSE YOU HATED THEM\n",
      "Prediction: BECAUSE YOU HATED THEM\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: WERE IN THE ROYAL COUNTY OF BERKSHIRE\n",
      "Prediction: WE I E E O O O E A\n",
      "Word Error Rate:  128.57142857142858%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: IS INCREDIBLY UPSET THAT THEYVE CUT DOWN THE TREES\n",
      "Prediction: IS IS Y AL LEA AN O A FOE EL\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2172s 3s/step - loss: 36.4878 - val_loss: 32.7393\n",
      "Epoch 21/300\n",
      "1/1 [==============================] - 0s 224ms/step\n",
      "Original: WHETHER WE LIKE IT OR NOT\n",
      "Prediction: WER HE WE LIKE IT ON NO\n",
      "Word Error Rate:  66.66666666666666%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: I WAS LOOKING FOR ONE HUNDRED\n",
      "Prediction: I O E O O ON\n",
      "Word Error Rate:  83.33333333333334%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THE ONLY POSITIVE IS\n",
      "Prediction: THE ONLY POSITIVE IS\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: YOUR PRESENCE IS ENOUGH OF A PRESENT FOR ME\n",
      "Prediction: YOUR PRESENCE IS ENOUGH OF A PRAESUT TIN I\n",
      "Word Error Rate:  33.33333333333333%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2320s 4s/step - loss: 37.0760 - val_loss: 32.8640\n",
      "Epoch 22/300\n",
      "1/1 [==============================] - 0s 252ms/step\n",
      "Original: AS WE APPROACH THE TH ANNIVERSARY OF HIS DEATH\n",
      "Prediction: AS WE APROACH THE TH ANIVERSARY OF HIS DEATH\n",
      "Word Error Rate:  22.22222222222222%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: OR YOU JUST FANCY HAVING A GO\n",
      "Prediction: I OU O A A O HA\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: IVE LOVED EVERY MINUTE OF IT\n",
      "Prediction: IE O L TE E\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THEYVE ALL TURNED UP HERE TODAY TO ASK OUR EXPERTS THAT ALL IMPORTANT QUESTION WHICH EVERYBODY\n",
      "Prediction: YO VOVE OE LUR TOENE TE AS A ASK OUR EXPERTS THAT AL IMPORTANT U TEAM\n",
      "Word Error Rate:  75.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1912s 3s/step - loss: 35.6913 - val_loss: 30.3156\n",
      "Epoch 23/300\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "Original: SO LETS GO BARGAIN\n",
      "Prediction: THES I A\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: AND SHES GOING TO LOVE THAT\n",
      "Prediction: AND SHES GOING TO LOVE THAT\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: TODAY COULD BE THE DAY IM MISTAKEN FOR SOMEONE\n",
      "Prediction: WO CO COLD BE THE DAY IM MIS SI A HIR IR OE EATHAND\n",
      "Word Error Rate:  111.11111111111111%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: WELL FIND OUT WHICH ONES LATER ON IN THE SHOW BUT RIGHT NOW\n",
      "Prediction: W HAP POIPE OIS SLATER ON TENE WIS SO BURT ES WOW\n",
      "Word Error Rate:  92.3076923076923%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1655s 3s/step - loss: 36.9257 - val_loss: 31.6951\n",
      "Epoch 24/300\n",
      "1/1 [==============================] - 0s 232ms/step\n",
      "Original: LETS HOPE OUR EXPERTS TODAY FIND SOME TREASURES TO RIVAL THE COLLECTIONS HERE AT WESTON PARK\n",
      "Prediction: LETS HOPE OUR EXPERTS TODAY FIND SOME TREASURES TO RIVAL THE COLECTIONS HERE AT WESTIEN LACE\n",
      "Word Error Rate:  18.75%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THAT SUGAR WAS PRODUCED ON LARGE PLANTATIONS AND THOSE PLANTATIONS EMPLOYED THE\n",
      "Prediction: THE SUSIROS O O ON WS ARS SOVE IRE AY OVE I TIH ORE DISL LOS HASISISIS ANY BUTOAN TONE TH\n",
      "Word Error Rate:  166.66666666666669%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: SERENGETI IS IN TROUBLE\n",
      "Prediction: SOENS SIA TE MANARELE\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: FOR THIS IMPOSING BUILDING\n",
      "Prediction: WHE GA THE ME\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2361s 4s/step - loss: 36.7123 - val_loss: 32.8321\n",
      "Epoch 25/300\n",
      "662/662 [==============================] - ETA: 0s - loss: 37.3887[' ', \"LET'S\", ' ', 'FIND', ' ', 'OUT', ' ', 'WHICH', ' ', 'CONTENDER', ' ', 'KNOWS', ' ', 'MORE', ' ', 'ABOUT', ' ', 'THE', ' ', 'WHAT'][' ', 'AND', ' ', \"I'M\", ' ', 'NOT', ' ', 'JOKING']\n",
      "\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  ValueError: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.\nTraceback (most recent call last):\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 269, in __call__\n    return func(device, token, args)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 147, in __call__\n    outputs = self._call(device, args)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 163, in _call\n    outputs = [\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 164, in <listcomp>\n    _maybe_copy_to_context_device(self._convert(x, dtype=dtype),\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 131, in _convert\n    return ops.convert_to_tensor(value, dtype=dtype)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py\", line 183, in wrapped\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1638, in convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 343, in _constant_tensor_conversion_function\n    return constant(v, dtype=dtype, name=name)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 267, in constant\n    return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 279, in _constant_impl\n    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 304, in _constant_eager_impl\n    t = convert_to_eager_tensor(value, ctx, dtype)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 102, in convert_to_eager_tensor\n    return ops.EagerTensor(value, ctx.device_name, dtype)\n\nValueError: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.\n\n\n\t [[{{node EagerPyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[CTCLoss/Shape_3/_4]]\n  (1) INVALID_ARGUMENT:  ValueError: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.\nTraceback (most recent call last):\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 269, in __call__\n    return func(device, token, args)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 147, in __call__\n    outputs = self._call(device, args)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 163, in _call\n    outputs = [\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 164, in <listcomp>\n    _maybe_copy_to_context_device(self._convert(x, dtype=dtype),\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 131, in _convert\n    return ops.convert_to_tensor(value, dtype=dtype)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py\", line 183, in wrapped\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1638, in convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 343, in _constant_tensor_conversion_function\n    return constant(v, dtype=dtype, name=name)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 267, in constant\n    return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 279, in _constant_impl\n    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 304, in _constant_eager_impl\n    t = convert_to_eager_tensor(value, ctx, dtype)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 102, in convert_to_eager_tensor\n    return ops.EagerTensor(value, ctx.device_name, dtype)\n\nValueError: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.\n\n\n\t [[{{node EagerPyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_test_function_767510983]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mInvalidArgumentError\u001B[0m                      Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[32], line 8\u001B[0m\n\u001B[0;32m      5\u001B[0m checkpointCallback2 \u001B[38;5;241m=\u001B[39m ModelCheckpoint(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mupdatedModel_v2.weights.h5\u001B[39m\u001B[38;5;124m'\u001B[39m, monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m'\u001B[39m,save_weights_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, save_freq\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mepoch\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;66;03m# save checkpoints after each epoch\u001B[39;00m\n\u001B[0;32m      6\u001B[0m exampleCallback2 \u001B[38;5;241m=\u001B[39m ProduceExample(test)\n\u001B[1;32m----> 8\u001B[0m \u001B[43mmodel2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m300\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mcheckpointCallback2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexampleCallback2\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_multiprocessing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 54\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m pywrap_tfe\u001B[38;5;241m.\u001B[39mTFE_Py_Execute(ctx\u001B[38;5;241m.\u001B[39m_handle, device_name, op_name,\n\u001B[0;32m     55\u001B[0m                                       inputs, attrs, num_outputs)\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     57\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mInvalidArgumentError\u001B[0m: Graph execution error:\n\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  ValueError: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.\nTraceback (most recent call last):\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 269, in __call__\n    return func(device, token, args)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 147, in __call__\n    outputs = self._call(device, args)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 163, in _call\n    outputs = [\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 164, in <listcomp>\n    _maybe_copy_to_context_device(self._convert(x, dtype=dtype),\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 131, in _convert\n    return ops.convert_to_tensor(value, dtype=dtype)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py\", line 183, in wrapped\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1638, in convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 343, in _constant_tensor_conversion_function\n    return constant(v, dtype=dtype, name=name)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 267, in constant\n    return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 279, in _constant_impl\n    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 304, in _constant_eager_impl\n    t = convert_to_eager_tensor(value, ctx, dtype)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 102, in convert_to_eager_tensor\n    return ops.EagerTensor(value, ctx.device_name, dtype)\n\nValueError: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.\n\n\n\t [[{{node EagerPyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[CTCLoss/Shape_3/_4]]\n  (1) INVALID_ARGUMENT:  ValueError: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.\nTraceback (most recent call last):\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 269, in __call__\n    return func(device, token, args)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 147, in __call__\n    outputs = self._call(device, args)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 163, in _call\n    outputs = [\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 164, in <listcomp>\n    _maybe_copy_to_context_device(self._convert(x, dtype=dtype),\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 131, in _convert\n    return ops.convert_to_tensor(value, dtype=dtype)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py\", line 183, in wrapped\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1638, in convert_to_tensor\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 343, in _constant_tensor_conversion_function\n    return constant(v, dtype=dtype, name=name)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 267, in constant\n    return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 279, in _constant_impl\n    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 304, in _constant_eager_impl\n    t = convert_to_eager_tensor(value, ctx, dtype)\n\n  File \"C:\\Users\\monis\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 102, in convert_to_eager_tensor\n    return ops.EagerTensor(value, ctx.device_name, dtype)\n\nValueError: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.\n\n\n\t [[{{node EagerPyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_test_function_767510983]"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T04:23:59.376930Z",
     "start_time": "2024-07-10T00:53:45.474628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model2.compile(optimizer=Adam(learning_rate=0.00002), loss=CTCLoss)\n",
    "# create all the callbacks \n",
    "checkpointCallback2 = ModelCheckpoint('updatedModel_v3.weights.h5', monitor='loss',save_weights_only=False, save_freq='epoch') # save checkpoints after each epoch\n",
    "exampleCallback2 = ProduceExample(test)\n",
    "scheduleCallback2 = LearningRateScheduler(scheduler)\n",
    "\n",
    "model2.fit(train, validation_data=test, epochs=300, callbacks=[checkpointCallback2, exampleCallback2, scheduleCallback2], use_multiprocessing=True)"
   ],
   "id": "9ed847ccbbdefe0d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "Original: THROUGHOUT THIS YEAR RACHEL HAS MADE FOR THE FIRST TIME A CUT FLOWER GARDEN AND KEPT A DIARY OF IT\n",
      "Prediction: THROUGHOUT THIS YEAR RACHEL HAS MADE FOR THE FIRST TIME A CUT FLOWER GARDEN AND KEPT A DIARY OF IT\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: I ENJOY GOING TO GIGS\n",
      "Prediction: I ENJOY GOING TO GIGS\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: OR YOU JUST FANCY HAVING A GO\n",
      "Prediction: I I E I E ET\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: I WOULD SAY IM OPTIMISTIC\n",
      "Prediction: I WOULD SAY IM OPTIMISTIC\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2698s 4s/step - loss: 36.4023 - val_loss: 32.2581 - lr: 2.0000e-05\n",
      "Epoch 2/300\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "Original: JUST TO BRING BACK THESE EXOTIC SPICES FOR US TO ENJOY\n",
      "Prediction: JUST TO BRING BACK THESE EXOTIC SPICES FOR US TO ENJOY\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: FIVE CONTENDERS TONIGHT\n",
      "Prediction: FIVE CONTENDERS TONIGHT\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THE PREPARATORY WORK IS ALL IMPORTANT\n",
      "Prediction: WHE E A O A O A O A\n",
      "Word Error Rate:  150.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: RECKONED TO BE WORTH AROUND TWO BILLION TO THE UK ECONOMY\n",
      "Prediction: BE E AN HAN TEMI TI ENE PEA TO E I BESTI\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2163s 3s/step - loss: 36.9049 - val_loss: 32.1921 - lr: 2.0000e-05\n",
      "Epoch 3/300\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "Original: GARAGE AND TECHNO FUNK\n",
      "Prediction: INE A ANE SOE PECEN O FUNK\n",
      "Word Error Rate:  150.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: MY GRANDMOTHERS RECIPE FOR HER YORKSHIRE PUDDING\n",
      "Prediction: IN N TERASOINT TI OR AR HONE EN OTIN\n",
      "Word Error Rate:  128.57142857142858%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: CONSTANTINE THE GREAT\n",
      "Prediction: THE E TE TO E\n",
      "Word Error Rate:  166.66666666666669%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: MY MONEY WAS ON NAOMI RUSHBROOK\n",
      "Prediction: WA BARTIE ONDINS AN BUSTROK\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2272s 3s/step - loss: 34.6706 - val_loss: 28.6365 - lr: 2.0000e-05\n",
      "Epoch 4/300\n",
      "1/1 [==============================] - 0s 200ms/step\n",
      "Original: FOUR CONTENDERS WITH US THIS EVENING WHOSE CONFIDENCE MAY HAVE BEEN BOOSTED AS MUCH AS POSSIBLE BY\n",
      "Prediction: FOUR CONTENDERS WITH US THIS EVENING WHOSE CONFIDENCE MAY HAVE BEN BOSTED AS UACUSTHENAND SAM HOY\n",
      "Word Error Rate:  35.294117647058826%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: WHAT ELSE CAN I BE\n",
      "Prediction: WHI IT E HAN TI BE\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THIS IS THE PROGRAMME THAT TESTS OUR CONTENDERS ON THREE LEVELS\n",
      "Prediction: TS WOE FRS SOE TA E I E STISR AERSTALE O SPE TE ESINS\n",
      "Word Error Rate:  127.27272727272727%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: I DONT MEAN SHES A WOMAN\n",
      "Prediction: I DONT MEAN SHES A WOMAN\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2329s 4s/step - loss: 36.0265 - val_loss: 32.7497 - lr: 2.0000e-05\n",
      "Epoch 5/300\n",
      "1/1 [==============================] - 0s 222ms/step\n",
      "Original: HE REWROTE THE RULES SO IT BECAME ALL ABOUT BECOMING AS RICH AS POSSIBLE AND BANKRUPTING ALL THE\n",
      "Prediction: HE REWROTE THE RULES SO IT BECAME AL ABOUT BECOMING AS RICH AS POSIBLE AND MAN ORTCAUR\n",
      "Word Error Rate:  27.77777777777778%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: CHARLIE HAD SUCH GOOD SUCCESS YESTERDAY\n",
      "Prediction: SOE IS IT THAME TIA THA\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: ITS IN TWENTY FOUR HOURS\n",
      "Prediction: IE WAN A O O OE\n",
      "Word Error Rate:  120.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: YOUVE GOT A BAPTIST CHURCH\n",
      "Prediction: IS BE THAR HE UCH\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2267s 3s/step - loss: 35.5068 - val_loss: 30.5419 - lr: 2.0000e-05\n",
      "Epoch 6/300\n",
      "574/662 [=========================>....] - ETA: 2:15 - loss: 35.7935"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[34], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m exampleCallback2 \u001B[38;5;241m=\u001B[39m ProduceExample(test)\n\u001B[0;32m      5\u001B[0m scheduleCallback2 \u001B[38;5;241m=\u001B[39m LearningRateScheduler(scheduler)\n\u001B[1;32m----> 7\u001B[0m \u001B[43mmodel2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m300\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mcheckpointCallback2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexampleCallback2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduleCallback2\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_multiprocessing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:1564\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1556\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[0;32m   1557\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1558\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1561\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m   1562\u001B[0m ):\n\u001B[0;32m   1563\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[1;32m-> 1564\u001B[0m     tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1565\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[0;32m   1566\u001B[0m         context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    912\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    914\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[1;32m--> 915\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    917\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[0;32m    918\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001B[0m, in \u001B[0;36mFunction._call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    944\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[0;32m    945\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[0;32m    946\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[1;32m--> 947\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateless_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)  \u001B[38;5;66;03m# pylint: disable=not-callable\u001B[39;00m\n\u001B[0;32m    948\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateful_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    949\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[0;32m    950\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[0;32m    951\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2493\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m   2494\u001B[0m   (graph_function,\n\u001B[0;32m   2495\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[1;32m-> 2496\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2497\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiltered_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1858\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[0;32m   1859\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[0;32m   1860\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[0;32m   1861\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[1;32m-> 1862\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_call_outputs(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1863\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcancellation_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcancellation_manager\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m   1864\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[0;32m   1865\u001B[0m     args,\n\u001B[0;32m   1866\u001B[0m     possible_gradient_type,\n\u001B[0;32m   1867\u001B[0m     executing_eagerly)\n\u001B[0;32m   1868\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001B[0m, in \u001B[0;36m_EagerDefinedFunction.call\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    497\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _InterpolateFunctionError(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    498\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m cancellation_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 499\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    501\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_num_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    503\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    504\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    505\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    506\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[0;32m    507\u001B[0m         \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msignature\u001B[38;5;241m.\u001B[39mname),\n\u001B[0;32m    508\u001B[0m         num_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_outputs,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    511\u001B[0m         ctx\u001B[38;5;241m=\u001B[39mctx,\n\u001B[0;32m    512\u001B[0m         cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_manager)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 54\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     57\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-10T04:25:04.652689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model2.compile(optimizer=Adam(learning_rate=0.000007), loss=CTCLoss)\n",
    "# create all the callbacks \n",
    "checkpointCallback2 = ModelCheckpoint('updatedModel_v4.weights.h5', monitor='loss',save_weights_only=False, save_freq='epoch') # save checkpoints after each epoch\n",
    "exampleCallback2 = ProduceExample(test)\n",
    "scheduleCallback3 = LearningRateScheduler(scheduler2)\n",
    "\n",
    "model2.fit(train, validation_data=test, epochs=300, callbacks=[checkpointCallback2, exampleCallback2, scheduleCallback3], use_multiprocessing=True)"
   ],
   "id": "f11b486a3e527ff3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Original: IT WAS A DECADE OF MASSIVE SOCIAL AND POLITICAL CHANGE BUT FOR THOSE OF US WHO LIVED THROUGH IT\n",
      "Prediction: IE I E AD OF MASIVE SOCIAL AND POLITICAL CHANGE BUT FOR THOSE OF US WHO LIVED THROUGH IT\n",
      "Word Error Rate:  26.31578947368421%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: WHEN IT COMES TO DESIGNING FABULOUS COSTUMES\n",
      "Prediction: WHE IS O A O I IN T\n",
      "Word Error Rate:  114.28571428571428%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: MARY AND JANE PARMENTER\n",
      "Prediction: MARY AND JANE PARMENTER\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: WELCOME TO EGGHEADS\n",
      "Prediction: WELCOME TO EGHEADS\n",
      "Word Error Rate:  33.33333333333333%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2006s 3s/step - loss: 35.4785 - val_loss: 30.0917 - lr: 6.3339e-06\n",
      "Epoch 2/300\n",
      "1/1 [==============================] - 0s 277ms/step\n",
      "Original: THATS WHAT YOU THINK\n",
      "Prediction: WH E A OIN\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: HOW ABOUT SAYING\n",
      "Prediction: HOWOW ABOUT SAYING\n",
      "Word Error Rate:  33.33333333333333%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: FIVE AND THEN THIS ONE HAD SIX\n",
      "Prediction: WIVE AV AND THEN THIS ONE HAD SIX\n",
      "Word Error Rate:  28.57142857142857%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THERE IS NO ONE TO HOLD MY ORSE\n",
      "Prediction: H E T TO ARAR\n",
      "Word Error Rate:  87.5%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1428s 2s/step - loss: 35.6308 - val_loss: 32.8134 - lr: 5.7311e-06\n",
      "Epoch 3/300\n",
      "1/1 [==============================] - 0s 256ms/step\n",
      "Original: FROM THEIR FORTIFIED PALACES\n",
      "Prediction: FROM THEIR FORTIFIED PALACES\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: LETS JUST CALM DOWN\n",
      "Prediction: LETS JUST CALM DOWN\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THEY ALREADY HAD A SCIENTIFIC NAME\n",
      "Prediction: TY A CANY AD A STIENT OF CICE NAME\n",
      "Word Error Rate:  116.66666666666667%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: A GREAT SELECTION OF ANTIQUES SHOPS\n",
      "Prediction: I I O ON OR R R\n",
      "Word Error Rate:  116.66666666666667%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1512s 2s/step - loss: 36.0024 - val_loss: 30.5199 - lr: 5.1857e-06\n",
      "Epoch 4/300\n",
      "1/1 [==============================] - 0s 193ms/step\n",
      "Original: SOMEHOW WE MUST RAISE MONEY\n",
      "Prediction: TH A A A A IN \n",
      "Word Error Rate:  120.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: COMES FROM NOTTINGHAM\n",
      "Prediction: CE FOFO TOTINGHAM\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: ACTUALLY COMES FROM THE WORD FOR KIDNEY\n",
      "Prediction: A A A E E E\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: IF YOU CANT BE WITH KUSH\n",
      "Prediction: I O TA A I ON\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1559s 2s/step - loss: 35.4174 - val_loss: 30.1066 - lr: 4.6922e-06\n",
      "Epoch 5/300\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "Original: I THINK IT WOULD PROBABLY IN MOST SENSES\n",
      "Prediction: I WAT O A A E O A A\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: LETS NOT GET CARRIED AWAY\n",
      "Prediction: I E E O O AM\n",
      "Word Error Rate:  120.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: SEVERAL HUNDRED YEARS AGO\n",
      "Prediction: SEVERAL HUNDRED YEARS AGO\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: WHEN I WAS IN MY S\n",
      "Prediction: WHE E O A O\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1418s 2s/step - loss: 35.8165 - val_loss: 32.2309 - lr: 4.2457e-06\n",
      "Epoch 6/300\n",
      "1/1 [==============================] - 0s 206ms/step\n",
      "Original: WEVE GOT HIGH VALUATIONS AND AUCTION TENSION\n",
      "Prediction: WEVE GOT HIGH VALUATIONS AND AUCTION TENSION\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: WORKING ALL HOURS\n",
      "Prediction: B OY ACA AN\n",
      "Word Error Rate:  133.33333333333331%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: FOUR MORE FAMOUS FACES WILL SHOW US WHAT THEY KNOW ABOUT THEIR SPECIALIST SUBJECT AND HOW MUCH\n",
      "Prediction: IT TIS CON DRER RME CUIE O O A OR EVOEN OSOESANE OS EN TI\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: COUNTERTERROR POLICE TRY TO TRACE THREE MISSING SCHOOLGIRLS FROM EAST LONDON WHO ARE HEADING FOR\n",
      "Prediction: ID ID O O O T O OD S\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1510s 2s/step - loss: 34.2505 - val_loss: 32.6265 - lr: 3.8417e-06\n",
      "Epoch 7/300\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "Original: IVE BEEN VERY FORTUNATE AND IVE DANCED IN MAJOR FINALS AT BLACKPOOL DANCE\n",
      "Prediction: IVE BEN VERY FORTUNATE AND IVE DANCED IN M JOR FO A HA HO GO O O O \n",
      "Word Error Rate:  84.61538461538461%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: ACCORDING TO THE KENNEL CLUBS LATEST DATA\n",
      "Prediction: AN DOINT THAN TO KE WE BINBIN TEAT ASI HIS\n",
      "Word Error Rate:  128.57142857142858%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: YOUVE MADE THE PROMISE\n",
      "Prediction: WE ER E E\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: TO ASK THAT ALL IMPORTANT QUESTION\n",
      "Prediction: WEA TA TA IN AT PEOPESTION\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 1970s 3s/step - loss: 35.5586 - val_loss: 32.0773 - lr: 3.4761e-06\n",
      "Epoch 8/300\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "Original: WE WORK TOGETHER\n",
      "Prediction: WE WORK TOGETHER\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: AND I REALLY HOPE\n",
      "Prediction: I I I TE\n",
      "Word Error Rate:  75.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: AND THEYRE LOOKING FOR ALL THESE WONDERFUL SIGNS OF AUTUMN\n",
      "Prediction: AND THEYRE LOKING FOR AL THESE WIN OS PILO HEN\n",
      "Word Error Rate:  60.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: SIX HUNDRED SQUARE METRES AT A GUIDE PRICE OF NINETY FIVE\n",
      "Prediction: LAS THUNENS TINE COAC TANSE H E BAESE ININY MOF HIVE\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2180s 3s/step - loss: 35.7872 - val_loss: 31.7321 - lr: 3.1453e-06\n",
      "Epoch 9/300\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "Original: HES ONE OF THE PEOPLE IVE BEEN COACHING\n",
      "Prediction: ITS ONE OF THE PEOPLE IVE BEN COACHING\n",
      "Word Error Rate:  25.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: IM INVITING EVERYBODY HERE TO BRING IN TWO THOUSAND AND WHATEVER IT IS\n",
      "Prediction: I AV OVE ONE ONY AE TO BRINGINT SES WAT THAT WT HOURED I\n",
      "Word Error Rate:  107.6923076923077%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: AND FROSTING IT\n",
      "Prediction: BHT AE O IN HT TEIT NST PATA\n",
      "Word Error Rate:  266.66666666666663%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: YOU ARE FRIENDLY WITH THE COLONEL\n",
      "Prediction: I WE E E ANE E ONE\n",
      "Word Error Rate:  116.66666666666667%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2119s 3s/step - loss: 33.1725 - val_loss: 32.2176 - lr: 2.8460e-06\n",
      "Epoch 10/300\n",
      "1/1 [==============================] - 0s 189ms/step\n",
      "Original: I DONT WANT TO TALK ABOUT IT\n",
      "Prediction: IDINT WANT TO TALK ABOUT IT\n",
      "Word Error Rate:  28.57142857142857%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THEY STARTED THE WHOLE THING\n",
      "Prediction: WHE E O TEN\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THE FIRST MUSLIM WOMAN TO SERVE AS A CABINET MINISTER\n",
      "Prediction: THE E A I A A A E A ON ON I T\n",
      "Word Error Rate:  110.00000000000001%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: HE WOULD HAVE LEFT IT OVERNIGHT\n",
      "Prediction: WE E E TOE ON \n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2129s 3s/step - loss: 34.8191 - val_loss: 31.5654 - lr: 2.5752e-06\n",
      "Epoch 11/300\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "Original: SPACE TO GROW THINGS\n",
      "Prediction: I E E E\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: AND DEPART WITH THE FINANCIAL BACKING OF A MULTI MILLIONAIRE DRAGON INVESTOR\n",
      "Prediction: AND DEPART WITH THE FINANCIAL BACKING OF A MULTIE INI STUAT PNT\n",
      "Word Error Rate:  33.33333333333333%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: IVE DEFINITELY HAD IT\n",
      "Prediction: I E O INY THE L\n",
      "Word Error Rate:  150.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: IVE NEVER TRIED TO REPLACE YOUR MUM\n",
      "Prediction: THUS NO RIED O OBULE MOUNE N\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2107s 3s/step - loss: 35.3964 - val_loss: 30.1299 - lr: 2.3301e-06\n",
      "Epoch 12/300\n",
      "1/1 [==============================] - 0s 178ms/step\n",
      "Original: THIS WAS A BARREN LANDSCAPE NO LEVEL GROUND\n",
      "Prediction: THIS WAS A BAREN LANDSCAPE NO LEVEL GROUND\n",
      "Word Error Rate:  12.5%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: YOUR PRESENCE IS ENOUGH OF A PRESENT FOR ME\n",
      "Prediction: YOURPRESENCE IS ENOUGH OF A PRESENT ING TE\n",
      "Word Error Rate:  44.44444444444444%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: WE HAVE TAKEN THEIR PLACE\n",
      "Prediction: WE HINT U E AN TRAM SESE\n",
      "Word Error Rate:  120.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: WHY YOU COVER FOR HIM\n",
      "Prediction: AS AR\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2124s 3s/step - loss: 35.1665 - val_loss: 32.0968 - lr: 2.1084e-06\n",
      "Epoch 13/300\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "Original: A CASTLE THATS DOMINATED THE LANDSCAPE EVER SINCE THE MIDDLE AGES\n",
      "Prediction: A CASTLE THATS DOMINATED THE LANDSCAPE EVER SINCE THE MIDLE EGUNH\n",
      "Word Error Rate:  18.181818181818183%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: MY LEGS WERE OK\n",
      "Prediction: AI BELER OUNON\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: ALL AFTER A VALUATION AND ALL EAGER TO GET INSIDE\n",
      "Prediction: E E BE TE TH O TO O BE TENET E\n",
      "Word Error Rate:  110.00000000000001%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: IM TICKETY BOO\n",
      "Prediction: I I WOMWACS SELIN\n",
      "Word Error Rate:  133.33333333333331%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2107s 3s/step - loss: 35.3406 - val_loss: 32.8943 - lr: 1.9077e-06\n",
      "Epoch 14/300\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "Original: I CANT HELP MYSELF\n",
      "Prediction: WE E E\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: WHICH WILL LATER BE TAKEN TO AUCTION WHERE THEYLL BE SOLD AND THE TEAM THAT WINS IS THE TEAM THAT\n",
      "Prediction: WET IE O E E O O T E E E ON I THER T TO T OD DEDE ERT TIE\n",
      "Word Error Rate:  105.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THE NATURAL HOME\n",
      "Prediction: THE HE PRE HOUR MY U A\n",
      "Word Error Rate:  200.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: SCALE AND MAYBE AN INTERESTING FOCAL POINT FOR YOUR VIEWER TO DISCOVER\n",
      "Prediction: SHALE AND MAYBE AN RE RIRESTON THE TET UN F FOUR EVIE WER TO IR GAVE\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2102s 3s/step - loss: 35.2602 - val_loss: 32.1400 - lr: 1.7262e-06\n",
      "Epoch 15/300\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "Original: TO FIND OUT IF THE TUMOURS SHRUNK\n",
      "Prediction: TO E I O ON TE\n",
      "Word Error Rate:  85.71428571428571%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THE UNDERLYING PATTERN WAS CLEAR\n",
      "Prediction: THE UNDERLYING PATERN WAS CLEAR\n",
      "Word Error Rate:  20.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THE KING IS NOT ALONE HE HAS DARTAGNAN\n",
      "Prediction: THE KING IS NOT ALONE HE HEY STMALIN\n",
      "Word Error Rate:  25.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: BUT SHES NOT CONFUSED\n",
      "Prediction: BETHE THE ONG A OU\n",
      "Word Error Rate:  125.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2103s 3s/step - loss: 35.0421 - val_loss: 33.8047 - lr: 1.5619e-06\n",
      "Epoch 16/300\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "Original: WERE QUITE SERIOUS\n",
      "Prediction: WHE A A E\n",
      "Word Error Rate:  133.33333333333331%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: SO ITS A MASSIVE RISK\n",
      "Prediction: I S S O I I\n",
      "Word Error Rate:  120.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: IF SOMEBODY WHO UNFORTUNATELY HAS RENAL CANCER OR KIDNEY CANCER\n",
      "Prediction: I O E E O TH E E E E TENE E BE\n",
      "Word Error Rate:  130.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: COME TO MY BOSOM\n",
      "Prediction: SHS O THS HISE\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2097s 3s/step - loss: 35.3031 - val_loss: 30.0062 - lr: 1.4133e-06\n",
      "Epoch 17/300\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "Original: I KNOW WHAT YOURE LIKE\n",
      "Prediction: I KNOW WHAT YOURE LIKE\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: WHICH IS BOTH A CITY AND A REGION ON THE SPANISH MEDITERRANEAN COAST\n",
      "Prediction: WO A A A TIS TAD AD REGION ON THE SPANISH MEDOITAS MON LORS\n",
      "Word Error Rate:  69.23076923076923%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: SOMEBODY WHO HAS BEEN TRIPPING THE LIGHT FANTASTIC\n",
      "Prediction: SOMY MAE SE OUN LOTS A THIR OA MOINS\n",
      "Word Error Rate:  112.5%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: EVEN THOUGH THERES SOMETHING WHICH CAN BE QUITE POMPOUS ABOUT IT AND QUITE SORT OF HAUGHTY\n",
      "Prediction: THE MENE TUTHUR SRMET THE TE TO EGHE TEMASESIET DOE OUT OF TH PE S\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2103s 3s/step - loss: 35.3080 - val_loss: 31.0874 - lr: 1.2788e-06\n",
      "Epoch 18/300\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "Original: IF YOU CAN LEAD ME\n",
      "Prediction: I A A A \n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: COMING UP ON TONIGHTS SHOW\n",
      "Prediction: COMING UP ON TONIGHTS SHOW\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: WAS WILLIAM CAXTON\n",
      "Prediction: WHE I WE EL\n",
      "Word Error Rate:  133.33333333333331%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: INDIGENOUS PEOPLE AND RELATIVELY FEW EUROPEANS THAT CREATED SOME OF THE ESSENTIAL CHARACTERISTICS\n",
      "Prediction: I E O E E E E E A HE A AN TO E I ENER\n",
      "Word Error Rate:  123.07692307692308%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2091s 3s/step - loss: 35.1188 - val_loss: 31.6047 - lr: 1.1571e-06\n",
      "Epoch 19/300\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "Original: IF YOU GOT A POINT TO MAKE\n",
      "Prediction: IF YOU GOT A POINT TO MAKE\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: ITS NOT SO BAD\n",
      "Prediction: ITS NOT SO BAD\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: I ENJOY WATCHING SPORT ON TV\n",
      "Prediction: I N NOY WON CH POS TYIY IT CAT\n",
      "Word Error Rate:  133.33333333333331%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THE SOUTH COAST IN PARTICULAR BECAME REALLY FASHIONABLE\n",
      "Prediction: WHT WHE THAS PIN LONTON WUN U THU H UT\n",
      "Word Error Rate:  125.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2104s 3s/step - loss: 34.6158 - val_loss: 32.7251 - lr: 1.0470e-06\n",
      "Epoch 20/300\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "Original: ARE YOU GOING HOME FOR THE HOLIDAYS\n",
      "Prediction: WR YOUROING HOME FOR THE HOLIDAYS\n",
      "Word Error Rate:  42.857142857142854%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: WHAT DID YOU PUT IN THE MULLED WINE\n",
      "Prediction: THAT DID YOU PUT IN THE MULED WINE\n",
      "Word Error Rate:  25.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: IF YOU TAKE THAT LONELINESS AWAY FROM IT\n",
      "Prediction: T AT LON EN TA ONO THAND BANY Y\n",
      "Word Error Rate:  112.5%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: FIVE CONTENDERS TONIGHT\n",
      "Prediction: THIS ARIT GOD SECRA HAGHE\n",
      "Word Error Rate:  166.66666666666669%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2093s 3s/step - loss: 33.9601 - val_loss: 31.7795 - lr: 9.4735e-07\n",
      "Epoch 21/300\n",
      "1/1 [==============================] - 0s 226ms/step\n",
      "Original: THE FIRST WAS THE SECRET INTELLIGENCE SERVICE\n",
      "Prediction: THE FIRST WAS THE SECRET INTELIGENCE SERVICE\n",
      "Word Error Rate:  14.285714285714285%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: ITS BEEN A POPULAR HOLIDAY DESTINATION FOR WELL OVER ONE HUNDRED YEARS\n",
      "Prediction: ITS BEN A POPULAR HOLIDAY DESTINATION FOR WEL ON ERT THE AY YEARS\n",
      "Word Error Rate:  50.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: LETS PLAY A GAME\n",
      "Prediction: SOTE TA LG GHAVE\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: IT SEEMED TO BE\n",
      "Prediction: IS WARED OME OR\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2105s 3s/step - loss: 34.6373 - val_loss: 31.1932 - lr: 8.5719e-07\n",
      "Epoch 22/300\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "Original: AND I WOULD LIKE THAT FOR MYSELF\n",
      "Prediction: I O E O I R\n",
      "Word Error Rate:  85.71428571428571%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: IN MY HOME COUNTY\n",
      "Prediction: IN MY HOME COUNTY\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: CHARLIE HAD SUCH GOOD SUCCESS YESTERDAY\n",
      "Prediction: SO E O T CES STESDAY\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THE NUMBER OF HORSES IN BRITAIN DROPPED DRAMATICALLY BECAUSE OF THE USE OF CARS AND TRACTORS\n",
      "Prediction: THEN NUBER O GOY SHINTIT TOUHAS EA US IEARGLAYT SISOS HO EN CO A A THATOR\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2099s 3s/step - loss: 35.0029 - val_loss: 33.3531 - lr: 7.7562e-07\n",
      "Epoch 23/300\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "Original: IVE NEVER HEARD OF THAT\n",
      "Prediction: IN E E AN I\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: ITS THAT TIME OF THE DAY AGAIN\n",
      "Prediction: ITS THAT TIME OF THE DAY AGAIN\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: ALL IVE GOT IN HERE IS MY LAUNDRY\n",
      "Prediction: I E E T TN T\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: WE WOULD NOT HAVE A SHOW\n",
      "Prediction: W O O A \n",
      "Word Error Rate:  83.33333333333334%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2120s 3s/step - loss: 34.8716 - val_loss: 30.3236 - lr: 7.0181e-07\n",
      "Epoch 24/300\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "Original: REGULARLY WITH THE COURT\n",
      "Prediction: REGULARLY WITH THE COURT\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: COS YOU CAN HARDLY HEAR OVER THE FAN\n",
      "Prediction: COS YOU CAN HARDLY HEAR OVER THE FAN\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THE USUAL RULES APPLY A MINUTE AND A HALF ON THEIR SPECIALIST SUBJECT AND TWO MINUTES ON GENERAL\n",
      "Prediction: THEN LE ILE ORS TE WA O HANE AN A I E TPON ELT RO TEROUN HE IE BEAL\n",
      "Word Error Rate:  105.55555555555556%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: I GET THE OPPORTUNITY TO EXPLORE THE BRONTES HOME\n",
      "Prediction: I INTE TOY A PLORE THE BIE TORE\n",
      "Word Error Rate:  77.77777777777779%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2106s 3s/step - loss: 35.7334 - val_loss: 33.9456 - lr: 6.3502e-07\n",
      "Epoch 25/300\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "Original: ITS COMPANY POLICY\n",
      "Prediction: S IS TO E A \n",
      "Word Error Rate:  166.66666666666669%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THE BEST THING FOR YOU WOULD BE TO STAY AS FAR AWAY FROM ME AS POSSIBLE\n",
      "Prediction: THE E THING FOR YOU WOULD BE TO STAY AS FAR AWAY FROM ME AS POE TETE\n",
      "Word Error Rate:  18.75%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: GET YOU A DRINK\n",
      "Prediction: I E O E\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: LETS GO BARGAIN HUNTING\n",
      "Prediction: ES GO GOHANG ST THAN\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2111s 3s/step - loss: 34.7869 - val_loss: 32.4313 - lr: 5.7459e-07\n",
      "Epoch 26/300\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "Original: DONT LETS BOTHER DRESSING UP\n",
      "Prediction: TO E A O O IG\n",
      "Word Error Rate:  120.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: IM NOT WILD ABOUT IT\n",
      "Prediction: SO ON CON\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: HE ROLLED ACROSS CENTRAL ASIA LIKE A SANDSTORM A CENTURY AFTER GENGHIS KHAN\n",
      "Prediction: I E O E TO O I O OE O A O HA E THE HE\n",
      "Word Error Rate:  115.38461538461537%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: HES CHARLIES DAD\n",
      "Prediction: A AE AE ERE HATIAN\n",
      "Word Error Rate:  166.66666666666669%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2105s 3s/step - loss: 34.4697 - val_loss: 30.3074 - lr: 5.1991e-07\n",
      "Epoch 27/300\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "Original: WHY WOULDNT THEY\n",
      "Prediction: WHY WOULDNT THEY\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THE PATTERNS WENT A LITTLE BIT\n",
      "Prediction: THA ATES WENT A LITLE BIT\n",
      "Word Error Rate:  50.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THEY KNOW WHAT THEYRE DOING\n",
      "Prediction: THEY KNOW WHAT THEYRE DOING\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: GARAGE AND TECHNO FUNK\n",
      "Prediction: GHARG AEN SE ETA TETHIN LIKE\n",
      "Word Error Rate:  150.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2107s 3s/step - loss: 35.9058 - val_loss: 30.9014 - lr: 4.7044e-07\n",
      "Epoch 28/300\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "Original: I HONOURED MY FATHER WHILE HE WAS ALIVE\n",
      "Prediction: I O O O HE HE TO E E\n",
      "Word Error Rate:  87.5%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: WHAT MONEY WE GET\n",
      "Prediction: WHAT MONEY WE GET\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: I HAVE TO WRITE THEM DOWN JUST LIKE THEY DID WITH THE BIBLE\n",
      "Prediction: I HAVE TO WRITE THEM DOWN UST LIE EOL TIARCDEIG THOUNE\n",
      "Word Error Rate:  53.84615384615385%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THINGS HAVE GONE FROM BAD TO WORSE\n",
      "Prediction: THINGS HVE BEN DR ONT AMN \n",
      "Word Error Rate:  85.71428571428571%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2095s 3s/step - loss: 35.5868 - val_loss: 33.1286 - lr: 4.2567e-07\n",
      "Epoch 29/300\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "Original: LETTER BY LETTER\n",
      "Prediction: TETAR BY LETER\n",
      "Word Error Rate:  66.66666666666666%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: YOU HAVE TO LOOK AT BROWN BEARS OR POLAR BEARS\n",
      "Prediction: YOU HAVE TRO OA AT BRON BEARS OR POLAR BEARS\n",
      "Word Error Rate:  30.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: BUT EVERYONE GOING INTO THE DEN GETS A FRESH CHANCE TO TURN THINGS ROUND\n",
      "Prediction: BUT EVERYONE GOING INTO TO EME HEN OT E STS CHOACH A TOT THER THE COUND\n",
      "Word Error Rate:  85.71428571428571%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: WHAT WERE LOOKING FOR IS WHIPPED\n",
      "Prediction: I E TO E I O O T\n",
      "Word Error Rate:  133.33333333333331%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2079s 3s/step - loss: 34.6899 - val_loss: 32.5525 - lr: 3.8516e-07\n",
      "Epoch 30/300\n",
      "1/1 [==============================] - 0s 255ms/step\n",
      "Original: ILL BE DISCOVERING THE SCIENCE BEHIND THAT THINKING\n",
      "Prediction: IL BE DISCOVERING THE SCIENCE BEHIND THAT THINKING\n",
      "Word Error Rate:  12.5%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: PRESS LIFT BUTTONS\n",
      "Prediction: I LAT BOUS\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: HOW MANY DO YOU WANT ME TO SELL\n",
      "Prediction: THE O O O AN NOT TO M LILOD\n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: IN THIS WEEK OF REMEMBRANCE\n",
      "Prediction: IE HE E OR OR ORLET\n",
      "Word Error Rate:  120.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2104s 3s/step - loss: 34.0625 - val_loss: 31.6914 - lr: 3.4851e-07\n",
      "Epoch 31/300\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "Original: IM TALKING SHOTS\n",
      "Prediction: IM TALKING SHOTS\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: CAN THEY STAY AHEAD AND BEAT THE PACK\n",
      "Prediction: CAN THEY STAY AHEAD AND BEAT THE PACK\n",
      "Word Error Rate:  0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: THIS IS A PLACE OF WORK\n",
      "Prediction: TH O O O \n",
      "Word Error Rate:  100.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: CHARTERING A SPECIAL TRAIN\n",
      "Prediction: IMIE TET TATH OE NE ORESOINT IN\n",
      "Word Error Rate:  175.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "662/662 [==============================] - 2227s 3s/step - loss: 34.7448 - val_loss: 33.4997 - lr: 3.1534e-07\n",
      "Epoch 32/300\n",
      "662/662 [==============================] - ETA: 0s - loss: 34.7501"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[36], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m exampleCallback2 \u001B[38;5;241m=\u001B[39m ProduceExample(test)\n\u001B[0;32m      5\u001B[0m scheduleCallback3 \u001B[38;5;241m=\u001B[39m LearningRateScheduler(scheduler2)\n\u001B[1;32m----> 7\u001B[0m \u001B[43mmodel2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m300\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mcheckpointCallback2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexampleCallback2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduleCallback3\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_multiprocessing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:1606\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1591\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_eval_data_handler\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1592\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_eval_data_handler \u001B[38;5;241m=\u001B[39m data_adapter\u001B[38;5;241m.\u001B[39mget_data_handler(\n\u001B[0;32m   1593\u001B[0m         x\u001B[38;5;241m=\u001B[39mval_x,\n\u001B[0;32m   1594\u001B[0m         y\u001B[38;5;241m=\u001B[39mval_y,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1604\u001B[0m         steps_per_execution\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_steps_per_execution,\n\u001B[0;32m   1605\u001B[0m     )\n\u001B[1;32m-> 1606\u001B[0m val_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1607\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_x\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1608\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_y\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1609\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_sample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1610\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_batch_size\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1611\u001B[0m \u001B[43m    \u001B[49m\u001B[43msteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1612\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1613\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_queue_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_queue_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1614\u001B[0m \u001B[43m    \u001B[49m\u001B[43mworkers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mworkers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1615\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_multiprocessing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_multiprocessing\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1616\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1617\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_use_cached_eval_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1618\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1619\u001B[0m val_logs \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m   1620\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mval_\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m name: val \u001B[38;5;28;01mfor\u001B[39;00m name, val \u001B[38;5;129;01min\u001B[39;00m val_logs\u001B[38;5;241m.\u001B[39mitems()\n\u001B[0;32m   1621\u001B[0m }\n\u001B[0;32m   1622\u001B[0m epoch_logs\u001B[38;5;241m.\u001B[39mupdate(val_logs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:1947\u001B[0m, in \u001B[0;36mModel.evaluate\u001B[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001B[0m\n\u001B[0;32m   1943\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[0;32m   1944\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m\"\u001B[39m, step_num\u001B[38;5;241m=\u001B[39mstep, _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1945\u001B[0m ):\n\u001B[0;32m   1946\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_test_batch_begin(step)\n\u001B[1;32m-> 1947\u001B[0m     tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtest_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1948\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[0;32m   1949\u001B[0m         context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    912\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    914\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[1;32m--> 915\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    917\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[0;32m    918\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:954\u001B[0m, in \u001B[0;36mFunction._call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    951\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[0;32m    952\u001B[0m \u001B[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001B[39;00m\n\u001B[0;32m    953\u001B[0m \u001B[38;5;66;03m# run the first trace but we should fail if variables are created.\u001B[39;00m\n\u001B[1;32m--> 954\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateful_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    955\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_created_variables \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001B[0;32m    956\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCreating variables on a non-first call to a function\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    957\u001B[0m                    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m decorated with tf.function.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2493\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m   2494\u001B[0m   (graph_function,\n\u001B[0;32m   2495\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[1;32m-> 2496\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2497\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiltered_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1858\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[0;32m   1859\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[0;32m   1860\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[0;32m   1861\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[1;32m-> 1862\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_call_outputs(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1863\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcancellation_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcancellation_manager\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m   1864\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[0;32m   1865\u001B[0m     args,\n\u001B[0;32m   1866\u001B[0m     possible_gradient_type,\n\u001B[0;32m   1867\u001B[0m     executing_eagerly)\n\u001B[0;32m   1868\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001B[0m, in \u001B[0;36m_EagerDefinedFunction.call\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    497\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _InterpolateFunctionError(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    498\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m cancellation_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 499\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    501\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_num_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    503\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    504\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    505\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    506\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[0;32m    507\u001B[0m         \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msignature\u001B[38;5;241m.\u001B[39mname),\n\u001B[0;32m    508\u001B[0m         num_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_outputs,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    511\u001B[0m         ctx\u001B[38;5;241m=\u001B[39mctx,\n\u001B[0;32m    512\u001B[0m         cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_manager)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 54\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     57\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T22:14:48.094443Z",
     "start_time": "2024-07-10T22:14:47.867720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get training and test loss histories\n",
    "training_loss = model2.history.history['loss']\n",
    "test_loss = model2.history.history['val_loss']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show();"
   ],
   "id": "6eff0658378dde17",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T22:14:52.406743Z",
     "start_time": "2024-07-10T22:14:51.425033Z"
    }
   },
   "cell_type": "code",
   "source": "value, yHat, decoded = makePrediction(model2)",
   "id": "2a4447c34db46abd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 549ms/step\n",
      "Original: THEY THINK I SWAN ROUND MAYFAIR SPENDING RICH PEOPLES MONEY\n",
      "Prediction: THEY THINK I SWAN ROUND MAYFAIR SPENDING RICH PEOPLES MONEY\n",
      "Word Error Rate on Prediction: 0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: HE WAS VERY HAPPY\n",
      "Prediction: HE WAS VERY HAPY\n",
      "Word Error Rate on Prediction: 25.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: DO I HAVE A MOBILE\n",
      "Prediction: YO I HAVE A MOBILE\n",
      "Word Error Rate on Prediction: 20.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Original: CHRIS IS STAYING IN AUSTRALIA\n",
      "Prediction: TE A E O A E\n",
      "Word Error Rate on Prediction: 120.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Avg Word Error Rate: 33.33333333333333%\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(290, 40, 120, 1)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 111,
   "source": "val[0][0].shape",
   "id": "949ed77bc9baf9e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T23:24:53.059945Z",
     "start_time": "2024-07-05T23:24:53.055433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def padTensor(tensor, target_shape, padding_value=0):\n",
    "  \"\"\"Pads a tensor to the specified target shape with a given padding value.\n",
    "\n",
    "  Args:\n",
    "      tensor: The tensor to be padded.\n",
    "      target_shape: The desired padded shape.\n",
    "      padding_value: The value to use for padding (default: 0).\n",
    "\n",
    "  Returns:\n",
    "      The padded tensor.\n",
    "  \"\"\"\n",
    "\n",
    "  paddings = [[0, target_shape[0] - tensor.shape[0]],  # Pad leading and trailing dimensions 0\n",
    "             [0, target_shape[1] - tensor.shape[1]],  # for the first 3 dimensions\n",
    "             [0, target_shape[2] - tensor.shape[2]],\n",
    "             [0, target_shape[3] - tensor.shape[3]]]  # Pad only for dimension 3\n",
    "  return tf.pad(tensor, paddings, constant_values=padding_value)\n",
    "  \n",
    "def fixPath(path): \n",
    "    return path.replace('\\\\', '\\\\\\\\')\n",
    "  "
   ],
   "id": "4192a78f190f1ea5",
   "outputs": [],
   "execution_count": 198
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T23:35:20.759129Z",
     "start_time": "2024-07-05T23:35:20.744128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def loadData2(fileName): \n",
    "    # tf has the paths as bytes so decode that\n",
    " \n",
    "    # generate the respective paths of the data\n",
    "    vidName = fileName + (\".mp4\")\n",
    "    txtName = fileName + (\".txt\")\n",
    "    print(vidName)\n",
    "    videoPath = os.path.join(rootDir2, \"customVids\", vidName)\n",
    "    alignmentPath = os.path.join(rootDir2, \"customVids\", txtName)\n",
    "    \n",
    "    # return the frames and alignments\n",
    "    frames = loadVideo(videoPath) \n",
    "    alignments = loadText(alignmentPath)\n",
    "    return frames, alignments\n",
    "\n",
    "def loadVideo2(path, ): \n",
    "    cap = cv2.VideoCapture(path)\n",
    "    global lastKnownCrop, frameSizeOld, frameSize, newFrameSize, grayFrame\n",
    "    global errorNums, maxFrameCt\n",
    "    processedFrames = []\n",
    "    isFirstFrame = True\n",
    "    frameShape = None\n",
    "    frameCount = 0\n",
    "    \n",
    "    # for each frame \n",
    "    for n in range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))): \n",
    "        if frameCount != maxFrameCt:\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            # in case a frame is missing, just continue\n",
    "            if frame is None or frame.shape[0] == 0: \n",
    "                continue\n",
    "            \n",
    "            if isFirstFrame: \n",
    "                frameShape  = frame.shape\n",
    "                isFirstFrame = False\n",
    "            \n",
    "            if frame.shape != frameShape: \n",
    "                continue\n",
    "            # crop only the mouth like we'll do on the RPI \n",
    "            frameSizeOld = frame.shape\n",
    "            frame = cropForMouth(frame)\n",
    "            frameSize = frame.shape\n",
    "            frame = cv2.resize(frame, (newImageSize[1], newImageSize[0]))\n",
    "            newFrameSize = frame.shape\n",
    "            try: \n",
    "                grayFrame = tf.image.rgb_to_grayscale(frame)\n",
    "            except: \n",
    "                continue\n",
    "            processedFrames.append(grayFrame)\n",
    "            # processedFrames = [*processedFrames, grayFrame]\n",
    "            lastFrame = grayFrame\n",
    "            frameCount += 1\n",
    "\n",
    "    while len(processedFrames) < maxFrameCt: \n",
    "        processedFrames.append(lastFrame)\n",
    "    cap.release()    \n",
    "\n",
    "    # generate the normalized frames (deviation from the average)\n",
    "    mean = tf.math.reduce_mean(processedFrames, keepdims=True)\n",
    "    try: \n",
    "        std = tf.math.reduce_std(tf.cast(processedFrames, tf.float32),  keepdims=True)\n",
    "    except: \n",
    "        \n",
    "        errorPaths.append(path)\n",
    "        errorInfo.append(\"SECOND STATEMENT\")\n",
    "        errorInfo.append(len(processedFrames))\n",
    "        errorInfo.append(frameSizeOld)\n",
    "        errorInfo.append(frameSize)\n",
    "        errorInfo.append(newFrameSize)\n",
    "        errorInfo.append(grayFrame)\n",
    "    std = tf.math.reduce_std(tf.cast(processedFrames, tf.float32), keepdims=True)\n",
    "    frames = tf.cast(processedFrames, tf.float32)\n",
    "    normalizedFrames = (tf.cast(frames, tf.float32) - tf.cast(mean, tf.float32)) / tf.cast(std, tf.float32)\n",
    "    return normalizedFrames\n",
    "\n",
    "def makePredictionOnVid(path, model):\n",
    "    sample = loadData(tf.convert_to_tensor(path))\n",
    "    print('~'*60)\n",
    "    original = tf.strings.reduce_join(numToChar(sample[1])).numpy().decode('utf-8')\n",
    "    paddedVid =  padTensor(sample[0], (290, 40, 120, 1))\n",
    "    yhat = model.predict(tf.expand_dims(paddedVid, axis=0))\n",
    "    \n",
    "    decoded = tf.keras.backend.ctc_decode(yhat, input_length=[maxFrameCt], greedy=True)[0][0].numpy()\n",
    "    prediction = tf.strings.reduce_join(numToChar(decoded[0])).numpy().decode('utf-8')\n",
    "    print('Original:', original)\n",
    "    print('Prediction:', prediction)\n",
    "    print(\"Word Error Rate on Prediction:\", str(wer(original,prediction) * 100) + \"%\")\n",
    "\n",
    "def makePredictionCustomVid(path, model): \n",
    "    sample = loadData2((path))\n",
    "    original = tf.strings.reduce_join(numToChar(sample[1])).numpy().decode('utf-8')\n",
    "    paddedVid =  padTensor(sample[0], (290, 40, 120, 1))\n",
    "    yhat = model.predict(tf.expand_dims(paddedVid, axis=0))\n",
    "    \n",
    "    decoded = tf.keras.backend.ctc_decode(yhat, input_length=[maxFrameCt], greedy=True)[0][0].numpy()\n",
    "    prediction = tf.strings.reduce_join(numToChar(decoded[0])).numpy().decode('utf-8')\n",
    "    print('Original:', original)\n",
    "    print('Prediction:', prediction)\n",
    "    print(\"Word Error Rate on Prediction:\", str(wer(original,prediction) * 100) + \"%\")"
   ],
   "id": "9b8c8a3c40b391a8",
   "outputs": [],
   "execution_count": 241
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T23:39:00.452939Z",
     "start_time": "2024-07-05T23:38:58.526473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "makePredictionCustomVid(\"5570920046221178499_00015\", model2)\n",
    "\n",
    "path = \"A:\\Lip Reading\\Potential Datasets\\BBC LRS2\\\\allFiles\\\\5570920046221178499_00015.mp4\"\n",
    "makePredictionOnVid(path, model2)"
   ],
   "id": "d3e17401309985fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5570920046221178499_00015.mp4\n",
      "1/1 [==============================] - 0s 389ms/step\n",
      "Original: WHICH OF OUR CONTESTANTS TODAY IS GOING TO MAKE A THUMPING GREAT PROFIT\n",
      "Prediction: WHICH OF OUR CONTESTANTS TODAY IS GOING TO MAKE A THUMPING GREAT PROFIT\n",
      "Word Error Rate on Prediction: 0.0%\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "Original: WHICH OF OUR CONTESTANTS TODAY IS GOING TO MAKE A THUMPING GREAT PROFIT\n",
      "Prediction: WHICH OF OUR CONTESTANTS TODAY IS GOING TO MAKE A THUMPING GREAT PROFIT\n",
      "Word Error Rate on Prediction: 0.0%\n"
     ]
    }
   ],
   "execution_count": 248
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T22:17:18.268174Z",
     "start_time": "2024-07-10T22:17:05.294327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model2.save('finalModelMOREDATA.h5')\n",
    "tf.saved_model.save(model2, 'finalModelMOREDATA')"
   ],
   "id": "cb4ee05381ede6ad",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, gru_cell_7_layer_call_fn, gru_cell_7_layer_call_and_return_conditional_losses while saving (showing 5 of 11). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: finalModelMOREDATA\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: finalModelMOREDATA\\assets\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T22:18:05.808836Z",
     "start_time": "2024-07-10T22:17:36.030307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model2)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "converter.experimental_new_converter=True\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('finalModelMOREDATA.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ],
   "id": "a54706f60d61aa2c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, gru_cell_7_layer_call_fn, gru_cell_7_layer_call_and_return_conditional_losses while saving (showing 5 of 11). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\monis\\AppData\\Local\\Temp\\tmpy067d2og\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\monis\\AppData\\Local\\Temp\\tmpy067d2og\\assets\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4c9ea03a888895ba"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
