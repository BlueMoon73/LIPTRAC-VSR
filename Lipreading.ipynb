{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T20:48:44.596507Z",
     "start_time": "2024-06-28T20:48:39.088997Z"
    }
   },
   "source": [
    "# imports \n",
    "import os \n",
    "import tensorflow as tf \n",
    "import cv2 \n",
    "import numpy\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# making GPU be used, and setting memory limits\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "# gpus = tf.config.list_logical_devices('GPU')\n",
    "print(gpus)\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    print(\"gpu set\")\n",
    "except:\n",
    "    pass\n",
    "    print(\"failed\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "gpu set\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "7c48bcf107f4c31b",
   "metadata": {},
   "source": [
    "## basic functions"
   ]
  },
  {
   "cell_type": "code",
   "id": "787272fefa6ec811",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T20:48:45.399182Z",
     "start_time": "2024-06-28T20:48:44.598507Z"
    }
   },
   "source": [
    "# setting up the functions to convert from chars to num and vice versa\n",
    "vocab = [x for x in \"ABCDEFGHIJKLMNOPQRSTUVWXYZ \"]\n",
    "charToNum = tf.keras.layers.StringLookup(vocabulary=vocab, oov_token=\"\")\n",
    "numToChar = tf.keras.layers.StringLookup(vocabulary=charToNum.get_vocabulary(), oov_token=\"\", invert=True)\n",
    "\n",
    "# facial detection vars \n",
    "faceCascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "lastKnownCrop = (0, 0, 160, 150)\n",
    "\n",
    "# data dir\n",
    "rootDir = 'A:\\Lip Reading\\Potential Datasets\\BBC LRS2\\\\allFiles'\n",
    "rootDir2 = 'A:\\Lip Reading\\Potential Datasets\\\\BBC LRS2'\n",
    "# r = \"A:\\Lip Reading\\Potential Datasets\\BBC LRS2\\\\allFiles\"\n",
    "\n",
    "batchSize = 1\n",
    "\n",
    "errorNums = 0 \n",
    "errorPaths = []"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "c2e49edea7bd4712",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T20:48:45.414225Z",
     "start_time": "2024-06-28T20:48:45.400183Z"
    }
   },
   "source": [
    "# util funcs \n",
    "def faceDetection(img):\n",
    "    # TROUBLESHOOTING\n",
    "    # print(\"max size:\",img.shape, img.shape[0] - 3 * padding, img.shape[1] - 3 * padding)\n",
    "    return faceCascade.detectMultiScale(\n",
    "        img,\n",
    "        scaleFactor=1.3,\n",
    "        minNeighbors=5,\n",
    "        minSize=(30, 30),\n",
    "    )\n",
    "\n",
    "def cropForMouth(img) -> numpy.ndarray:\n",
    "    global lastKnownCrop\n",
    "    rects = faceDetection(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY))\n",
    "    \n",
    "    # finding the largest face in a given image \n",
    "    largestFace = (0,0,0,0)\n",
    "    for (x, y, w, l) in rects:\n",
    "        if (w * l) > largestFace[2] * largestFace[3]:\n",
    "            largestFace = (x, y,w,l)\n",
    "        \n",
    "    if largestFace == (0,0,0,0):\n",
    "        largestFace =lastKnownCrop\n",
    "    # cropping for face \n",
    "    lastKnownCrop = largestFace\n",
    "    y1 = lastKnownCrop[1] \n",
    "    x1 = lastKnownCrop[0]\n",
    "    y2 = y1 + lastKnownCrop[3] \n",
    "    x2 = x1 + lastKnownCrop[2]\n",
    "    return img[y1 + int(0.65 * lastKnownCrop[3]): y2, x1 + int(0.05 * lastKnownCrop[2]): int(0.95 * x2)]\n",
    "\n",
    "def numberToWords(num):  \n",
    "    if num == 0:  \n",
    "        return \"zero\"  \n",
    "    ones = [\"\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"]  \n",
    "    tens = [\"\", \"\", \"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\"]  \n",
    "    teens = [\"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\", \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\"]  \n",
    "    words = \"\"  \n",
    "    if num>= 1000:  \n",
    "        words += ones[num // 1000] + \" thousand \"  \n",
    "        num %= 1000  \n",
    "    if num>= 100:  \n",
    "        words += ones[num // 100] + \" hundred \"  \n",
    "        num %= 100  \n",
    "    if num>= 10 and num<= 19:  \n",
    "        words += teens[num - 10] + \" \"  \n",
    "        num = 0  \n",
    "    elif num>= 20:  \n",
    "        words += tens[num // 10] + \" \"  \n",
    "        num %= 10  \n",
    "    if num>= 1 and num<= 9:  \n",
    "        words += ones[num] + \" \"  \n",
    "    return words.strip().upper()"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "c7fbad6e80991f3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T20:48:45.429184Z",
     "start_time": "2024-06-28T20:48:45.416183Z"
    }
   },
   "source": [
    "def loadData(path): \n",
    "    # tf has the paths as bytes so decode that\n",
    "    path = bytes.decode(path.numpy())\n",
    "    \n",
    "    # extract just the file names\n",
    "    global rootDir\n",
    "    fileName = path.split('\\\\')[-1].split('.')[0]\n",
    "    # generate the respective paths of the data\n",
    "    videoPath = os.path.join(rootDir,f'{fileName}.mp4')\n",
    "    alignmentPath = os.path.join(rootDir,f'{fileName}.txt')\n",
    "    \n",
    "    # return the frames and alignments\n",
    "    frames = loadVideo(videoPath) \n",
    "    alignments = loadText(alignmentPath)\n",
    "    return frames, alignments\n",
    "\n",
    "def loadVideo(path): \n",
    "    cap = cv2.VideoCapture(path)\n",
    "    global lastKnownCrop\n",
    "    global errorNums\n",
    "    processedFrames = []\n",
    "    \n",
    "    # for each frame \n",
    "    for n in range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))): \n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # in case a frame is missing, just continue\n",
    "        if frame is None or frame.shape[0] == 0: \n",
    "            continue\n",
    "        \n",
    "        # crop only the mouth like we'll do on the RPI \n",
    "        frame = cropForMouth(frame)\n",
    "        frame = cv2.resize(frame, (90, 30))\n",
    "        \n",
    "        try: frame = tf.image.rgb_to_grayscale(frame)\n",
    "        except:\n",
    "            errorNums += 1 \n",
    "            errorPaths.append((videoPath, alignmentPath, fileName))\n",
    "            continue\n",
    "        \n",
    "        processedFrames.append(frame)\n",
    "        \n",
    "    cap.release()    \n",
    "\n",
    "    # generate the normalized frames (deviation from the average) \n",
    "    mean = tf.math.reduce_mean(processedFrames)\n",
    "    std = tf.math.reduce_std(tf.cast(processedFrames, tf.float32), axis=[0, 1, 2], keepdims=True)\n",
    "    frames = tf.cast(processedFrames, tf.float32)\n",
    "    normalizedFrames = (tf.cast(frames, tf.float32) - tf.cast(mean, tf.float32)) / tf.cast(std, tf.float32)\n",
    "    return normalizedFrames\n",
    "\n",
    "def loadText(path): \n",
    "    # open and parse the file \n",
    "    with open(path, 'r') as file: lines = file.readlines()\n",
    "    file.close()\n",
    "    \n",
    "    # return the number equivalent of each of the characters of the word \n",
    "    tokens = []\n",
    "    words = lines[0].split()\n",
    "    del words[0]\n",
    "\n",
    "    for word in words: \n",
    "        if word.isnumeric():\n",
    "            newWord = numberToWords(int(word))\n",
    "            words[words.index(word)] = newWord\n",
    "    words = \" \".join(words).split()\n",
    "    \n",
    "    for word in words: \n",
    "        tokens = [*tokens,' ', word]\n",
    "    \n",
    "    return charToNum(tf.reshape(tf.strings.unicode_split(tokens, input_encoding='UTF-8'), (-1)))[1:]   \n",
    "\n",
    "def processData(path): \n",
    "    return tf.py_function(loadData, [path],  (tf.float32, tf.int64))"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "a401cd6fa0067aed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T20:48:46.579593Z",
     "start_time": "2024-06-28T20:48:46.570592Z"
    }
   },
   "source": [
    "def getFrameCount(path) -> int: \n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frameCount = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    cap.release()\n",
    "    return frameCount\n",
    "\n",
    "def getCharCount(path) -> int: \n",
    "    return len(loadText(path))"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "9203a9532b428b04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T20:48:46.860811Z",
     "start_time": "2024-06-28T20:48:46.832297Z"
    }
   },
   "source": [
    "numberPath = \"A:\\\\Lip Reading\\\\Potential Datasets\\\\BBC LRS2\\\\Numbers.txt\"\n",
    "tensorPath = tf.convert_to_tensor(numberPath, dtype=tf.string)\n",
    "path = bytes.decode(tensorPath.numpy())\n",
    "fileName = path.split('\\\\')[-1].split('.')[0]\n",
    "\n",
    "# testing if the loadData, loadVideo, and loadText function all work\n",
    "alignmentPath = os.path.join(rootDir2,f'{fileName}.txt')\n",
    "loadText(alignmentPath)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(25,), dtype=int64, numpy=\n",
       "array([20,  8, 18,  5,  5, 27,  8, 21, 14,  4, 18,  5,  4, 27, 20, 23, 15,\n",
       "       27,  8, 21, 14,  4, 18,  5,  4], dtype=int64)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "1dc8265fa1dc4688",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T20:48:48.797589Z",
     "start_time": "2024-06-28T20:48:46.984327Z"
    }
   },
   "source": [
    "rawPath = \"A:\\\\Lip Reading\\\\Potential Datasets\\\\BBC LRS2\\\\allFiles\\\\5535415699068794046_00001.mp4\"\n",
    "maxCharCt = 145 # found from the dataStats.ipynb\n",
    "maxFrameCt = 154\n",
    "\n",
    "tensorPath = tf.convert_to_tensor(rawPath, dtype=tf.string)\n",
    "path = bytes.decode(tensorPath.numpy())\n",
    "fileName = path.split('\\\\')[-1].split('.')[0]\n",
    "\n",
    "# testing if the loadData, loadVideo, and loadText function all work\n",
    "videoPath = os.path.join(rootDir,f'{fileName}.mp4')\n",
    "alignmentPath = os.path.join(rootDir,f'{fileName}.txt')\n",
    "\n",
    "loadVideo(videoPath)\n",
    "loadText(alignmentPath)\n",
    "\n",
    "frames, text = loadData(tensorPath)\n",
    "print(type(frames))\n",
    "print(len(frames[0][0]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "90\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "b323b3673a461c65",
   "metadata": {},
   "source": [
    "## reading data"
   ]
  },
  {
   "cell_type": "code",
   "id": "15d040463f02dc61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T17:37:40.447145Z",
     "start_time": "2024-06-29T17:37:40.365863Z"
    }
   },
   "source": [
    "# reading all files within the root directory\n",
    "# data = tf.data.Dataset.list_files('A:\\Lip Reading\\Potential Datasets\\BBC LRS2\\mvlrs_v1\\main\\*\\*.mp4')\n",
    "data = tf.data.Dataset.list_files('A:/Lip Reading/Potential Datasets/BBC LRS2/trainFiles6/*.mp4')\n",
    "\n",
    "data = data.shuffle(2000, reshuffle_each_iteration=False) # shuffling data\n",
    "data = data.map(processData) # \"processing\" the data to obtain frames and the respective text \n",
    "\n",
    "dim1 = frames.shape[1]\n",
    "dim2 = frames.shape[2]\n",
    "print(\"dataset size before padding:\", len(data))\n",
    "print(\"data shape of example video:\", frames.shape)\n",
    "print(\"dims\",dim1,dim2)\n",
    "\n",
    "# combining 8 videos as one \"input\"\n",
    "# ensuring all videos are padded to match the longest video, \n",
    "# ensuring the length of all the alignments is the size of the longest text characters, as some are lower. \n",
    "batchSize = 2\n",
    "data = data.padded_batch(batchSize, padded_shapes=([2*maxCharCt,None, None,None], [maxCharCt])) \n",
    "print(\"autotune\",tf.data.AUTOTUNE)\n",
    "# data = data.prefetch(tf.data.AUTOTUNE)\n",
    "data=data.prefetch(1)\n",
    "print(\"data length after padding:\", len(data))\n",
    "print(\"batch size:\", batchSize)\n",
    "\n",
    "train = data.take(int(len(data) * 0.6))\n",
    "test = data.skip(int(len(data) * 0.9))\n",
    "print(\"train data size:\", len(train))\n",
    "print(\"test data size:\",  len(test))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size before padding: 4009\n",
      "data shape of example video: (35, 30, 90, 1)\n",
      "dims 30 90\n",
      "autotune -1\n",
      "data length after padding: 2005\n",
      "batch size: 2\n",
      "train data size: 1203\n",
      "test data size: 201\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "id": "b98206929338ffa0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T17:37:41.348985Z",
     "start_time": "2024-06-29T17:37:41.335480Z"
    }
   },
   "source": [
    "data"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 290, None, None, None), dtype=tf.float32, name=None), TensorSpec(shape=(None, 145), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "id": "771515f5cef4617f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T17:37:42.567373Z",
     "start_time": "2024-06-29T17:37:41.594603Z"
    }
   },
   "source": [
    "val = data.as_numpy_iterator().next()\n",
    "plt.imshow(val[0][0][30])\n",
    "print(len(val[0][0]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "id": "e1b71a67fbe998bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T17:37:42.754641Z",
     "start_time": "2024-06-29T17:37:42.569373Z"
    }
   },
   "source": [
    "tf.strings.reduce_join([numToChar(word) for word in val[1][0]])\n",
    "print(\"num of chars:\", len(([numToChar(word) for word in val[1][0]])))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of chars: 145\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "cell_type": "markdown",
   "id": "7b9fe2e89759a25",
   "metadata": {},
   "source": [
    "## designing the model"
   ]
  },
  {
   "cell_type": "code",
   "id": "44de38608ebec792",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T17:37:43.813804Z",
     "start_time": "2024-06-29T17:37:43.798781Z"
    }
   },
   "source": [
    "# imports for the model architecture \n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv3D, LSTM, Dense, Dropout, Bidirectional, MaxPooling3D, TimeDistributed, Flatten, GRU\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, LearningRateScheduler"
   ],
   "outputs": [],
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "id": "a4e35802d732d3c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T17:37:45.140603Z",
     "start_time": "2024-06-29T17:37:44.423971Z"
    }
   },
   "source": [
    "inputShape = data.as_numpy_iterator().next()[0][0].shape\n",
    "print(inputShape)\n",
    "print(charToNum.get_vocabulary())\n",
    "print(len(charToNum.get_vocabulary()))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(290, 30, 90, 1)\n",
      "['', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', ' ']\n",
      "28\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "id": "84706072692f52d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T17:38:01.157141Z",
     "start_time": "2024-06-29T17:38:00.162643Z"
    }
   },
   "source": [
    "# model to be actually trained\n",
    "model = Sequential([\n",
    "Conv3D(180, kernel_size=(3,3,3), strides=(1,2,2), input_shape=inputShape, padding='same', activation='relu', name=\"conv1\"),\n",
    "MaxPooling3D((1,2,2), name=\"maxPool1\"),\n",
    "\n",
    "Conv3D(256, kernel_size=(1,3,3), strides=(1,2,2), padding='same', activation='relu', name=\"conv2\"),\n",
    "MaxPooling3D((1,2,2), name=\"maxPool2\"),\n",
    "    \n",
    "Conv3D(180, kernel_size=(1,3,3), strides=(1, 2,2), padding='same', activation='relu', name=\"conv3\"),\n",
    "# MaxPooling3D((1,2,1), name=\"maxPool3\"),\n",
    "\n",
    "TimeDistributed(Flatten()),\n",
    "\n",
    "Bidirectional(LSTM(256, kernel_initializer='orthogonal', return_sequences=True)),\n",
    "Dropout(.5),\n",
    "    \n",
    "Bidirectional(LSTM(256, kernel_initializer='orthogonal' , return_sequences=True)),\n",
    "Dropout(.5),\n",
    "\n",
    "Dense(charToNum.vocabulary_size()+1, kernel_initializer='he_normal', activation='softmax')\n",
    "])"
   ],
   "outputs": [],
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "id": "d728bc2771721d6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T17:38:01.606178Z",
     "start_time": "2024-06-29T17:38:01.587151Z"
    }
   },
   "source": [
    "print(inputShape)\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(290, 30, 90, 1)\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1 (Conv3D)              (None, 290, 15, 45, 180)  5040      \n",
      "                                                                 \n",
      " maxPool1 (MaxPooling3D)     (None, 290, 7, 22, 180)   0         \n",
      "                                                                 \n",
      " conv2 (Conv3D)              (None, 290, 4, 11, 256)   414976    \n",
      "                                                                 \n",
      " maxPool2 (MaxPooling3D)     (None, 290, 2, 5, 256)    0         \n",
      "                                                                 \n",
      " conv3 (Conv3D)              (None, 290, 1, 3, 180)    414900    \n",
      "                                                                 \n",
      " time_distributed_9 (TimeDis  (None, 290, 540)         0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " bidirectional_24 (Bidirecti  (None, 290, 512)         1632256   \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 290, 512)          0         \n",
      "                                                                 \n",
      " bidirectional_25 (Bidirecti  (None, 290, 512)         1574912   \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 290, 512)          0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 290, 29)           14877     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,056,961\n",
      "Trainable params: 4,056,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "id": "9cd1b05996f23f8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T23:14:57.722930Z",
     "start_time": "2024-06-29T23:14:57.713386Z"
    }
   },
   "source": [
    "# custom functions \n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 40:\n",
    "        return lr\n",
    "    elif epoch % 30 == 0:\n",
    "        lr = 0.03\n",
    "        return lr \n",
    "        \n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "# custom loss function \n",
    "def CTCLoss(yTrue, yPred):\n",
    "    \n",
    "    # y true is the text alignment (None, 99) \n",
    "    # y pred is the end result of the model (154, 41) \n",
    "    batchLen = tf.cast(tf.shape(yTrue)[0], dtype=\"int64\")\n",
    "\n",
    "    inputLen = tf.cast(tf.shape(yPred)[1], dtype=\"int64\")\n",
    "    labelLen = tf.cast(tf.shape(yTrue)[1], dtype=\"int64\")\n",
    "    inputLen = inputLen * tf.ones(shape=(batchLen, 1), dtype=\"int64\")\n",
    "    labelLen = labelLen * tf.ones(shape=(batchLen, 1), dtype=\"int64\")\n",
    "\n",
    "    loss = tf.keras.backend.ctc_batch_cost(yTrue, yPred, inputLen, labelLen)   \n",
    "    return loss \n",
    "\n",
    "class ProduceExample(tf.keras.callbacks.Callback): \n",
    "    def __init__(self, dataset) -> None: \n",
    "        self.dataset = dataset.as_numpy_iterator()\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None) -> None:\n",
    "        data = self.dataset.next()\n",
    "        yhat = self.model.predict(data[0])\n",
    "        try: \n",
    "            decoded = tf.keras.backend.ctc_decode(yhat, [maxFrameCt, maxFrameCt], greedy=False)[0][0].numpy()\n",
    "            for x in range(len(yhat)):           \n",
    "                print('Original:', tf.strings.reduce_join(numToChar(data[1][x])).numpy().decode('utf-8'))\n",
    "                print('Prediction:', tf.strings.reduce_join(numToChar(decoded[x])).numpy().decode('utf-8'))\n",
    "                print(\"Word Error Rate\", wer(tf.strings.reduce_join(numToChar(data[1][x])).numpy().decode('utf-8'), tf.strings.reduce_join(numToChar(decoded[x])).numpy().decode('utf-8')))\n",
    "                print('~'*100)\n",
    "        except: \n",
    "            pass\n",
    "            \n",
    "      "
   ],
   "outputs": [],
   "execution_count": 95
  },
  {
   "cell_type": "code",
   "id": "d5afa6ac776aca30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T23:15:37.336630Z",
     "start_time": "2024-06-29T23:15:36.820078Z"
    }
   },
   "source": [
    "from jiwer import wer \n",
    "yhat = model.predict(val[0])\n",
    "\n",
    "decoded = tf.keras.backend.ctc_decode(yhat, [maxFrameCt, maxFrameCt], greedy=False)[0][0].numpy()\n",
    "originalArr = []\n",
    "predArr = []\n",
    "for x in range(len(yhat)):          \n",
    "    originalArr.append(tf.strings.reduce_join(numToChar(val[1][x])).numpy().decode('utf-8'))\n",
    "    predArr.append( tf.strings.reduce_join(numToChar(decoded[x])).numpy().decode('utf-8'))\n",
    "    \n",
    "    print('Original:', tf.strings.reduce_join(numToChar(val[1][x])).numpy().decode('utf-8'))\n",
    "    print('Prediction:', tf.strings.reduce_join(numToChar(decoded[x])).numpy().decode('utf-8'))\n",
    "    print(\"Word Error Rate on Prediction\", wer(tf.strings.reduce_join(numToChar(val[1][x])).numpy().decode('utf-8'),  tf.strings.reduce_join(numToChar(decoded[x])).numpy().decode('utf-8')))\n",
    "\n",
    "    print('~' * 10)\n",
    "print(\"Original:\", originalArr, \"\\nPredictions:\", predArr)\n",
    "\n",
    "print(\"Avg Word Error Rate\", wer(originalArr, predArr))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 386ms/step\n",
      "Original: MUCH DISILLUSIONMENT WITH THE LABOUR PARTY IN SCOTLAND\n",
      "Prediction: T \n",
      "Word Error Rate on Prediction 1.0\n",
      "~~~~~~~~~~\n",
      "Original: SIX ON THE SECOND\n",
      "Prediction: T \n",
      "Word Error Rate on Prediction 1.0\n",
      "~~~~~~~~~~\n",
      "Original: ['MUCH DISILLUSIONMENT WITH THE LABOUR PARTY IN SCOTLAND', 'SIX ON THE SECOND'] \n",
      "Predictions: ['T ', 'T ']\n",
      "Avg Word Error Rate 1.0\n"
     ]
    }
   ],
   "execution_count": 98
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## compilign the model??",
   "id": "25b028a1d086a394"
  },
  {
   "cell_type": "code",
   "id": "f92dcff213216a6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T23:15:46.486624Z",
     "start_time": "2024-06-29T23:15:46.454088Z"
    }
   },
   "source": [
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import RMSprop, SGD, Nadam\n",
    "model.compile(optimizer=Nadam(learning_rate=0.004), loss=CTCLoss)\n",
    "checkpointCallback = ModelCheckpoint('newLipModelv5_m1.weights.h5', monitor='loss',save_weights_only=False, save_freq='epoch') \n",
    "scheduleCallback = LearningRateScheduler(scheduler)\n",
    "exampleCallback = ProduceExample(test)\n"
   ],
   "outputs": [],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T23:15:47.235109Z",
     "start_time": "2024-06-29T23:15:47.175492Z"
    }
   },
   "cell_type": "code",
   "source": "model.load_weights('newLipModelv4_m1.weights.h5')",
   "id": "b5432a9e433b06f6",
   "outputs": [],
   "execution_count": 100
  },
  {
   "cell_type": "code",
   "id": "441f0789798741da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T23:01:57.938857Z",
     "start_time": "2024-06-29T22:40:56.717539Z"
    }
   },
   "source": "model.fit(train, validation_data=test, epochs=300, callbacks=[scheduleCallback, checkpointCallback, exampleCallback])",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1203/1203 [==============================] - ETA: 0s - loss: 126.5710"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[71], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m300\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mscheduleCallback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheckpointCallback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexampleCallback\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:1606\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1591\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_eval_data_handler\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1592\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_eval_data_handler \u001B[38;5;241m=\u001B[39m data_adapter\u001B[38;5;241m.\u001B[39mget_data_handler(\n\u001B[0;32m   1593\u001B[0m         x\u001B[38;5;241m=\u001B[39mval_x,\n\u001B[0;32m   1594\u001B[0m         y\u001B[38;5;241m=\u001B[39mval_y,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1604\u001B[0m         steps_per_execution\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_steps_per_execution,\n\u001B[0;32m   1605\u001B[0m     )\n\u001B[1;32m-> 1606\u001B[0m val_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1607\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_x\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1608\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_y\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1609\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_sample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1610\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_batch_size\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1611\u001B[0m \u001B[43m    \u001B[49m\u001B[43msteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1612\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1613\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_queue_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_queue_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1614\u001B[0m \u001B[43m    \u001B[49m\u001B[43mworkers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mworkers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1615\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_multiprocessing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_multiprocessing\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1616\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1617\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_use_cached_eval_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1618\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1619\u001B[0m val_logs \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m   1620\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mval_\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m name: val \u001B[38;5;28;01mfor\u001B[39;00m name, val \u001B[38;5;129;01min\u001B[39;00m val_logs\u001B[38;5;241m.\u001B[39mitems()\n\u001B[0;32m   1621\u001B[0m }\n\u001B[0;32m   1622\u001B[0m epoch_logs\u001B[38;5;241m.\u001B[39mupdate(val_logs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:1947\u001B[0m, in \u001B[0;36mModel.evaluate\u001B[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001B[0m\n\u001B[0;32m   1943\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[0;32m   1944\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m\"\u001B[39m, step_num\u001B[38;5;241m=\u001B[39mstep, _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1945\u001B[0m ):\n\u001B[0;32m   1946\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_test_batch_begin(step)\n\u001B[1;32m-> 1947\u001B[0m     tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtest_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1948\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[0;32m   1949\u001B[0m         context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    912\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    914\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[1;32m--> 915\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    917\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[0;32m    918\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:986\u001B[0m, in \u001B[0;36mFunction._call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    982\u001B[0m   _, _, filtered_flat_args \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    983\u001B[0m       \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateful_fn\u001B[38;5;241m.\u001B[39m_function_spec\u001B[38;5;241m.\u001B[39mcanonicalize_function_inputs(  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m    984\u001B[0m           \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds))\n\u001B[0;32m    985\u001B[0m   \u001B[38;5;66;03m# If we did not create any variables the trace we have is good enough.\u001B[39;00m\n\u001B[1;32m--> 986\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_concrete_stateful_fn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    987\u001B[0m \u001B[43m      \u001B[49m\u001B[43mfiltered_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_concrete_stateful_fn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m    989\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfn_with_cond\u001B[39m(inner_args, inner_kwds, inner_filtered_flat_args):\n\u001B[0;32m    990\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Conditionally runs initialization if it's needed.\"\"\"\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1858\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[0;32m   1859\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[0;32m   1860\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[0;32m   1861\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[1;32m-> 1862\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_call_outputs(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1863\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcancellation_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcancellation_manager\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m   1864\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[0;32m   1865\u001B[0m     args,\n\u001B[0;32m   1866\u001B[0m     possible_gradient_type,\n\u001B[0;32m   1867\u001B[0m     executing_eagerly)\n\u001B[0;32m   1868\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001B[0m, in \u001B[0;36m_EagerDefinedFunction.call\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    497\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _InterpolateFunctionError(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    498\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m cancellation_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 499\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    501\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_num_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    503\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    504\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    505\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    506\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[0;32m    507\u001B[0m         \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msignature\u001B[38;5;241m.\u001B[39mname),\n\u001B[0;32m    508\u001B[0m         num_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_outputs,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    511\u001B[0m         ctx\u001B[38;5;241m=\u001B[39mctx,\n\u001B[0;32m    512\u001B[0m         cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_manager)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 54\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     57\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "id": "5afc328efb5d2476",
   "metadata": {},
   "source": [
    "    errorNums"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8c641bb605c23341",
   "metadata": {},
   "source": [
    "yHat = model.predict(val[0])\n",
    "print(tf.strings.reduce_join([numToChar(tf.argmax(x)) for x in yHat[0]]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1a18ff5285cbb556",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6eff0658378dde17",
   "metadata": {},
   "source": [
    "# Get training and test loss histories\n",
    "training_loss = model.history.history['loss']\n",
    "test_loss = model.history.history['val_loss']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "88f6fecc2fb453ef",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
